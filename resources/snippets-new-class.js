const newClassSnippets = {
    "Blank Class": "public class REPLACE_CLASS\n{\n}",
    "Blank OpMode": "@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\npublic class REPLACE_CLASS extends OpMode\n{\n@Override\npublic void init() {\n\n}\n\n@Override\npublic void init_loop() {\n}\n\n@Override\npublic void start() {\n\n}\n\n@Override\npublic void loop() {\n\n}\n\n@Override\npublic void stop() {\n}\n\n}",
    "ConceptSoundsOnBotJava": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.ftccommon.SoundPlayer;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport java.io.File;\n\n/**\n * This file demonstrates how to play simple sounds on both the RC and DS phones.\n * It illustrates how to play sound files that have been copied to the RC Phone\n * This technique is best suited for use with OnBotJava since it does not require the app to be modified.\n *\n * Operation:\n *\n * Gamepad X & B buttons are used to trigger sounds in this example, but any event can be used.\n * Note: Time should be allowed for sounds to complete before playing other sounds.\n *\n *  To play a new sound, you will need to copy the .wav files to the phone, and then provide the full path to them as part of your OpMode.\n *  This is done in this sample for the two sound files.  silver.wav and gold.wav\n *\n *  You can put the files in a variety of soundPaths, but we recommend you put them in the /FIRST/blocks/sounds folder.\n *  Your OpModes will have guaranteed access to this folder, and you can transfer files into this folder using the BLOCKS web page.\n *  --  There is a link called \"sounds\" on the right hand side of the color bar on the BLOCKS page that can be used to send sound files to this folder by default.\n *  Or you can use Windows File Manager, or ADB to transfer the sound files\n *\n *  To get full use of THIS sample, you will need to copy two sound file called silver.wav and gold.wav to /FIRST/blocks/sounds on the RC phone.\n *  They can be located here:\n *      https://github.com/ftctechnh/ftc_app/tree/master/FtcRobotController/src/main/res/raw/gold.wav\n *      https://github.com/ftctechnh/ftc_app/tree/master/FtcRobotController/src/main/res/raw/silver.wav\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\"/gold.wav\");\n    private File silverFile = new File(\"/sdcard\" + soundPath + \"/silver.wav\");\n\n    // Declare OpMode members.\n    private boolean isX = false;    // Gamepad button state variables\n    private boolean isB = false;\n\n    private boolean wasX = false;   // Gamepad button history variables\n    private boolean WasB = false;\n\n    @Override\n    public void runOpMode() {\n\n        // Make sure that the sound files exist on the phone\n        boolean goldFound   = goldFile.exists();\n        boolean silverFound = silverFile.exists();\n\n        // Display sound status\n        telemetry.addData(\"gold sound\",   goldFound ?   \"Found\" : \"NOT Found \\nCopy gold.wav to \" + soundPath  );\n        telemetry.addData(\"silver sound\", silverFound ? \"Found\" : \"NOT Found \\nCopy silver.wav to \" + soundPath );\n\n        // Wait for the game to start (driver presses PLAY)\n        telemetry.addData(\">\", \"Press Start to continue\");\n        telemetry.update();\n        waitForStart();\n\n        telemetry.addData(\">\", \"Press X or B to play sounds.\");\n        telemetry.update();\n\n        // run until the end of the match (driver presses STOP)\n        while (opModeIsActive()) {\n\n            // say Silver each time gamepad X is pressed (This sound is a resource)\n            if (silverFound && (isX = gamepad1.x) && !wasX) {\n                SoundPlayer.getInstance().startPlaying(hardwareMap.appContext, silverFile);\n                telemetry.addData(\"Playing\", \"Silver File\");\n                telemetry.update();\n            }\n\n            // say Gold each time gamepad B is pressed  (This sound is a resource)\n            if (goldFound && (isB = gamepad1.b) && !WasB) {\n                SoundPlayer.getInstance().startPlaying(hardwareMap.appContext, goldFile);\n                telemetry.addData(\"Playing\", \"Gold File\");\n                telemetry.update();\n            }\n\n            // Save last button states\n            wasX = isX;\n            WasB = isB;\n        }\n    }\n}\n",
    "BasicOpMode Linear": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.hardware.DcMotor;\nimport com.qualcomm.robotcore.util.ElapsedTime;\nimport com.qualcomm.robotcore.util.Range;\n\n\n/**\n * This file contains an minimal example of a Linear \"OpMode\". An OpMode is a 'program' that runs in either\n * the autonomous or the teleop period of an FTC match. The names of OpModes appear on the menu\n * of the FTC Driver Station. When an selection is made from the menu, the corresponding OpMode\n * class REPLACE_CLASS instantiated on the Robot Controller and executed.\n *\n * This particular OpMode just executes a basic Tank Drive Teleop for a two wheeled robot\n * It includes all the skeletal structure that all linear OpModes contain.\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    // Declare OpMode members.\n    private ElapsedTime runtime = new ElapsedTime();\n    private DcMotor leftDrive = null;\n    private DcMotor rightDrive = null;\n\n    @Override\n    public void runOpMode() {\n        telemetry.addData(\"Status\", \"Initialized\");\n        telemetry.update();\n\n        // Initialize the hardware variables. Note that the strings used here as parameters\n        // to 'get' must correspond to the names assigned during the robot configuration\n        // step (using the FTC Robot Controller app on the phone).\n        leftDrive  = hardwareMap.get(DcMotor.class, \"left_drive\");\n        rightDrive = hardwareMap.get(DcMotor.class, \"right_drive\");\n\n        // Most robots need the motor on one side to be reversed to drive forward\n        // Reverse the motor that runs backwards when connected directly to the battery\n        leftDrive.setDirection(DcMotor.Direction.FORWARD);\n        rightDrive.setDirection(DcMotor.Direction.REVERSE);\n\n        // Wait for the game to start (driver presses PLAY)\n        waitForStart();\n        runtime.reset();\n\n        // run until the end of the match (driver presses STOP)\n        while (opModeIsActive()) {\n\n            // Setup a variable for each drive wheel to save power level for telemetry\n            double leftPower;\n            double rightPower;\n\n            // Choose to drive using either Tank Mode, or POV Mode\n            // Comment out the method that's not used.  The default below is POV.\n\n            // POV Mode uses left stick to go forward, and right stick to turn.\n            // - This uses basic math to combine motions and is easier to drive straight.\n            double drive = -gamepad1.left_stick_y;\n            double turn  =  gamepad1.right_stick_x;\n            leftPower    = Range.clip(drive + turn, -1.0, 1.0) ;\n            rightPower   = Range.clip(drive - turn, -1.0, 1.0) ;\n\n            // Tank Mode uses one stick to control each wheel.\n            // - This requires no math, but it is hard to drive forward slowly and keep straight.\n            // leftPower  = -gamepad1.left_stick_y ;\n            // rightPower = -gamepad1.right_stick_y ;\n\n            // Send calculated power to wheels\n            leftDrive.setPower(leftPower);\n            rightDrive.setPower(rightPower);\n\n            // Show the elapsed game time and wheel power.\n            telemetry.addData(\"Status\", \"Run Time: \" + runtime.toString());\n            telemetry.addData(\"Motors\", \"left (%.2f), right (%.2f)\", leftPower, rightPower);\n            telemetry.update();\n        }\n    }\n}\n",
    "ConceptCompassCalibration": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.CompassSensor;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\n/**\n * This file illustrates the concept of calibrating a MR Compass\n * It uses the common Pushbot hardware class REPLACE_CLASS define the drive on the robot.\n * The code is structured as a LinearOpMode\n *\n *   This code assumes there is a compass configured with the name \"compass\"\n *\n *   This code will put the compass into calibration mode, wait three seconds and then attempt\n *   to rotate two full turns clockwise.  This will allow the compass to do a magnetic calibration.\n *\n *   Once compete, the program will put the compass back into measurement mode and check to see if the\n *   calibration was successful.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    /* Declare OpMode members. */\n    HardwarePushbot     robot   = new HardwarePushbot();   // Use a Pushbot's hardware\n    private ElapsedTime runtime = new ElapsedTime();\n    CompassSensor       compass;\n\n    final static double     MOTOR_POWER   = 0.2; // scale from 0 to 1\n    static final long       HOLD_TIME_MS  = 3000;\n    static final double     CAL_TIME_SEC  = 20;\n\n    @Override\n    public void runOpMode() {\n\n        /* Initialize the drive system variables.\n         * The init() method of the hardware class REPLACE_CLASS all the work here\n         */\n        robot.init(hardwareMap);\n\n        // get a reference to our Compass Sensor object.\n        compass = hardwareMap.get(CompassSensor.class, \"compass\");\n\n        // Send telemetry message to signify robot waiting;\n        telemetry.addData(\"Status\", \"Ready to cal\");    //\n        telemetry.update();\n\n        // Wait for the game to start (driver presses PLAY)\n        waitForStart();\n\n        // Set the compass to calibration mode\n        compass.setMode(CompassSensor.CompassMode.CALIBRATION_MODE);\n        telemetry.addData(\"Compass\", \"Compass in calibration mode\");\n        telemetry.update();\n\n        sleep(HOLD_TIME_MS);  // Just do a sleep while we switch modes\n\n        // Start the robot rotating clockwise\n        telemetry.addData(\"Compass\", \"Calibration mode. Turning the robot...\");\n        telemetry.update();\n        robot.leftDrive.setPower(MOTOR_POWER);\n        robot.rightDrive.setPower(-MOTOR_POWER);\n\n        // run until time expires OR the driver presses STOP;\n        runtime.reset();\n        while (opModeIsActive() && (runtime.time() < CAL_TIME_SEC)) {\n            idle();\n        }\n\n        // Stop all motors and turn off claibration\n        robot.leftDrive.setPower(0);\n        robot.rightDrive.setPower(0);\n        compass.setMode(CompassSensor.CompassMode.MEASUREMENT_MODE);\n        telemetry.addData(\"Compass\", \"Returning to measurement mode\");\n        telemetry.update();\n\n        sleep(HOLD_TIME_MS);  // Just do a sleep while we switch modes\n\n        // Report whether the Calibration was successful or not.\n        if (compass.calibrationFailed())\n            telemetry.addData(\"Compass\", \"Calibrate Failed. Try Again!\");\n        else\n            telemetry.addData(\"Compass\", \"Calibrate Passed.\");\n        telemetry.update();\n    }\n}\n",
    "ConceptTensorFlowObjectDetectionSwitchableCameras": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport java.util.List;\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.SwitchableCamera;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.tfod.TFObjectDetector;\nimport org.firstinspires.ftc.robotcore.external.tfod.Recognition;\n\n/**\n * This 2020-2021 OpMode illustrates the basics of using the TensorFlow Object Detection API to\n * determine the position of the Freight Frenzy game elements.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n  /* Note: This sample uses the all-objects Tensor Flow model (FreightFrenzy_BCDM.tflite), which contains\n   * the following 4 detectable objects\n   *  0: Ball,\n   *  1: Cube,\n   *  2: Duck,\n   *  3: Marker (duck location tape marker)\n   *\n   *  Two additional model assets are available which only contain a subset of the objects:\n   *  FreightFrenzy_BC.tflite  0: Ball,  1: Cube\n   *  FreightFrenzy_DM.tflite  0: Duck,  1: Marker\n   */\n    private static final String TFOD_MODEL_ASSET = \"FreightFrenzy_BCDM.tflite\";\n    private static final String[] LABELS = {\n      \"Ball\",\n      \"Cube\",\n      \"Duck\",\n      \"Marker\"\n    };\n\n    /*\n     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n     * web site at https://developer.vuforia.com/license-manager.\n     *\n     * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n     * random data. As an example, here is a example of a fragment of a valid key:\n     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n     * Once you've obtained a license key, copy the string from the Vuforia web site\n     * and paste it in to your code on the next line, between the double quotes.\n     */\n    private static final String VUFORIA_KEY =\n            \" -- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n    /**\n     * {@link #vuforia} is the variable we will use to store our instance of the Vuforia\n     * localization engine.\n     */\n    private VuforiaLocalizer vuforia;\n\n    /**\n     * Variables used for switching cameras.\n     */\n    private WebcamName webcam1, webcam2;\n    private SwitchableCamera switchableCamera;\n    private boolean oldLeftBumper;\n    private boolean oldRightBumper;\n\n    /**\n     * {@link #tfod} is the variable we will use to store our instance of the TensorFlow Object\n     * Detection engine.\n     */\n    private TFObjectDetector tfod;\n\n    @Override\n    public void runOpMode() {\n        // The TFObjectDetector uses the camera frames from the VuforiaLocalizer, so we create that\n        // first.\n        initVuforia();\n        initTfod();\n\n        /**\n         * Activate TensorFlow Object Detection before we wait for the start command.\n         * Do it here so that the Camera Stream window will have the TensorFlow annotations visible.\n         **/\n        if (tfod != null) {\n            tfod.activate();\n\n            // The TensorFlow software will scale the input images from the camera to a lower resolution.\n            // This can result in lower detection accuracy at longer distances (> 55cm or 22\").\n            // If your target is at distance greater than 50 cm (20\") you can adjust the magnification value\n            // to artificially zoom in to the center of image.  For best results, the \"aspectRatio\" argument\n            // should be set to the value of the images used to create the TensorFlow Object Detection model\n            // (typically 16/9).\n            tfod.setZoom(2.5, 16.0/9.0);\n        }\n\n        /** Wait for the game to begin */\n        telemetry.addData(\">\", \"Press Play to start op mode\");\n        telemetry.update();\n        waitForStart();\n\n        if (opModeIsActive()) {\n            while (opModeIsActive()) {\n                if (tfod != null) {\n                    doCameraSwitching();\n                    List<Recognition> recognitions = tfod.getRecognitions();\n                    telemetry.addData(\"# Object Detected\", recognitions.size());\n                    // step through the list of recognitions and display boundary info.\n                    int i = 0;\n                    for (Recognition recognition : recognitions) {\n                      telemetry.addData(String.format(\"label (%d)\", i), recognition.getLabel());\n                      telemetry.addData(String.format(\"  left,top (%d)\", i), \"%.03f , %.03f\",\n                              recognition.getLeft(), recognition.getTop());\n                      telemetry.addData(String.format(\"  right,bottom (%d)\", i), \"%.03f , %.03f\",\n                              recognition.getRight(), recognition.getBottom());\n                      i++;\n                    }\n                    telemetry.update();\n                }\n            }\n        }\n    }\n\n    /**\n     * Initialize the Vuforia localization engine.\n     */\n    private void initVuforia() {\n        /*\n         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.\n         */\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        parameters.vuforiaLicenseKey = VUFORIA_KEY;\n\n        // Indicate that we wish to be able to switch cameras.\n        webcam1 = hardwareMap.get(WebcamName.class, \"Webcam 1\");\n        webcam2 = hardwareMap.get(WebcamName.class, \"Webcam 2\");\n        parameters.cameraName = ClassFactory.getInstance().getCameraManager().nameForSwitchableCamera(webcam1, webcam2);\n\n        //  Instantiate the Vuforia engine\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        // Set the active camera to Webcam 1.\n        switchableCamera = (SwitchableCamera) vuforia.getCamera();\n        switchableCamera.setActiveCamera(webcam1);\n\n        // Loading trackables is not necessary for the TensorFlow Object Detection engine.\n    }\n\n    /**\n     * Initialize the TensorFlow Object Detection engine.\n     */\n    private void initTfod() {\n        int tfodMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\n            \"tfodMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        TFObjectDetector.Parameters tfodParameters = new TFObjectDetector.Parameters(tfodMonitorViewId);\n        tfodParameters.minResultConfidence = 0.8f;\n        tfodParameters.isModelTensorFlow2 = true;\n        tfodParameters.inputSize = 320;\n        tfod = ClassFactory.getInstance().createTFObjectDetector(tfodParameters, vuforia);\n        tfod.loadModelFromAsset(TFOD_MODEL_ASSET, LABELS);\n    }\n\n    private void doCameraSwitching() {\n        // If the left bumper is pressed, use Webcam 1.\n        // If the right bumper is pressed, use Webcam 2.\n        boolean newLeftBumper = gamepad1.left_bumper;\n        boolean newRightBumper = gamepad1.right_bumper;\n        if (newLeftBumper && !oldLeftBumper) {\n            switchableCamera.setActiveCamera(webcam1);\n        } else if (newRightBumper && !oldRightBumper) {\n            switchableCamera.setActiveCamera(webcam2);\n        }\n        oldLeftBumper = newLeftBumper;\n        oldRightBumper = newRightBumper;\n\n        if (switchableCamera.getActiveCamera().equals(webcam1)) {\n            telemetry.addData(\"activeCamera\", \"Webcam 1\");\n            telemetry.addData(\"Press RightBumper\", \"to switch to Webcam 2\");\n        } else {\n            telemetry.addData(\"activeCamera\", \"Webcam 2\");\n            telemetry.addData(\"Press LeftBumper\", \"to switch to Webcam 1\");\n        }\n    }\n}\n",
    "ConceptDIMAsIndicator": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DeviceInterfaceModule;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\n/**\n * This OpMode illustrates using the Device Interface Module as a signalling device.\n * The code is structured as a LinearOpMode\n *\n * This code assumes a DIM name \"dim\".\n *\n * There are many examples where the robot might like to signal the driver, without requiring them\n * to look at the driver station.  This might be something like a \"ball in hopper\" condition or a\n * \"ready to shoot\" condition.\n *\n * The DIM has two user settable indicator LEDs (one red one blue).  These can be controlled\n * directly from your program.\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    static final int    BLUE_LED    = 0;     // Blue LED channel on DIM\n    static final int    RED_LED     = 1;     // Red LED Channel on DIM\n\n    // Create timer to toggle LEDs\n    private ElapsedTime runtime = new ElapsedTime();\n\n    // Define class REPLACE_CLASS\n    DeviceInterfaceModule   dim;\n\n    @Override\n    public void runOpMode() {\n\n        // Connect to motor (Assume standard left wheel)\n        // Change the text in quotes to match any motor name on your robot.\n        dim = hardwareMap.get(DeviceInterfaceModule.class, \"dim\");\n\n        // Toggle LEDs while Waiting for the start button\n        telemetry.addData(\">\", \"Press Play to test LEDs.\" );\n        telemetry.update();\n\n        while (!isStarted()) {\n            // Determine if we are on an odd or even second\n            boolean even = (((int)(runtime.time()) & 0x01) == 0);\n            dim.setLED(RED_LED,   even); // Red for even\n            dim.setLED(BLUE_LED, !even); // Blue for odd\n            idle();\n        }\n\n        // Running now\n        telemetry.addData(\">\", \"Press X for Blue, B for Red.\" );\n        telemetry.update();\n\n        // Now just use red and blue buttons to set red and blue LEDs\n        while(opModeIsActive()){\n            dim.setLED(BLUE_LED, gamepad1.x);\n            dim.setLED(RED_LED,  gamepad1.b);\n            idle();\n        }\n\n        // Turn off LEDs;\n        dim.setLED(BLUE_LED, false);\n        dim.setLED(RED_LED,  false);\n        telemetry.addData(\">\", \"Done\");\n        telemetry.update();\n    }\n}\n",
    "ConceptNullOp": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.OpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\nimport java.text.SimpleDateFormat;\nimport java.util.Date;\n\n/**\n * Demonstrates empty OpMode\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends OpMode {\n\n  private ElapsedTime runtime = new ElapsedTime();\n\n  @Override\n  public void init() {\n    telemetry.addData(\"Status\", \"Initialized\");\n  }\n\n  /*\n     * Code to run when the op mode is first enabled goes here\n     * @see com.qualcomm.robotcore.eventloop.opmode.OpMode#start()\n     */\n  @Override\n  public void init_loop() {\n  }\n\n  /*\n   * This method will be called ONCE when start is pressed\n   * @see com.qualcomm.robotcore.eventloop.opmode.OpMode#loop()\n   */\n  @Override\n  public void start() {\n    runtime.reset();\n  }\n\n  /*\n   * This method will be called repeatedly in a loop\n   * @see com.qualcomm.robotcore.eventloop.opmode.OpMode#loop()\n   */\n  @Override\n  public void loop() {\n    telemetry.addData(\"Status\", \"Run Time: \" + runtime.toString());\n  }\n}\n",
    "ConceptMotorBulkRead": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.lynx.LynxModule;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DcMotorEx;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\nimport java.util.Iterator;\nimport java.util.List;\n\n    /*\n        This sample illustrates how to use the Expansion Hub's Bulk-Read feature to speed up control cycle times.\n        In this example there are 4 motors that need their encoder positions, and velocities read.\n        The sample is written to work with one or two expansion hubs, with no assumption as to where the motors are located.\n\n        Three scenarios are tested:\n        Cache Mode = OFF    This is the normal default, where no cache is used, and every read produces a discrete transaction with\n                            an expansion hub, which is the slowest approach.\n        Cache Mode = AUTO   This mode will attempt to minimize the number of discrete read commands, by performing bulk-reads\n                            and then returning values that have been cached.  The cache is updated automatically whenever a specific read operation is repeated.\n                            This mode will always return fresh data, but it may perform more bulk-reads than absolutely required.\n                            Extra reads will be performed if multiple identical encoder/velocity reads are performed in one control cycle.\n                            This mode is a good compromise between the OFF and MANUAL modes.\n        Cache Mode = MANUAL This mode enables the user's code to determine the best time to refresh the cached bulk-read data.\n                            Well organized code can place all the sensor reads in one location, and then just reset the cache once per control cycle.\n                            The approach will produce the shortest cycle times, but it does require the user to manually clear the cache.\n\n        -------------------------------------\n\n        General tip to speed up your control cycles:\n        No matter what method you use to read encoders and other inputs, you should try to\n        avoid reading the same input multiple times around a control loop.\n        Under normal conditions, this will slow down the control loop.\n        The preferred method is to read all the required inputs ONCE at the beginning of the loop,\n        and save the values in variable that can be used by other parts of the control code.\n\n        eg: if you are sending encoder positions to your telemetry display, putting a getCurrentPosition()\n        call in the telemetry statement will force the code to go and get another copy which will take time.\n        It's much better read the position into a variable once, and use that variable for control AND display.\n        Reading saved variables takes no time at all.\n\n        Once you put all your sensor reads at the beginning of the control cycle, it's very easy to use\n        the bulk-read AUTO mode to streamline your cycle timing.\n\n     */\n@TeleOp (name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    final int       TEST_CYCLES    = 500;   // Number of control cycles to run to determine cycle times.\n\n    private DcMotorEx m1, m2, m3, m4; // Motor Objects\n    private long      e1, e2, e3, e4; // Encoder Values\n    private double    v1, v2, v3, v4; // Velocities\n\n    // Cycle Times\n    double t1 = 0;\n    double t2 = 0;\n    double t3 = 0;\n\n    @Override\n    public void runOpMode() {\n\n        int cycles;\n\n        // Important Step 1:  Make sure you use DcMotorEx when you instantiate your motors.\n        m1 = hardwareMap.get(DcMotorEx.class, \"m1\");  // Configure the robot to use these 4 motor names,\n        m2 = hardwareMap.get(DcMotorEx.class, \"m2\");  // or change these strings to match your existing Robot Configuration.\n        m3 = hardwareMap.get(DcMotorEx.class, \"m3\");\n        m4 = hardwareMap.get(DcMotorEx.class, \"m4\");\n\n        // Important Step 2: Get access to a list of Expansion Hub Modules to enable changing caching methods.\n        List<LynxModule> allHubs = hardwareMap.getAll(LynxModule.class);\n\n        ElapsedTime timer = new ElapsedTime();\n\n        telemetry.addData(\">\", \"Press play to start tests\");\n        telemetry.addData(\">\", \"Test results will update for each access method.\");\n        telemetry.update();\n        waitForStart();\n\n        // --------------------------------------------------------------------------------------\n        // Run control loop using legacy encoder reads\n        // In this mode, a single read is done for each encoder position, and a bulk read is done for each velocity read.\n        // This is the worst case scenario.\n        // This is the same as using LynxModule.BulkCachingMode.OFF\n        // --------------------------------------------------------------------------------------\n\n        displayCycleTimes(\"Test 1 of 3 (Wait for completion)\");\n\n        timer.reset();\n        cycles = 0;\n        while (opModeIsActive() && (cycles++ < TEST_CYCLES)) {\n            e1 = m1.getCurrentPosition();\n            e2 = m2.getCurrentPosition();\n            e3 = m3.getCurrentPosition();\n            e4 = m4.getCurrentPosition();\n\n            v1 = m1.getVelocity();\n            v2 = m2.getVelocity();\n            v3 = m3.getVelocity();\n            v4 = m4.getVelocity();\n\n            // Put Control loop action code here.\n\n        }\n        // calculate the average cycle time.\n        t1 = timer.milliseconds() / cycles;\n        displayCycleTimes(\"Test 2 of 3 (Wait for completion)\");\n\n        // --------------------------------------------------------------------------------------\n        // Run test cycles using AUTO cache mode\n        // In this mode, only one bulk read is done per cycle, UNLESS you read a specific encoder/velocity item AGAIN in that cycle.\n        // --------------------------------------------------------------------------------------\n\n        // Important Step 3: Option A. Set all Expansion hubs to use the AUTO Bulk Caching mode\n        for (LynxModule module : allHubs) {\n            module.setBulkCachingMode(LynxModule.BulkCachingMode.AUTO);\n        }\n\n        timer.reset();\n        cycles = 0;\n        while (opModeIsActive() && (cycles++ < TEST_CYCLES)) {\n            e1 = m1.getCurrentPosition();  // Uses 1 bulk-read for all 4 encoder/velocity reads,\n            e2 = m2.getCurrentPosition();  // but don't do any `get` operations more than once per cycle.\n            e3 = m3.getCurrentPosition();\n            e4 = m4.getCurrentPosition();\n\n            v1 = m1.getVelocity();\n            v2 = m2.getVelocity();\n            v3 = m3.getVelocity();\n            v4 = m4.getVelocity();\n\n            // Put Control loop action code here.\n\n        }\n        // calculate the average cycle time.\n        t2 = timer.milliseconds() / cycles;\n        displayCycleTimes(\"Test 3 of 3 (Wait for completion)\");\n\n        // --------------------------------------------------------------------------------------\n        // Run test cycles using MANUAL cache mode\n        // In this mode, only one block read is done each control cycle.\n        // This is the MOST efficient method, but it does require that the cache is cleared manually each control cycle.\n        // --------------------------------------------------------------------------------------\n\n        // Important Step 3: Option B. Set all Expansion hubs to use the MANUAL Bulk Caching mode\n        for (LynxModule module : allHubs) {\n            module.setBulkCachingMode(LynxModule.BulkCachingMode.MANUAL);\n        }\n\n        timer.reset();\n        cycles = 0;\n        while (opModeIsActive() && (cycles++ < TEST_CYCLES)) {\n\n            // Important Step 4: If you are using MANUAL mode, you must clear the BulkCache once per control cycle\n            for (LynxModule module : allHubs) {\n                module.clearBulkCache();\n            }\n\n            e1 = m1.getCurrentPosition();   // Uses 1 bulk-read to obtain ALL the motor data\n            e2 = m2.getCurrentPosition();   // There is no penalty for doing more `get` operations in this cycle,\n            e3 = m3.getCurrentPosition();   // but they will return the same data.\n            e4 = m4.getCurrentPosition();\n\n            v1 = m1.getVelocity();\n            v2 = m2.getVelocity();\n            v3 = m3.getVelocity();\n            v4 = m4.getVelocity();\n\n            // Put Control loop action code here.\n\n        }\n        // calculate the average cycle time.\n        t3 = timer.milliseconds() / cycles;\n        displayCycleTimes(\"Complete\");\n\n        // wait until op-mode is stopped by user, before clearing display.\n        while (opModeIsActive()) ;\n    }\n\n    // Display three comparison times.\n    void displayCycleTimes(String status) {\n        telemetry.addData(\"Testing\", status);\n        telemetry.addData(\"Cache = OFF\",    \"%5.1f mS/cycle\", t1);\n        telemetry.addData(\"Cache = AUTO\",   \"%5.1f mS/cycle\", t2);\n        telemetry.addData(\"Cache = MANUAL\", \"%5.1f mS/cycle\", t3);\n        telemetry.update();\n    }\n}\n\n",
    "HardwarePushbot": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.hardware.DcMotor;\nimport com.qualcomm.robotcore.hardware.HardwareMap;\nimport com.qualcomm.robotcore.hardware.Servo;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\n/**\n * This is NOT an opmode.\n *\n * This class REPLACE_CLASS be used to define all the specific hardware for a single robot.\n * In this case that robot is a Pushbot.\n * See PushbotTeleopTank_Iterative and others classes starting with \"Pushbot\" for usage examples.\n *\n * This hardware class REPLACE_CLASS the following device names have been configured on the robot:\n * Note:  All names are lower case and some have single spaces between words.\n *\n * Motor channel:  Left  drive motor:        \"left_drive\"\n * Motor channel:  Right drive motor:        \"right_drive\"\n * Motor channel:  Manipulator drive motor:  \"left_arm\"\n * Servo channel:  Servo to open left claw:  \"left_hand\"\n * Servo channel:  Servo to open right claw: \"right_hand\"\n */\npublic class REPLACE_CLASS\n{\n    /* Public OpMode members. */\n    public DcMotor  leftDrive   = null;\n    public DcMotor  rightDrive  = null;\n    public DcMotor  leftArm     = null;\n    public Servo    leftClaw    = null;\n    public Servo    rightClaw   = null;\n\n    public static final double MID_SERVO       =  0.5 ;\n    public static final double ARM_UP_POWER    =  0.45 ;\n    public static final double ARM_DOWN_POWER  = -0.45 ;\n\n    /* local OpMode members. */\n    HardwareMap hwMap           =  null;\n    private ElapsedTime period  = new ElapsedTime();\n\n    /* Constructor */\n    public HardwarePushbot(){\n\n    }\n\n    /* Initialize standard Hardware interfaces */\n    public void init(HardwareMap ahwMap) {\n        // Save reference to Hardware map\n        hwMap = ahwMap;\n\n        // Define and Initialize Motors\n        leftDrive  = hwMap.get(DcMotor.class, \"left_drive\");\n        rightDrive = hwMap.get(DcMotor.class, \"right_drive\");\n        leftArm    = hwMap.get(DcMotor.class, \"left_arm\");\n        leftDrive.setDirection(DcMotor.Direction.FORWARD); // Set to REVERSE if using AndyMark motors\n        rightDrive.setDirection(DcMotor.Direction.REVERSE);// Set to FORWARD if using AndyMark motors\n\n        // Set all motors to zero power\n        leftDrive.setPower(0);\n        rightDrive.setPower(0);\n        leftArm.setPower(0);\n\n        // Set all motors to run without encoders.\n        // May want to use RUN_USING_ENCODERS if encoders are installed.\n        leftDrive.setMode(DcMotor.RunMode.RUN_WITHOUT_ENCODER);\n        rightDrive.setMode(DcMotor.RunMode.RUN_WITHOUT_ENCODER);\n        leftArm.setMode(DcMotor.RunMode.RUN_WITHOUT_ENCODER);\n\n        // Define and initialize ALL installed servos.\n        leftClaw  = hwMap.get(Servo.class, \"left_hand\");\n        rightClaw = hwMap.get(Servo.class, \"right_hand\");\n        leftClaw.setPosition(MID_SERVO);\n        rightClaw.setPosition(MID_SERVO);\n    }\n }\n\n",
    "ConceptRevSPARKMini": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DcMotorSimple;\nimport com.qualcomm.robotcore.util.ElapsedTime;\nimport com.qualcomm.robotcore.util.Range;\n\n\n/**\n *\n * This OpMode executes a basic Tank Drive Teleop for a two wheeled robot using two REV SPARKminis.\n * To use this example, connect two REV SPARKminis into servo ports on the Expansion Hub. On the\n * robot configuration, use the drop down list under 'Servos' to select 'REV SPARKmini Controller'\n * and name them 'left_drive' and 'right_drive'.\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    // Declare OpMode members.\n    private ElapsedTime runtime = new ElapsedTime();\n    private DcMotorSimple leftDrive = null;\n    private DcMotorSimple rightDrive = null;\n\n    @Override\n    public void runOpMode() {\n        telemetry.addData(\"Status\", \"Initialized\");\n        telemetry.update();\n\n        // Initialize the hardware variables. Note that the strings used here as parameters\n        // to 'get' must correspond to the names assigned during the robot configuration\n        // step (using the FTC Robot Controller app on the phone).\n        leftDrive  = hardwareMap.get(DcMotorSimple.class, \"left_drive\");\n        rightDrive = hardwareMap.get(DcMotorSimple.class, \"right_drive\");\n\n        // Most robots need the motor on one side to be reversed to drive forward\n        // Reverse the motor that runs backwards when connected directly to the battery\n        leftDrive.setDirection(DcMotorSimple.Direction.FORWARD);\n        rightDrive.setDirection(DcMotorSimple.Direction.REVERSE);\n\n        // Wait for the game to start (driver presses PLAY)\n        waitForStart();\n        runtime.reset();\n\n        // run until the end of the match (driver presses STOP)\n        while (opModeIsActive()) {\n\n            // Setup a variable for each drive wheel to save power level for telemetry\n            double leftPower;\n            double rightPower;\n\n            // Choose to drive using either Tank Mode, or POV Mode\n            // Comment out the method that's not used.  The default below is POV.\n\n            // POV Mode uses left stick to go forward, and right stick to turn.\n            // - This uses basic math to combine motions and is easier to drive straight.\n            double drive = -gamepad1.left_stick_y;\n            double turn  =  gamepad1.right_stick_x;\n            leftPower    = Range.clip(drive + turn, -1.0, 1.0) ;\n            rightPower   = Range.clip(drive - turn, -1.0, 1.0) ;\n\n            // Tank Mode uses one stick to control each wheel.\n            // - This requires no math, but it is hard to drive forward slowly and keep straight.\n            // leftPower  = -gamepad1.left_stick_y ;\n            // rightPower = -gamepad1.right_stick_y ;\n\n            // Send calculated power to wheels\n            leftDrive.setPower(leftPower);\n            rightDrive.setPower(rightPower);\n\n            // Show the elapsed game time and wheel power.\n            telemetry.addData(\"Status\", \"Run Time: \" + runtime.toString());\n            telemetry.addData(\"Motors\", \"left (%.2f), right (%.2f)\", leftPower, rightPower);\n            telemetry.update();\n        }\n    }\n}\n",
    "SensorAdafruitRGB": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport android.app.Activity;\nimport android.graphics.Color;\nimport android.view.View;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.ColorSensor;\nimport com.qualcomm.robotcore.hardware.DeviceInterfaceModule;\nimport com.qualcomm.robotcore.hardware.DigitalChannel;\n\n/*\n *\n * This is an example LinearOpMode that shows how to use\n * the Adafruit RGB Sensor.  It assumes that the I2C\n * cable for the sensor is connected to an I2C port on the\n * Core Device Interface Module.\n *\n * It also assuems that the LED pin of the sensor is connected\n * to the digital signal pin of a digital port on the\n * Core Device Interface Module.\n *\n * You can use the digital port to turn the sensor's onboard\n * LED on or off.\n *\n * The op mode assumes that the Core Device Interface Module\n * is configured with a name of \"dim\" and that the Adafruit color sensor\n * is configured as an I2C device with a name of \"sensor_color\".\n *\n * It also assumes that the LED pin of the RGB sensor\n * is connected to the signal pin of digital port #5 (zero indexed)\n * of the Core Device Interface Module.\n *\n * You can use the X button on gamepad1 to toggle the LED on and off.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled                            // Comment this out to add to the opmode list\npublic class REPLACE_CLASS extends LinearOpMode {\n\n  ColorSensor sensorRGB;\n  DeviceInterfaceModule cdim;\n\n  // we assume that the LED pin of the RGB sensor is connected to\n  // digital port 5 (zero indexed).\n  static final int LED_CHANNEL = 5;\n\n  @Override\n  public void runOpMode() {\n\n    // hsvValues is an array that will hold the hue, saturation, and value information.\n    float hsvValues[] = {0F,0F,0F};\n\n    // values is a reference to the hsvValues array.\n    final float values[] = hsvValues;\n\n    // get a reference to the RelativeLayout so we can change the background\n    // color of the Robot Controller app to match the hue detected by the RGB sensor.\n    int relativeLayoutId = hardwareMap.appContext.getResources().getIdentifier(\"RelativeLayout\", \"id\", hardwareMap.appContext.getPackageName());\n    final View relativeLayout = ((Activity) hardwareMap.appContext).findViewById(relativeLayoutId);\n\n    // bPrevState and bCurrState represent the previous and current state of the button.\n    boolean bPrevState = false;\n    boolean bCurrState = false;\n\n    // bLedOn represents the state of the LED.\n    boolean bLedOn = true;\n\n    // get a reference to our DeviceInterfaceModule object.\n    cdim = hardwareMap.deviceInterfaceModule.get(\"dim\");\n\n    // set the digital channel to output mode.\n    // remember, the Adafruit sensor is actually two devices.\n    // It's an I2C sensor and it's also an LED that can be turned on or off.\n    cdim.setDigitalChannelMode(LED_CHANNEL, DigitalChannel.Mode.OUTPUT);\n\n    // get a reference to our ColorSensor object.\n    sensorRGB = hardwareMap.colorSensor.get(\"sensor_color\");\n\n    // turn the LED on in the beginning, just so user will know that the sensor is active.\n    cdim.setDigitalChannelState(LED_CHANNEL, bLedOn);\n\n    // wait for the start button to be pressed.\n    waitForStart();\n\n    // loop and read the RGB data.\n    // Note we use opModeIsActive() as our loop condition because it is an interruptible method.\n    while (opModeIsActive())  {\n\n      // check the status of the x button on gamepad.\n      bCurrState = gamepad1.x;\n\n      // check for button-press state transitions.\n      if ((bCurrState == true) && (bCurrState != bPrevState))  {\n\n        // button is transitioning to a pressed state. Toggle the LED.\n        bLedOn = !bLedOn;\n        cdim.setDigitalChannelState(LED_CHANNEL, bLedOn);\n      }\n\n      // update previous state variable.\n      bPrevState = bCurrState;\n\n      // convert the RGB values to HSV values.\n      Color.RGBToHSV((sensorRGB.red() * 255) / 800, (sensorRGB.green() * 255) / 800, (sensorRGB.blue() * 255) / 800, hsvValues);\n\n      // send the info back to driver station using telemetry function.\n      telemetry.addData(\"LED\", bLedOn ? \"On\" : \"Off\");\n      telemetry.addData(\"Clear\", sensorRGB.alpha());\n      telemetry.addData(\"Red  \", sensorRGB.red());\n      telemetry.addData(\"Green\", sensorRGB.green());\n      telemetry.addData(\"Blue \", sensorRGB.blue());\n      telemetry.addData(\"Hue\", hsvValues[0]);\n\n      // change the background color to match the color detected by the RGB sensor.\n      // pass a reference to the hue, saturation, and value array as an argument\n      // to the HSVToColor method.\n      relativeLayout.post(new Runnable() {\n        public void run() {\n          relativeLayout.setBackgroundColor(Color.HSVToColor(0xff, values));\n        }\n      });\n\n      telemetry.update();\n    }\n\n    // Set the panel back to the default color\n    relativeLayout.post(new Runnable() {\n      public void run() {\n        relativeLayout.setBackgroundColor(Color.WHITE);\n      }\n    });\n  }\n}\n",
    "ConceptVuforiaUltimateGoalNavigationWebcam": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.robotcore.external.matrices.OpenGLMatrix;\nimport org.firstinspires.ftc.robotcore.external.matrices.VectorF;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackable;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackableDefaultListener;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackables;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport static org.firstinspires.ftc.robotcore.external.navigation.AngleUnit.DEGREES;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesOrder.XYZ;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesOrder.XZY;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesReference.EXTRINSIC;\n\n/**\n * This 2020-2021 OpMode illustrates the basics of using the Vuforia localizer to determine\n * positioning and orientation of robot on the ULTIMATE GOAL FTC field.\n * The code is structured as a LinearOpMode\n *\n * When images are located, Vuforia is able to determine the position and orientation of the\n * image relative to the camera.  This sample code then combines that information with a\n * knowledge of where the target images are on the field, to determine the location of the camera.\n *\n * From the Audience perspective, the Red Alliance station is on the right and the\n * Blue Alliance Station is on the left.\n\n * There are a total of five image targets for the ULTIMATE GOAL game.\n * Three of the targets are placed in the center of the Red Alliance, Audience (Front),\n * and Blue Alliance perimeter walls.\n * Two additional targets are placed on the perimeter wall, one in front of each Tower Goal.\n * Refer to the Field Setup manual for more specific location details\n *\n * A final calculation then uses the location of the camera on the robot to determine the\n * robot's location and orientation on the field.\n *\n * @see VuforiaLocalizer\n * @see VuforiaTrackableDefaultListener\n * see  ultimategoal/doc/tutorial/FTC_FieldCoordinateSystemDefinition.pdf\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    /*\n     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n     * web site at https://developer.vuforia.com/license-manager.\n     *\n     * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n     * random data. As an example, here is a example of a fragment of a valid key:\n     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n     * Once you've obtained a license key, copy the string from the Vuforia web site\n     * and paste it in to your code on the next line, between the double quotes.\n     */\n    private static final String VUFORIA_KEY =\n            \" --- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n    // Since ImageTarget trackables use mm to specifiy their dimensions, we must use mm for all the physical dimension.\n    // We will define some constants and conversions here\n    private static final float mmPerInch        = 25.4f;\n    private static final float mmTargetHeight   = (6) * mmPerInch;          // the height of the center of the target image above the floor\n\n    // Constants for perimeter targets\n    private static final float halfField = 72 * mmPerInch;\n    private static final float quadField  = 36 * mmPerInch;\n\n    // Class Members\n    private OpenGLMatrix lastLocation = null;\n    private VuforiaLocalizer vuforia = null;\n\n    /**\n     * This is the webcam we are to use. As with other hardware devices such as motors and\n     * servos, this device is identified using the robot configuration tool in the FTC application.\n     */\n    WebcamName webcamName = null;\n\n    private boolean targetVisible = false;\n    private float phoneXRotate    = 0;\n    private float phoneYRotate    = 0;\n    private float phoneZRotate    = 0;\n\n    @Override public void runOpMode() {\n        /*\n         * Retrieve the camera we are to use.\n         */\n        webcamName = hardwareMap.get(WebcamName.class, \"Webcam 1\");\n\n        /*\n         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.\n         * We can pass Vuforia the handle to a camera preview resource (on the RC screen);\n         * If no camera monitor is desired, use the parameter-less constructor instead (commented out below).\n         * Note: A preview window is required if you want to view the camera stream on the Driver Station Phone.\n         */\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters(cameraMonitorViewId);\n\n        // VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        parameters.vuforiaLicenseKey = VUFORIA_KEY;\n\n        /**\n         * We also indicate which camera on the RC we wish to use.\n         */\n        parameters.cameraName = webcamName;\n\n        // Make sure extended tracking is disabled for this example.\n        parameters.useExtendedTracking = false;\n\n        //  Instantiate the Vuforia engine\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        // Load the data sets for the trackable objects. These particular data\n        // sets are stored in the 'assets' part of our application.\n        VuforiaTrackables targetsUltimateGoal = this.vuforia.loadTrackablesFromAsset(\"UltimateGoal\");\n        VuforiaTrackable blueTowerGoalTarget = targetsUltimateGoal.get(0);\n        blueTowerGoalTarget.setName(\"Blue Tower Goal Target\");\n        VuforiaTrackable redTowerGoalTarget = targetsUltimateGoal.get(1);\n        redTowerGoalTarget.setName(\"Red Tower Goal Target\");\n        VuforiaTrackable redAllianceTarget = targetsUltimateGoal.get(2);\n        redAllianceTarget.setName(\"Red Alliance Target\");\n        VuforiaTrackable blueAllianceTarget = targetsUltimateGoal.get(3);\n        blueAllianceTarget.setName(\"Blue Alliance Target\");\n        VuforiaTrackable frontWallTarget = targetsUltimateGoal.get(4);\n        frontWallTarget.setName(\"Front Wall Target\");\n\n        // For convenience, gather together all the trackable objects in one easily-iterable collection */\n        List<VuforiaTrackable> allTrackables = new ArrayList<VuforiaTrackable>();\n        allTrackables.addAll(targetsUltimateGoal);\n\n        /**\n         * In order for localization to work, we need to tell the system where each target is on the field, and\n         * where the phone resides on the robot.  These specifications are in the form of <em>transformation matrices.</em>\n         * Transformation matrices are a central, important concept in the math here involved in localization.\n         * See <a href=\"https://en.wikipedia.org/wiki/Transformation_matrix\">Transformation Matrix</a>\n         * for detailed information. Commonly, you'll encounter transformation matrices as instances\n         * of the {@link OpenGLMatrix} class.\n         *\n         * If you are standing in the Red Alliance Station looking towards the center of the field,\n         *     - The X axis runs from your left to the right. (positive from the center to the right)\n         *     - The Y axis runs from the Red Alliance Station towards the other side of the field\n         *       where the Blue Alliance Station is. (Positive is from the center, towards the BlueAlliance station)\n         *     - The Z axis runs from the floor, upwards towards the ceiling.  (Positive is above the floor)\n         *\n         * Before being transformed, each target image is conceptually located at the origin of the field's\n         *  coordinate system (the center of the field), facing up.\n         */\n\n        //Set the position of the perimeter targets with relation to origin (center of field)\n        redAllianceTarget.setLocation(OpenGLMatrix\n                .translation(0, -halfField, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0, 180)));\n\n        blueAllianceTarget.setLocation(OpenGLMatrix\n                .translation(0, halfField, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0, 0)));\n        frontWallTarget.setLocation(OpenGLMatrix\n                .translation(-halfField, 0, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0, 90)));\n\n        // The tower goal targets are located a quarter field length from the ends of the back perimeter wall.\n        blueTowerGoalTarget.setLocation(OpenGLMatrix\n                .translation(halfField, quadField, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0, -90)));\n        redTowerGoalTarget.setLocation(OpenGLMatrix\n                .translation(halfField, -quadField, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0, -90)));\n\n        //\n        // Create a transformation matrix describing where the phone is on the robot.\n        //\n        // Info:  The coordinate frame for the robot looks the same as the field.\n        // The robot's \"forward\" direction is facing out along X axis, with the LEFT side facing out along the Y axis.\n        // Z is UP on the robot.  This equates to a bearing angle of Zero degrees.\n        //\n        // For a WebCam, the default starting orientation of the camera is looking UP (pointing in the Z direction),\n        // with the wide (horizontal) axis of the camera aligned with the X axis, and\n        // the Narrow (vertical) axis of the camera aligned with the Y axis\n        //\n        // But, this example assumes that the camera is actually facing forward out the front of the robot.\n        // So, the \"default\" camera position requires two rotations to get it oriented correctly.\n        // 1) First it must be rotated +90 degrees around the X axis to get it horizontal (it's now facing out the right side of the robot)\n        // 2) Next it must be be rotated +90 degrees (counter-clockwise) around the Z axis to face forward.\n        //\n        // Finally the camera can be translated to its actual mounting position on the robot.\n        //      In this example, it is centered (left to right), but 4\" forward of the middle of the robot, and 8\" above ground level.\n\n        final float CAMERA_FORWARD_DISPLACEMENT  = 4.0f * mmPerInch;   // eg: Camera is 4 Inches in front of robot-center\n        final float CAMERA_VERTICAL_DISPLACEMENT = 8.0f * mmPerInch;   // eg: Camera is 8 Inches above ground\n        final float CAMERA_LEFT_DISPLACEMENT     = 0;     // eg: Camera is ON the robot's center line\n\n        OpenGLMatrix cameraLocationOnRobot = OpenGLMatrix\n                    .translation(CAMERA_FORWARD_DISPLACEMENT, CAMERA_LEFT_DISPLACEMENT, CAMERA_VERTICAL_DISPLACEMENT)\n                    .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XZY, DEGREES, 90, 90, 0));\n\n        /**  Let all the trackable listeners know where the phone is.  */\n        for (VuforiaTrackable trackable : allTrackables) {\n            ((VuforiaTrackableDefaultListener) trackable.getListener()).setCameraLocationOnRobot(parameters.cameraName, cameraLocationOnRobot);\n        }\n\n        // WARNING:\n        // In this sample, we do not wait for PLAY to be pressed.  Target Tracking is started immediately when INIT is pressed.\n        // This sequence is used to enable the new remote DS Camera Preview feature to be used with this sample.\n        // CONSEQUENTLY do not put any driving commands in this loop.\n        // To restore the normal opmode structure, just un-comment the following line:\n\n        // waitForStart();\n\n        // Note: To use the remote camera preview:\n        // AFTER you hit Init on the Driver Station, use the \"options menu\" to select \"Camera Stream\"\n        // Tap the preview window to receive a fresh image.\n\n        targetsUltimateGoal.activate();\n        while (!isStopRequested()) {\n\n            // check all the trackable targets to see which one (if any) is visible.\n            targetVisible = false;\n            for (VuforiaTrackable trackable : allTrackables) {\n                if (((VuforiaTrackableDefaultListener)trackable.getListener()).isVisible()) {\n                    telemetry.addData(\"Visible Target\", trackable.getName());\n                    targetVisible = true;\n\n                    // getUpdatedRobotLocation() will return null if no new information is available since\n                    // the last time that call was made, or if the trackable is not currently visible.\n                    OpenGLMatrix robotLocationTransform = ((VuforiaTrackableDefaultListener)trackable.getListener()).getUpdatedRobotLocation();\n                    if (robotLocationTransform != null) {\n                        lastLocation = robotLocationTransform;\n                    }\n                    break;\n                }\n            }\n\n            // Provide feedback as to where the robot is located (if we know).\n            if (targetVisible) {\n                // express position (translation) of robot in inches.\n                VectorF translation = lastLocation.getTranslation();\n                telemetry.addData(\"Pos (in)\", \"{X, Y, Z} = %.1f, %.1f, %.1f\",\n                        translation.get(0) / mmPerInch, translation.get(1) / mmPerInch, translation.get(2) / mmPerInch);\n\n                // express the rotation of the robot in degrees.\n                Orientation rotation = Orientation.getOrientation(lastLocation, EXTRINSIC, XYZ, DEGREES);\n                telemetry.addData(\"Rot (deg)\", \"{Roll, Pitch, Heading} = %.0f, %.0f, %.0f\", rotation.firstAngle, rotation.secondAngle, rotation.thirdAngle);\n            }\n            else {\n                telemetry.addData(\"Visible Target\", \"none\");\n            }\n            telemetry.update();\n        }\n\n        // Disable Tracking when we are done;\n        targetsUltimateGoal.deactivate();\n    }\n}\n",
    "BasicOpMode Iterative": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.OpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DcMotor;\nimport com.qualcomm.robotcore.util.ElapsedTime;\nimport com.qualcomm.robotcore.util.Range;\n\n/**\n * This file contains an example of an iterative (Non-Linear) \"OpMode\".\n * An OpMode is a 'program' that runs in either the autonomous or the teleop period of an FTC match.\n * The names of OpModes appear on the menu of the FTC Driver Station.\n * When an selection is made from the menu, the corresponding OpMode\n * class REPLACE_CLASS instantiated on the Robot Controller and executed.\n *\n * This particular OpMode just executes a basic Tank Drive Teleop for a two wheeled robot\n * It includes all the skeletal structure that all iterative OpModes contain.\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends OpMode\n{\n    // Declare OpMode members.\n    private ElapsedTime runtime = new ElapsedTime();\n    private DcMotor leftDrive = null;\n    private DcMotor rightDrive = null;\n\n    /*\n     * Code to run ONCE when the driver hits INIT\n     */\n    @Override\n    public void init() {\n        telemetry.addData(\"Status\", \"Initialized\");\n\n        // Initialize the hardware variables. Note that the strings used here as parameters\n        // to 'get' must correspond to the names assigned during the robot configuration\n        // step (using the FTC Robot Controller app on the phone).\n        leftDrive  = hardwareMap.get(DcMotor.class, \"left_drive\");\n        rightDrive = hardwareMap.get(DcMotor.class, \"right_drive\");\n\n        // Most robots need the motor on one side to be reversed to drive forward\n        // Reverse the motor that runs backwards when connected directly to the battery\n        leftDrive.setDirection(DcMotor.Direction.FORWARD);\n        rightDrive.setDirection(DcMotor.Direction.REVERSE);\n\n        // Tell the driver that initialization is complete.\n        telemetry.addData(\"Status\", \"Initialized\");\n    }\n\n    /*\n     * Code to run REPEATEDLY after the driver hits INIT, but before they hit PLAY\n     */\n    @Override\n    public void init_loop() {\n    }\n\n    /*\n     * Code to run ONCE when the driver hits PLAY\n     */\n    @Override\n    public void start() {\n        runtime.reset();\n    }\n\n    /*\n     * Code to run REPEATEDLY after the driver hits PLAY but before they hit STOP\n     */\n    @Override\n    public void loop() {\n        // Setup a variable for each drive wheel to save power level for telemetry\n        double leftPower;\n        double rightPower;\n\n        // Choose to drive using either Tank Mode, or POV Mode\n        // Comment out the method that's not used.  The default below is POV.\n\n        // POV Mode uses left stick to go forward, and right stick to turn.\n        // - This uses basic math to combine motions and is easier to drive straight.\n        double drive = -gamepad1.left_stick_y;\n        double turn  =  gamepad1.right_stick_x;\n        leftPower    = Range.clip(drive + turn, -1.0, 1.0) ;\n        rightPower   = Range.clip(drive - turn, -1.0, 1.0) ;\n\n        // Tank Mode uses one stick to control each wheel.\n        // - This requires no math, but it is hard to drive forward slowly and keep straight.\n        // leftPower  = -gamepad1.left_stick_y ;\n        // rightPower = -gamepad1.right_stick_y ;\n\n        // Send calculated power to wheels\n        leftDrive.setPower(leftPower);\n        rightDrive.setPower(rightPower);\n\n        // Show the elapsed game time and wheel power.\n        telemetry.addData(\"Status\", \"Run Time: \" + runtime.toString());\n        telemetry.addData(\"Motors\", \"left (%.2f), right (%.2f)\", leftPower, rightPower);\n    }\n\n    /*\n     * Code to run ONCE after the driver hits STOP\n     */\n    @Override\n    public void stop() {\n    }\n\n}\n",
    "ConceptVuforiaNavigationWebcam": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport android.graphics.Bitmap;\n\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.util.RobotLog;\nimport com.qualcomm.robotcore.util.ThreadPool;\nimport com.vuforia.Frame;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.function.Consumer;\nimport org.firstinspires.ftc.robotcore.external.function.Continuation;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.robotcore.external.matrices.MatrixF;\nimport org.firstinspires.ftc.robotcore.external.matrices.OpenGLMatrix;\nimport org.firstinspires.ftc.robotcore.external.navigation.AngleUnit;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesOrder;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesReference;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackable;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackableDefaultListener;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackables;\nimport org.firstinspires.ftc.robotcore.internal.system.AppUtil;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.ArrayList;\nimport java.util.List;\nimport java.util.Locale;\n\n/**\n * This 2016-2017 OpMode illustrates the basics of using the Vuforia localizer to determine\n * positioning and orientation of robot on the FTC field.\n * The code is structured as a LinearOpMode\n *\n * Vuforia uses the phone's camera to inspect it's surroundings, and attempt to locate target images.\n *\n * When images are located, Vuforia is able to determine the position and orientation of the\n * image relative to the camera.  This sample code than combines that information with a\n * knowledge of where the target images are on the field, to determine the location of the camera.\n *\n * This example assumes a \"diamond\" field configuration where the red and blue alliance stations\n * are adjacent on the corner of the field furthest from the audience.\n * From the Audience perspective, the Red driver station is on the right.\n * The two vision target are located on the two walls closest to the audience, facing in.\n * The Stones are on the RED side of the field, and the Chips are on the Blue side.\n *\n * A final calculation then uses the location of the camera on the robot to determine the\n * robot's location and orientation on the field.\n *\n * @see VuforiaLocalizer\n * @see VuforiaTrackableDefaultListener\n * see  ftc_app/doc/tutorial/FTC_FieldCoordinateSystemDefinition.pdf\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\";\n\n    OpenGLMatrix lastLocation = null;\n\n    /**\n     * @see #captureFrameToFile()\n     */\n    int captureCounter = 0;\n    File captureDirectory = AppUtil.ROBOT_DATA_DIR;\n\n    /**\n     * {@link #vuforia} is the variable we will use to store our instance of the Vuforia\n     * localization engine.\n     */\n    VuforiaLocalizer vuforia;\n\n    /**\n     * This is the webcam we are to use. As with other hardware devices such as motors and\n     * servos, this device is identified using the robot configuration tool in the FTC application.\n     */\n    WebcamName webcamName;\n\n    @Override public void runOpMode() {\n\n        /*\n         * Retrieve the camera we are to use.\n         */\n        webcamName = hardwareMap.get(WebcamName.class, \"Webcam 1\");\n\n        /*\n         * To start up Vuforia, tell it the view that we wish to use for camera monitor (on the RC phone);\n         * If no camera monitor is desired, use the parameterless constructor instead (commented out below).\n         */\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters(cameraMonitorViewId);\n\n        // OR...  Do Not Activate the Camera Monitor View, to save power\n        // VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        /*\n         * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n         * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n         * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n         * web site at https://developer.vuforia.com/license-manager.\n         *\n         * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n         * random data. As an example, here is a example of a fragment of a valid key:\n         *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n         * Once you've obtained a license key, copy the string from the Vuforia web site\n         * and paste it in to your code on the next line, between the double quotes.\n         */\n        parameters.vuforiaLicenseKey = \" -- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n        /**\n         * We also indicate which camera on the RC we wish to use.\n         */\n        parameters.cameraName = webcamName;\n\n        /**\n         * Instantiate the Vuforia engine\n         */\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        /**\n         * Because this opmode processes frames in order to write them to a file, we tell Vuforia\n         * that we want to ensure that certain frame formats are available in the {@link Frame}s we\n         * see.\n         */\n        vuforia.enableConvertFrameToBitmap();\n\n        /** @see #captureFrameToFile() */\n        AppUtil.getInstance().ensureDirectoryExists(captureDirectory);\n\n\n        /**\n         * Load the data sets that for the trackable objects we wish to track. These particular data\n         * sets are stored in the 'assets' part of our application (you'll see them in the Android\n         * Studio 'Project' view over there on the left of the screen). You can make your own datasets\n         * with the Vuforia Target Manager: https://developer.vuforia.com/target-manager. PDFs for the\n         * example \"StonesAndChips\", datasets can be found in in this project in the\n         * documentation directory.\n         */\n        VuforiaTrackables stonesAndChips = vuforia.loadTrackablesFromAsset(\"StonesAndChips\");\n        VuforiaTrackable redTarget = stonesAndChips.get(0);\n        redTarget.setName(\"RedTarget\");  // Stones\n\n        VuforiaTrackable blueTarget  = stonesAndChips.get(1);\n        blueTarget.setName(\"BlueTarget\");  // Chips\n\n        /** For convenience, gather together all the trackable objects in one easily-iterable collection */\n        List<VuforiaTrackable> allTrackables = new ArrayList<VuforiaTrackable>();\n        allTrackables.addAll(stonesAndChips);\n\n        /**\n         * We use units of mm here because that's the recommended units of measurement for the\n         * size values specified in the XML for the ImageTarget trackables in data sets. E.g.:\n         *      <ImageTarget name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\"/>\n         * You don't *have to* use mm here, but the units here and the units used in the XML\n         * target configuration files *must* correspond for the math to work out correctly.\n         */\n        float mmPerInch        = 25.4f;\n        float mmBotWidth       = 18 * mmPerInch;            // ... or whatever is right for your robot\n        float mmFTCFieldWidth  = (12*12 - 2) * mmPerInch;   // the FTC field is ~11'10\" center-to-center of the glass panels\n\n        /**\n         * In order for localization to work, we need to tell the system where each target we\n         * wish to use for navigation resides on the field, and we need to specify where on the robot\n         * the camera resides. These specifications are in the form of <em>transformation matrices.</em>\n         * Transformation matrices are a central, important concept in the math here involved in localization.\n         * See <a href=\"https://en.wikipedia.org/wiki/Transformation_matrix\">Transformation Matrix</a>\n         * for detailed information. Commonly, you'll encounter transformation matrices as instances\n         * of the {@link OpenGLMatrix} class.\n         *\n         * For the most part, you don't need to understand the details of the math of how transformation\n         * matrices work inside (as fascinating as that is, truly). Just remember these key points:\n         * <ol>\n         *\n         *     <li>You can put two transformations together to produce a third that combines the effect of\n         *     both of them. If, for example, you have a rotation transform R and a translation transform T,\n         *     then the combined transformation matrix RT which does the rotation first and then the translation\n         *     is given by {@code RT = T.multiplied(R)}. That is, the transforms are multiplied in the\n         *     <em>reverse</em> of the chronological order in which they applied.</li>\n         *\n         *     <li>A common way to create useful transforms is to use methods in the {@link OpenGLMatrix}\n         *     class REPLACE_CLASS the Orientation class. See, for example, {@link OpenGLMatrix#translation(float,\n         *     float, float)}, {@link OpenGLMatrix#rotation(AngleUnit, float, float, float, float)}, and\n         *     {@link Orientation#getRotationMatrix(AxesReference, AxesOrder, AngleUnit, float, float, float)}.\n         *     Related methods in {@link OpenGLMatrix}, such as {@link OpenGLMatrix#rotated(AngleUnit,\n         *     float, float, float, float)}, are syntactic shorthands for creating a new transform and\n         *     then immediately multiplying the receiver by it, which can be convenient at times.</li>\n         *\n         *     <li>If you want to break open the black box of a transformation matrix to understand\n         *     what it's doing inside, use {@link MatrixF#getTranslation()} to fetch how much the\n         *     transform will move you in x, y, and z, and use {@link Orientation#getOrientation(MatrixF,\n         *     AxesReference, AxesOrder, AngleUnit)} to determine the rotational motion that the transform\n         *     will impart. See {@link #format(OpenGLMatrix)} below for an example.</li>\n         *\n         * </ol>\n         *\n         * This example places the \"stones\" image on the perimeter wall to the Left\n         *  of the Red Driver station wall.  Similar to the Red Beacon Location on the Res-Q\n         *\n         * This example places the \"chips\" image on the perimeter wall to the Right\n         *  of the Blue Driver station.  Similar to the Blue Beacon Location on the Res-Q\n         *\n         * See the doc folder of this project for a description of the Field Coordinate System\n         * conventions.\n         *\n         * Initially the target is conceptually lying at the origin of the Field Coordinate System\n         * (the center of the field), facing up.\n         *\n         * In this configuration, the target's coordinate system aligns with that of the field.\n         *\n         * In a real situation we'd also account for the vertical (Z) offset of the target,\n         * but for simplicity, we ignore that here; for a real robot, you'll want to fix that.\n         *\n         * To place the Stones Target on the Red Audience wall:\n         * - First we rotate it 90 around the field's X axis to flip it upright\n         * - Then we rotate it  90 around the field's Z access to face it away from the audience.\n         * - Finally, we translate it back along the X axis towards the red audience wall.\n         */\n        OpenGLMatrix redTargetLocationOnField = OpenGLMatrix\n                /* Then we translate the target off to the RED WALL. Our translation here\n                is a negative translation in X.*/\n                .translation(-mmFTCFieldWidth/2, 0, 0)\n                .multiplied(Orientation.getRotationMatrix(\n                        /* First, in the fixed (field) coordinate system, we rotate 90deg in X, then 90 in Z */\n                        AxesReference.EXTRINSIC, AxesOrder.XZX,\n                        AngleUnit.DEGREES, 90, 90, 0));\n        redTarget.setLocationFtcFieldFromTarget(redTargetLocationOnField);\n        RobotLog.ii(TAG, \"Red Target=%s\", format(redTargetLocationOnField));\n\n       /*\n        * To place the Stones Target on the Blue Audience wall:\n        * - First we rotate it 90 around the field's X axis to flip it upright\n        * - Finally, we translate it along the Y axis towards the blue audience wall.\n        */\n        OpenGLMatrix blueTargetLocationOnField = OpenGLMatrix\n                /* Then we translate the target off to the Blue Audience wall.\n                Our translation here is a positive translation in Y.*/\n                .translation(0, mmFTCFieldWidth/2, 0)\n                .multiplied(Orientation.getRotationMatrix(\n                        /* First, in the fixed (field) coordinate system, we rotate 90deg in X */\n                        AxesReference.EXTRINSIC, AxesOrder.XZX,\n                        AngleUnit.DEGREES, 90, 0, 0));\n        blueTarget.setLocationFtcFieldFromTarget(blueTargetLocationOnField);\n        RobotLog.ii(TAG, \"Blue Target=%s\", format(blueTargetLocationOnField));\n\n        /**\n         * We also need to tell Vuforia where the <em>cameras</em> are relative to the robot.\n         *\n         * Just as there is a Field Coordinate System, so too there is a Robot Coordinate System.\n         * The two share many similarities. The origin of the Robot Coordinate System is wherever\n         * you choose to make it on the robot, but typically you'd choose somewhere in the middle\n         * of the robot. From that origin, the Y axis is horizontal and positive out towards the\n         * \"front\" of the robot (however you choose \"front\" to be defined), the X axis is horizontal\n         * and positive out towards the \"right\" of the robot (i.e.: 90deg horizontally clockwise from\n         * the positive Y axis), and the Z axis is vertical towards the sky.\n         *\n         * Similarly, for each camera there is a Camera Coordinate System. The origin of a Camera\n         * Coordinate System lies in the middle of the sensor inside of the camera. The Z axis is\n         * positive coming out of the lens of the camera in a direction perpendicular to the plane\n         * of the sensor. When looking at the face of the lens of the camera (down the positive Z\n         * axis), the X axis is positive off to the right in the plane of the sensor, and the Y axis\n         * is positive out the top of the lens in the plane of the sensor at 90 horizontally\n         * counter clockwise from the X axis.\n         *\n         * Next, there is Phone Coordinate System (for robots that have phones, of course), though\n         * with the advent of Vuforia support for Webcams, this coordinate system is less significant\n         * than it was previously. The Phone Coordinate System is defined thusly: with the phone in\n         * flat front of you in portrait mode (i.e. as it is when running the robot controller app)\n         * and you are staring straight at the face of the phone,\n         *     * X is positive heading off to your right,\n         *     * Y is positive heading up through the top edge of the phone, and\n         *     * Z is pointing out of the screen, toward you.\n         * The origin of the Phone Coordinate System is at the origin of the Camera Coordinate System\n         * of the front-facing camera on the phone.\n         *\n         * Finally, it is worth noting that trackable Vuforia Image Targets have their <em>own</em>\n         * coordinate system (see {@link VuforiaTrackable}. This is sometimes referred to as the\n         * Target Coordinate System. In keeping with the above, when looking at the target in its\n         * natural orientation, in the Target Coodinate System\n         *     * X is positive heading off to your right,\n         *     * Y is positive heading up through the top edge of the target, and\n         *     * Z is pointing out of the target, toward you.\n         *\n         * One can observe that the Camera Coordinate System of the front-facing camera on a phone\n         * coincides with the Phone Coordinate System. Further, when a phone is placed on its back\n         * at the origin of the Robot Coordinate System and aligned appropriately, those coordinate\n         * systems also coincide with the Robot Coordinate System. Got it?\n         *\n         * In this example here, we're going to assume that we put the camera on the right side\n         * of the robot (facing outwards, of course). To determine the transformation matrix that\n         * describes that location, first consider the camera as lying on its back at the origin\n         * of the Robot Coordinate System such that the Camera Coordinate System and Robot Coordinate\n         * System coincide. Then the transformation we need is\n         *      * first a rotation of the camera by +90deg along the robot X axis,\n         *      * then a rotation of the camera by +90deg along the robot Z axis, and\n         *      * finally a translation of the camera to the side of the robot.\n         *\n         * When determining whether a rotation is positive or negative, consider yourself as looking\n         * down the (positive) axis of rotation from the positive towards the origin. Positive rotations\n         * are then CCW, and negative rotations CW. An example: consider looking down the positive Z\n         * axis towards the origin. A positive rotation about Z (ie: a rotation parallel to the the X-Y\n         * plane) is then CCW, as one would normally expect from the usual classic 2D geometry.\n         */\n\n        OpenGLMatrix robotFromCamera = OpenGLMatrix\n                .translation(mmBotWidth/2,0,0)\n                .multiplied(Orientation.getRotationMatrix(\n                        AxesReference.EXTRINSIC, AxesOrder.XZY,\n                        AngleUnit.DEGREES, 90, 90, 0));\n        RobotLog.ii(TAG, \"camera=%s\", format(robotFromCamera));\n\n        /**\n         * Let the trackable listeners we care about know where the camera is. We know that each\n         * listener is a {@link VuforiaTrackableDefaultListener} and can so safely cast because\n         * we have not ourselves installed a listener of a different type.\n         */\n        ((VuforiaTrackableDefaultListener)redTarget.getListener()).setCameraLocationOnRobot(parameters.cameraName, robotFromCamera);\n        ((VuforiaTrackableDefaultListener)blueTarget.getListener()).setCameraLocationOnRobot(parameters.cameraName, robotFromCamera);\n\n        /**\n         * A brief tutorial: here's how all the math is going to work:\n         *\n         * C = robotFromCamera          maps   camera coords -> robot coords\n         * P = tracker.getPose()        maps   image target coords -> camera coords\n         * L = redTargetLocationOnField maps   image target coords -> field coords\n         *\n         * So\n         *\n         * C.inverted()                 maps   robot coords -> camera coords\n         * P.inverted()                 maps   camera coords -> imageTarget coords\n         *\n         * Putting that all together,\n         *\n         * L x P.inverted() x C.inverted() maps robot coords to field coords.\n         *\n         * @see VuforiaTrackableDefaultListener#getRobotLocation()\n         */\n\n        /** Wait for the game to begin */\n        telemetry.addData(\">\", \"Press Play to start tracking\");\n        telemetry.update();\n        waitForStart();\n\n        /** Start tracking the data sets we care about. */\n        stonesAndChips.activate();\n\n        boolean buttonPressed = false;\n        while (opModeIsActive()) {\n\n            if (gamepad1.a && !buttonPressed) {\n                captureFrameToFile();\n                }\n            buttonPressed = gamepad1.a;\n\n            for (VuforiaTrackable trackable : allTrackables) {\n                /**\n                 * getUpdatedRobotLocation() will return null if no new information is available since\n                 * the last time that call was made, or if the trackable is not currently visible.\n                 * getRobotLocation() will return null if the trackable is not currently visible.\n                 */\n                telemetry.addData(trackable.getName(), ((VuforiaTrackableDefaultListener)trackable.getListener()).isVisible() ? \"Visible\" : \"Not Visible\");    //\n\n                OpenGLMatrix robotLocationTransform = ((VuforiaTrackableDefaultListener)trackable.getListener()).getUpdatedRobotLocation();\n                if (robotLocationTransform != null) {\n                    lastLocation = robotLocationTransform;\n                }\n            }\n            /**\n             * Provide feedback as to where the robot was last located (if we know).\n             */\n            if (lastLocation != null) {\n                //  RobotLog.vv(TAG, \"robot=%s\", format(lastLocation));\n                telemetry.addData(\"Pos\", format(lastLocation));\n            } else {\n                telemetry.addData(\"Pos\", \"Unknown\");\n            }\n            telemetry.update();\n        }\n    }\n\n    /**\n     * A simple utility that extracts positioning information from a transformation matrix\n     * and formats it in a form palatable to a human being.\n     */\n    String format(OpenGLMatrix transformationMatrix) {\n        return transformationMatrix.formatAsTransform();\n    }\n\n    /**\n     * Sample one frame from the Vuforia stream and write it to a .PNG image file on the robot\n     * controller in the /sdcard/FIRST/data directory. The images can be downloaded using Android\n     * Studio's Device File Explorer, ADB, or the Media Transfer Protocol (MTP) integration into\n     * Windows Explorer, among other means. The images can be useful during robot design and calibration\n     * in order to get a sense of what the camera is actually seeing and so assist in camera\n     * aiming and alignment.\n     */\n    void captureFrameToFile() {\n        vuforia.getFrameOnce(Continuation.create(ThreadPool.getDefault(), new Consumer<Frame>()\n            {\n            @Override public void accept(Frame frame)\n                {\n                Bitmap bitmap = vuforia.convertFrameToBitmap(frame);\n                if (bitmap != null) {\n                    File file = new File(captureDirectory, String.format(Locale.getDefault(), \"VuforiaFrame-%d.png\", captureCounter++));\n                    try {\n                        FileOutputStream outputStream = new FileOutputStream(file);\n                        try {\n                            bitmap.compress(Bitmap.CompressFormat.PNG, 100, outputStream);\n                        } finally {\n                            outputStream.close();\n                            telemetry.log().add(\"captured %s\", file.getName());\n                        }\n                    } catch (IOException e) {\n                        RobotLog.ee(TAG, e, \"exception in captureFrameToFile()\");\n                    }\n                }\n            }\n        }));\n    }\n}\n",
    "ConceptTelemetry": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.VoltageSensor;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\nimport org.firstinspires.ftc.robotcore.external.Func;\nimport org.firstinspires.ftc.robotcore.external.Telemetry;\n\n/**\n * {@link ConceptTelemetry} illustrates various ways in which telemetry can be\n * transmitted from the robot controller to the driver station. The sample illustrates\n * numeric and text data, formatted output, and optimized evaluation of expensive-to-acquire\n * information. The telemetry {@link Telemetry#log() log} is illustrated by scrolling a poem\n * to the driver station.\n *\n * @see Telemetry\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\"The lamb was sure to go.\",\n        \"\",\n        \"He followed her to school one day,\",\n        \"Which was against the rule,\",\n        \"It made the children laugh and play\",\n        \"To see a lamb at school.\",\n        \"\",\n        \"And so the teacher turned it out,\",\n        \"But still it lingered near,\",\n        \"And waited patiently about,\",\n        \"Till Mary did appear.\",\n        \"\",\n        \"\"Why does the lamb love Mary so?\"\",\n        \"The eager children cry.\",\n        \"\"Why, Mary loves the lamb, you know,\"\",\n        \"The teacher did reply.\",\n        \"\",\n        \"\"\n    };\n\n    @Override public void runOpMode() {\n\n        /* we keep track of how long it's been since the OpMode was started, just\n         * to have some interesting data to show */\n        ElapsedTime opmodeRunTime = new ElapsedTime();\n\n        // We show the log in oldest-to-newest order, as that's better for poetry\n        telemetry.log().setDisplayOrder(Telemetry.Log.DisplayOrder.OLDEST_FIRST);\n        // We can control the number of lines shown in the log\n        telemetry.log().setCapacity(6);\n        // The interval between lines of poetry, in seconds\n        double sPoemInterval = 0.6;\n\n        /**\n         * Wait until we've been given the ok to go. For something to do, we emit the\n         * elapsed time as we sit here and wait. If we didn't want to do anything while\n         * we waited, we would just call {@link #waitForStart()}.\n         */\n        while (!isStarted()) {\n            telemetry.addData(\"time\", \"%.1f seconds\", opmodeRunTime.seconds());\n            telemetry.update();\n            idle();\n        }\n\n        // Ok, we've been given the ok to go\n\n        /**\n         * As an illustration, the first line on our telemetry display will display the battery voltage.\n         * The idea here is that it's expensive to compute the voltage (at least for purposes of illustration)\n         * so you don't want to do it unless the data is <em>actually</em> going to make it to the\n         * driver station (recall that telemetry transmission is throttled to reduce bandwidth use.\n         * Note that getBatteryVoltage() below returns 'Infinity' if there's no voltage sensor attached.\n         *\n         * @see Telemetry#getMsTransmissionInterval()\n         */\n        telemetry.addData(\"voltage\", \"%.1f volts\", new Func<Double>() {\n            @Override public Double value() {\n                return getBatteryVoltage();\n            }\n            });\n\n        // Reset to keep some timing stats for the post-'start' part of the opmode\n        opmodeRunTime.reset();\n        int loopCount = 1;\n\n        // Go go gadget robot!\n        while (opModeIsActive()) {\n\n            // Emit poetry if it's been a while\n            if (poemElapsed.seconds() > sPoemInterval) {\n                emitPoemLine();\n            }\n\n            // As an illustration, show some loop timing information\n            telemetry.addData(\"loop count\", loopCount);\n            telemetry.addData(\"ms/loop\", \"%.3f ms\", opmodeRunTime.milliseconds() / loopCount);\n\n            // Show joystick information as some other illustrative data\n            telemetry.addLine(\"left joystick | \")\n                    .addData(\"x\", gamepad1.left_stick_x)\n                    .addData(\"y\", gamepad1.left_stick_y);\n            telemetry.addLine(\"right joystick | \")\n                    .addData(\"x\", gamepad1.right_stick_x)\n                    .addData(\"y\", gamepad1.right_stick_y);\n\n            /**\n             * Transmit the telemetry to the driver station, subject to throttling.\n             * @see Telemetry#getMsTransmissionInterval()\n             */\n            telemetry.update();\n\n            /** Update loop info and play nice with the rest of the {@link Thread}s in the system */\n            loopCount++;\n        }\n    }\n\n    // emits a line of poetry to the telemetry log\n    void emitPoemLine() {\n        telemetry.log().add(poem[poemLine]);\n        poemLine = (poemLine+1) % poem.length;\n        poemElapsed.reset();\n    }\n\n    // Computes the current battery voltage\n    double getBatteryVoltage() {\n        double result = Double.POSITIVE_INFINITY;\n        for (VoltageSensor sensor : hardwareMap.voltageSensor) {\n            double voltage = sensor.getVoltage();\n            if (voltage > 0) {\n                result = Math.min(result, voltage);\n            }\n        }\n        return result;\n    }\n}\n",
    "PushbotTeleopPOV Linear": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.util.Range;\n\n/**\n * This OpMode uses the common Pushbot hardware class REPLACE_CLASS define the devices on the robot.\n * All device access is managed through the HardwarePushbot class.\n * The code is structured as a LinearOpMode\n *\n * This particular OpMode executes a POV Game style Teleop for a PushBot\n * In this mode the left stick moves the robot FWD and back, the Right stick turns left and right.\n * It raises and lowers the claw using the Gampad Y and A buttons respectively.\n * It also opens and closes the claws slowly using the left and right Bumper buttons.\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    /* Declare OpMode members. */\n    HardwarePushbot robot           = new HardwarePushbot();   // Use a Pushbot's hardware\n    double          clawOffset      = 0;                       // Servo mid position\n    final double    CLAW_SPEED      = 0.02 ;                   // sets rate to move servo\n\n    @Override\n    public void runOpMode() {\n        double left;\n        double right;\n        double drive;\n        double turn;\n        double max;\n\n        /* Initialize the hardware variables.\n         * The init() method of the hardware class REPLACE_CLASS all the work here\n         */\n        robot.init(hardwareMap);\n\n        // Send telemetry message to signify robot waiting;\n        telemetry.addData(\"Say\", \"Hello Driver\");    //\n        telemetry.update();\n\n        // Wait for the game to start (driver presses PLAY)\n        waitForStart();\n\n        // run until the end of the match (driver presses STOP)\n        while (opModeIsActive()) {\n\n            // Run wheels in POV mode (note: The joystick goes negative when pushed forwards, so negate it)\n            // In this mode the Left stick moves the robot fwd and back, the Right stick turns left and right.\n            // This way it's also easy to just drive straight, or just turn.\n            drive = -gamepad1.left_stick_y;\n            turn  =  gamepad1.right_stick_x;\n\n            // Combine drive and turn for blended motion.\n            left  = drive + turn;\n            right = drive - turn;\n\n            // Normalize the values so neither exceed +/- 1.0\n            max = Math.max(Math.abs(left), Math.abs(right));\n            if (max > 1.0)\n            {\n                left /= max;\n                right /= max;\n            }\n\n            // Output the safe vales to the motor drives.\n            robot.leftDrive.setPower(left);\n            robot.rightDrive.setPower(right);\n\n            // Use gamepad left & right Bumpers to open and close the claw\n            if (gamepad1.right_bumper)\n                clawOffset += CLAW_SPEED;\n            else if (gamepad1.left_bumper)\n                clawOffset -= CLAW_SPEED;\n\n            // Move both servos to new position.  Assume servos are mirror image of each other.\n            clawOffset = Range.clip(clawOffset, -0.5, 0.5);\n            robot.leftClaw.setPosition(robot.MID_SERVO + clawOffset);\n            robot.rightClaw.setPosition(robot.MID_SERVO - clawOffset);\n\n            // Use gamepad buttons to move arm up (Y) and down (A)\n            if (gamepad1.y)\n                robot.leftArm.setPower(robot.ARM_UP_POWER);\n            else if (gamepad1.a)\n                robot.leftArm.setPower(robot.ARM_DOWN_POWER);\n            else\n                robot.leftArm.setPower(0.0);\n\n            // Send telemetry message to signify robot running;\n            telemetry.addData(\"claw\",  \"Offset = %.2f\", clawOffset);\n            telemetry.addData(\"left\",  \"%.2f\", left);\n            telemetry.addData(\"right\", \"%.2f\", right);\n            telemetry.update();\n\n            // Pace this loop so jaw action is reasonable speed.\n            sleep(50);\n        }\n    }\n}\n",
    "ConceptRampMotorSpeed": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DcMotor;\n\n/**\n * This OpMode ramps a single motor speed up and down repeatedly until Stop is pressed.\n * The code is structured as a LinearOpMode\n *\n * This code assumes a DC motor configured with the name \"left_drive\" as is found on a pushbot.\n *\n * INCREMENT sets how much to increase/decrease the power each cycle\n * CYCLE_MS sets the update period.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    static final double INCREMENT   = 0.01;     // amount to ramp motor each CYCLE_MS cycle\n    static final int    CYCLE_MS    =   50;     // period of each cycle\n    static final double MAX_FWD     =  1.0;     // Maximum FWD power applied to motor\n    static final double MAX_REV     = -1.0;     // Maximum REV power applied to motor\n\n    // Define class REPLACE_CLASS\n    DcMotor motor;\n    double  power   = 0;\n    boolean rampUp  = true;\n\n\n    @Override\n    public void runOpMode() {\n\n        // Connect to motor (Assume standard left wheel)\n        // Change the text in quotes to match any motor name on your robot.\n        motor = hardwareMap.get(DcMotor.class, \"left_drive\");\n\n        // Wait for the start button\n        telemetry.addData(\">\", \"Press Start to run Motors.\" );\n        telemetry.update();\n        waitForStart();\n\n        // Ramp motor speeds till stop pressed.\n        while(opModeIsActive()) {\n\n            // Ramp the motors, according to the rampUp variable.\n            if (rampUp) {\n                // Keep stepping up until we hit the max value.\n                power += INCREMENT ;\n                if (power >= MAX_FWD ) {\n                    power = MAX_FWD;\n                    rampUp = !rampUp;   // Switch ramp direction\n                }\n            }\n            else {\n                // Keep stepping down until we hit the min value.\n                power -= INCREMENT ;\n                if (power <= MAX_REV ) {\n                    power = MAX_REV;\n                    rampUp = !rampUp;  // Switch ramp direction\n                }\n            }\n\n            // Display the current value\n            telemetry.addData(\"Motor Power\", \"%5.2f\", power);\n            telemetry.addData(\">\", \"Press Stop to end test.\" );\n            telemetry.update();\n\n            // Set the motor to the new power and pause;\n            motor.setPower(power);\n            sleep(CYCLE_MS);\n            idle();\n        }\n\n        // Turn off motor and signal done;\n        motor.setPower(0);\n        telemetry.addData(\">\", \"Done\");\n        telemetry.update();\n\n    }\n}\n",
    "ConceptTensorFlowObjectDetection": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport java.util.List;\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer.CameraDirection;\nimport org.firstinspires.ftc.robotcore.external.tfod.TFObjectDetector;\nimport org.firstinspires.ftc.robotcore.external.tfod.Recognition;\n\n/**\n * This 2020-2021 OpMode illustrates the basics of using the TensorFlow Object Detection API to\n * determine the position of the Freight Frenzy game elements.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n  /* Note: This sample uses the all-objects Tensor Flow model (FreightFrenzy_BCDM.tflite), which contains\n   * the following 4 detectable objects\n   *  0: Ball,\n   *  1: Cube,\n   *  2: Duck,\n   *  3: Marker (duck location tape marker)\n   *\n   *  Two additional model assets are available which only contain a subset of the objects:\n   *  FreightFrenzy_BC.tflite  0: Ball,  1: Cube\n   *  FreightFrenzy_DM.tflite  0: Duck,  1: Marker\n   */\n    private static final String TFOD_MODEL_ASSET = \"FreightFrenzy_BCDM.tflite\";\n    private static final String[] LABELS = {\n      \"Ball\",\n      \"Cube\",\n      \"Duck\",\n      \"Marker\"\n    };\n\n    /*\n     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n     * web site at https://developer.vuforia.com/license-manager.\n     *\n     * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n     * random data. As an example, here is a example of a fragment of a valid key:\n     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n     * Once you've obtained a license key, copy the string from the Vuforia web site\n     * and paste it in to your code on the next line, between the double quotes.\n     */\n    private static final String VUFORIA_KEY =\n            \" -- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n    /**\n     * {@link #vuforia} is the variable we will use to store our instance of the Vuforia\n     * localization engine.\n     */\n    private VuforiaLocalizer vuforia;\n\n    /**\n     * {@link #tfod} is the variable we will use to store our instance of the TensorFlow Object\n     * Detection engine.\n     */\n    private TFObjectDetector tfod;\n\n    @Override\n    public void runOpMode() {\n        // The TFObjectDetector uses the camera frames from the VuforiaLocalizer, so we create that\n        // first.\n        initVuforia();\n        initTfod();\n\n        /**\n         * Activate TensorFlow Object Detection before we wait for the start command.\n         * Do it here so that the Camera Stream window will have the TensorFlow annotations visible.\n         **/\n        if (tfod != null) {\n            tfod.activate();\n\n            // The TensorFlow software will scale the input images from the camera to a lower resolution.\n            // This can result in lower detection accuracy at longer distances (> 55cm or 22\").\n            // If your target is at distance greater than 50 cm (20\") you can adjust the magnification value\n            // to artificially zoom in to the center of image.  For best results, the \"aspectRatio\" argument\n            // should be set to the value of the images used to create the TensorFlow Object Detection model\n            // (typically 16/9).\n            tfod.setZoom(2.5, 16.0/9.0);\n        }\n\n        /** Wait for the game to begin */\n        telemetry.addData(\">\", \"Press Play to start op mode\");\n        telemetry.update();\n        waitForStart();\n\n        if (opModeIsActive()) {\n            while (opModeIsActive()) {\n                if (tfod != null) {\n                    // getUpdatedRecognitions() will return null if no new information is available since\n                    // the last time that call was made.\n                    List<Recognition> updatedRecognitions = tfod.getUpdatedRecognitions();\n                    if (updatedRecognitions != null) {\n                      telemetry.addData(\"# Object Detected\", updatedRecognitions.size());\n\n                      // step through the list of recognitions and display boundary info.\n                      int i = 0;\n                      for (Recognition recognition : updatedRecognitions) {\n                        telemetry.addData(String.format(\"label (%d)\", i), recognition.getLabel());\n                        telemetry.addData(String.format(\"  left,top (%d)\", i), \"%.03f , %.03f\",\n                                          recognition.getLeft(), recognition.getTop());\n                        telemetry.addData(String.format(\"  right,bottom (%d)\", i), \"%.03f , %.03f\",\n                                recognition.getRight(), recognition.getBottom());\n                        i++;\n                      }\n                      telemetry.update();\n                    }\n                }\n            }\n        }\n    }\n\n    /**\n     * Initialize the Vuforia localization engine.\n     */\n    private void initVuforia() {\n        /*\n         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.\n         */\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        parameters.vuforiaLicenseKey = VUFORIA_KEY;\n        parameters.cameraDirection = CameraDirection.BACK;\n\n        //  Instantiate the Vuforia engine\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        // Loading trackables is not necessary for the TensorFlow Object Detection engine.\n    }\n\n    /**\n     * Initialize the TensorFlow Object Detection engine.\n     */\n    private void initTfod() {\n        int tfodMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\n            \"tfodMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        TFObjectDetector.Parameters tfodParameters = new TFObjectDetector.Parameters(tfodMonitorViewId);\n        tfodParameters.minResultConfidence = 0.8f;\n        tfodParameters.isModelTensorFlow2 = true;\n        tfodParameters.inputSize = 320;\n        tfod = ClassFactory.getInstance().createTFObjectDetector(tfodParameters, vuforia);\n        tfod.loadModelFromAsset(TFOD_MODEL_ASSET, LABELS);\n    }\n}\n",
    "ConceptWebcam": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport android.graphics.Bitmap;\nimport android.graphics.ImageFormat;\nimport android.os.Handler;\n\nimport androidx.annotation.NonNull;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.util.RobotLog;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.android.util.Size;\nimport org.firstinspires.ftc.robotcore.external.function.Consumer;\nimport org.firstinspires.ftc.robotcore.external.function.Continuation;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.Camera;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.CameraCaptureRequest;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.CameraCaptureSequenceId;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.CameraCaptureSession;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.CameraCharacteristics;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.CameraException;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.CameraFrame;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.CameraManager;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.robotcore.internal.collections.EvictingBlockingQueue;\nimport org.firstinspires.ftc.robotcore.internal.network.CallbackLooper;\nimport org.firstinspires.ftc.robotcore.internal.system.AppUtil;\nimport org.firstinspires.ftc.robotcore.internal.system.ContinuationSynchronizer;\nimport org.firstinspires.ftc.robotcore.internal.system.Deadline;\n\nimport java.io.File;\nimport java.io.FileOutputStream;\nimport java.io.IOException;\nimport java.util.Locale;\nimport java.util.concurrent.ArrayBlockingQueue;\nimport java.util.concurrent.TimeUnit;\n\n/**\n * This OpMode illustrates how to open a webcam and retrieve images from it. It requires a configuration\n * containing a webcam with the default name (\"Webcam 1\"). When the opmode runs, pressing the 'A' button\n * will cause a frame from the camera to be written to a file on the device, which can then be retrieved\n * by various means (e.g.: Device File Explorer in Android Studio; plugging the device into a PC and\n * using Media Transfer; ADB; etc)\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\";\n\n    /** How long we are to wait to be granted permission to use the camera before giving up. Here,\n     * we wait indefinitely */\n    private static final int secondsPermissionTimeout = Integer.MAX_VALUE;\n\n    /** State regarding our interaction with the camera */\n    private CameraManager cameraManager;\n    private WebcamName cameraName;\n    private Camera camera;\n    private CameraCaptureSession cameraCaptureSession;\n\n    /** The queue into which all frames from the camera are placed as they become available.\n     * Frames which are not processed by the OpMode are automatically discarded. */\n    private EvictingBlockingQueue<Bitmap> frameQueue;\n\n    /** State regarding where and how to save frames when the 'A' button is pressed. */\n    private int captureCounter = 0;\n    private File captureDirectory = AppUtil.ROBOT_DATA_DIR;\n\n    /** A utility object that indicates where the asynchronous callbacks from the camera\n     * infrastructure are to run. In this OpMode, that's all hidden from you (but see {@link #startCamera}\n     * if you're curious): no knowledge of multi-threading is needed here. */\n    private Handler callbackHandler;\n\n    //----------------------------------------------------------------------------------------------\n    // Main OpMode entry\n    //----------------------------------------------------------------------------------------------\n\n    @Override public void runOpMode() {\n\n        callbackHandler = CallbackLooper.getDefault().getHandler();\n\n        cameraManager = ClassFactory.getInstance().getCameraManager();\n        cameraName = hardwareMap.get(WebcamName.class, \"Webcam 1\");\n\n        initializeFrameQueue(2);\n        AppUtil.getInstance().ensureDirectoryExists(captureDirectory);\n\n        try {\n            openCamera();\n            if (camera == null) return;\n\n            startCamera();\n            if (cameraCaptureSession == null) return;\n\n            telemetry.addData(\">\", \"Press Play to start\");\n            telemetry.update();\n            waitForStart();\n            telemetry.clear();\n            telemetry.addData(\">\", \"Started...Press 'A' to capture frame\");\n\n            boolean buttonPressSeen = false;\n            boolean captureWhenAvailable = false;\n            while (opModeIsActive()) {\n\n                boolean buttonIsPressed = gamepad1.a;\n                if (buttonIsPressed && !buttonPressSeen) {\n                    captureWhenAvailable = true;\n                }\n                buttonPressSeen = buttonIsPressed;\n\n                if (captureWhenAvailable) {\n                    Bitmap bmp = frameQueue.poll();\n                    if (bmp != null) {\n                        captureWhenAvailable = false;\n                        onNewFrame(bmp);\n                    }\n                }\n\n                telemetry.update();\n            }\n        } finally {\n            closeCamera();\n        }\n    }\n\n    /** Do something with the frame */\n    private void onNewFrame(Bitmap frame) {\n        saveBitmap(frame);\n        frame.recycle(); // not strictly necessary, but helpful\n    }\n\n    //----------------------------------------------------------------------------------------------\n    // Camera operations\n    //----------------------------------------------------------------------------------------------\n\n    private void initializeFrameQueue(int capacity) {\n        /** The frame queue will automatically throw away bitmap frames if they are not processed\n         * quickly by the OpMode. This avoids a buildup of frames in memory */\n        frameQueue = new EvictingBlockingQueue<Bitmap>(new ArrayBlockingQueue<Bitmap>(capacity));\n        frameQueue.setEvictAction(new Consumer<Bitmap>() {\n            @Override public void accept(Bitmap frame) {\n                // RobotLog.ii(TAG, \"frame recycled w/o processing\");\n                frame.recycle(); // not strictly necessary, but helpful\n            }\n        });\n    }\n\n    private void openCamera() {\n        if (camera != null) return; // be idempotent\n\n        Deadline deadline = new Deadline(secondsPermissionTimeout, TimeUnit.SECONDS);\n        camera = cameraManager.requestPermissionAndOpenCamera(deadline, cameraName, null);\n        if (camera == null) {\n            error(\"camera not found or permission to use not granted: %s\", cameraName);\n        }\n    }\n\n    private void startCamera() {\n        if (cameraCaptureSession != null) return; // be idempotent\n\n        /** YUY2 is supported by all Webcams, per the USB Webcam standard: See \"USB Device Class Definition\n         * for Video Devices: Uncompressed Payload, Table 2-1\". Further, often this is the *only*\n         * image format supported by a camera */\n        final int imageFormat = ImageFormat.YUY2;\n\n        /** Verify that the image is supported, and fetch size and desired frame rate if so */\n        CameraCharacteristics cameraCharacteristics = cameraName.getCameraCharacteristics();\n        if (!contains(cameraCharacteristics.getAndroidFormats(), imageFormat)) {\n            error(\"image format not supported\");\n            return;\n        }\n        final Size size = cameraCharacteristics.getDefaultSize(imageFormat);\n        final int fps = cameraCharacteristics.getMaxFramesPerSecond(imageFormat, size);\n\n        /** Some of the logic below runs asynchronously on other threads. Use of the synchronizer\n         * here allows us to wait in this method until all that asynchrony completes before returning. */\n        final ContinuationSynchronizer<CameraCaptureSession> synchronizer = new ContinuationSynchronizer<>();\n        try {\n            /** Create a session in which requests to capture frames can be made */\n            camera.createCaptureSession(Continuation.create(callbackHandler, new CameraCaptureSession.StateCallbackDefault() {\n                @Override public void onConfigured(@NonNull CameraCaptureSession session) {\n                    try {\n                        /** The session is ready to go. Start requesting frames */\n                        final CameraCaptureRequest captureRequest = camera.createCaptureRequest(imageFormat, size, fps);\n                        session.startCapture(captureRequest,\n                            new CameraCaptureSession.CaptureCallback() {\n                                @Override public void onNewFrame(@NonNull CameraCaptureSession session, @NonNull CameraCaptureRequest request, @NonNull CameraFrame cameraFrame) {\n                                    /** A new frame is available. The frame data has <em>not</em> been copied for us, and we can only access it\n                                     * for the duration of the callback. So we copy here manually. */\n                                    Bitmap bmp = captureRequest.createEmptyBitmap();\n                                    cameraFrame.copyToBitmap(bmp);\n                                    frameQueue.offer(bmp);\n                                }\n                            },\n                            Continuation.create(callbackHandler, new CameraCaptureSession.StatusCallback() {\n                                @Override public void onCaptureSequenceCompleted(@NonNull CameraCaptureSession session, CameraCaptureSequenceId cameraCaptureSequenceId, long lastFrameNumber) {\n                                    RobotLog.ii(TAG, \"capture sequence %s reports completed: lastFrame=%d\", cameraCaptureSequenceId, lastFrameNumber);\n                                }\n                            })\n                        );\n                        synchronizer.finish(session);\n                    } catch (CameraException|RuntimeException e) {\n                        RobotLog.ee(TAG, e, \"exception starting capture\");\n                        error(\"exception starting capture\");\n                        session.close();\n                        synchronizer.finish(null);\n                    }\n                }\n            }));\n        } catch (CameraException|RuntimeException e) {\n            RobotLog.ee(TAG, e, \"exception starting camera\");\n            error(\"exception starting camera\");\n            synchronizer.finish(null);\n        }\n\n        /** Wait for all the asynchrony to complete */\n        try {\n            synchronizer.await();\n        } catch (InterruptedException e) {\n            Thread.currentThread().interrupt();\n        }\n\n        /** Retrieve the created session. This will be null on error. */\n        cameraCaptureSession = synchronizer.getValue();\n    }\n\n    private void stopCamera() {\n        if (cameraCaptureSession != null) {\n            cameraCaptureSession.stopCapture();\n            cameraCaptureSession.close();\n            cameraCaptureSession = null;\n        }\n    }\n\n    private void closeCamera() {\n        stopCamera();\n        if (camera != null) {\n            camera.close();\n            camera = null;\n        }\n    }\n\n    //----------------------------------------------------------------------------------------------\n    // Utilities\n    //----------------------------------------------------------------------------------------------\n\n    private void error(String msg) {\n        telemetry.log().add(msg);\n        telemetry.update();\n    }\n    private void error(String format, Object...args) {\n        telemetry.log().add(format, args);\n        telemetry.update();\n    }\n\n    private boolean contains(int[] array, int value) {\n        for (int i : array) {\n            if (i == value) return true;\n        }\n        return false;\n    }\n\n    private void saveBitmap(Bitmap bitmap) {\n        File file = new File(captureDirectory, String.format(Locale.getDefault(), \"webcam-frame-%d.jpg\", captureCounter++));\n        try {\n            try (FileOutputStream outputStream = new FileOutputStream(file)) {\n                bitmap.compress(Bitmap.CompressFormat.JPEG, 100, outputStream);\n                telemetry.log().add(\"captured %s\", file.getName());\n            }\n        } catch (IOException e) {\n            RobotLog.ee(TAG, e, \"exception in saveBitmap()\");\n            error(\"exception saving %s\", file.getName());\n        }\n    }\n}\n",
    "PushbotAutoDriveByTime Linear": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Autonomous;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\n/**\n * This file illustrates the concept of driving a path based on time.\n * It uses the common Pushbot hardware class REPLACE_CLASS define the drive on the robot.\n * The code is structured as a LinearOpMode\n *\n * The code assumes that you do NOT have encoders on the wheels,\n *   otherwise you would use: PushbotAutoDriveByEncoder;\n *\n *   The desired path in this example is:\n *   - Drive forward for 3 seconds\n *   - Spin right for 1.3 seconds\n *   - Drive Backwards for 1 Second\n *   - Stop and close the claw.\n *\n *  The code is written in a simple form with no optimizations.\n *  However, there are several ways that this type of sequence could be streamlined,\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@Autonomous(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    /* Declare OpMode members. */\n    HardwarePushbot         robot   = new HardwarePushbot();   // Use a Pushbot's hardware\n    private ElapsedTime     runtime = new ElapsedTime();\n\n\n    static final double     FORWARD_SPEED = 0.6;\n    static final double     TURN_SPEED    = 0.5;\n\n    @Override\n    public void runOpMode() {\n\n        /*\n         * Initialize the drive system variables.\n         * The init() method of the hardware class REPLACE_CLASS all the work here\n         */\n        robot.init(hardwareMap);\n\n        // Send telemetry message to signify robot waiting;\n        telemetry.addData(\"Status\", \"Ready to run\");    //\n        telemetry.update();\n\n        // Wait for the game to start (driver presses PLAY)\n        waitForStart();\n\n        // Step through each leg of the path, ensuring that the Auto mode has not been stopped along the way\n\n        // Step 1:  Drive forward for 3 seconds\n        robot.leftDrive.setPower(FORWARD_SPEED);\n        robot.rightDrive.setPower(FORWARD_SPEED);\n        runtime.reset();\n        while (opModeIsActive() && (runtime.seconds() < 3.0)) {\n            telemetry.addData(\"Path\", \"Leg 1: %2.5f S Elapsed\", runtime.seconds());\n            telemetry.update();\n        }\n\n        // Step 2:  Spin right for 1.3 seconds\n        robot.leftDrive.setPower(TURN_SPEED);\n        robot.rightDrive.setPower(-TURN_SPEED);\n        runtime.reset();\n        while (opModeIsActive() && (runtime.seconds() < 1.3)) {\n            telemetry.addData(\"Path\", \"Leg 2: %2.5f S Elapsed\", runtime.seconds());\n            telemetry.update();\n        }\n\n        // Step 3:  Drive Backwards for 1 Second\n        robot.leftDrive.setPower(-FORWARD_SPEED);\n        robot.rightDrive.setPower(-FORWARD_SPEED);\n        runtime.reset();\n        while (opModeIsActive() && (runtime.seconds() < 1.0)) {\n            telemetry.addData(\"Path\", \"Leg 3: %2.5f S Elapsed\", runtime.seconds());\n            telemetry.update();\n        }\n\n        // Step 4:  Stop and close the claw.\n        robot.leftDrive.setPower(0);\n        robot.rightDrive.setPower(0);\n        robot.leftClaw.setPosition(1.0);\n        robot.rightClaw.setPosition(0.0);\n\n        telemetry.addData(\"Path\", \"Complete\");\n        telemetry.update();\n        sleep(1000);\n    }\n}\n",
    "SensorColor": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport android.app.Activity;\nimport android.graphics.Color;\nimport android.view.View;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DistanceSensor;\nimport com.qualcomm.robotcore.hardware.NormalizedColorSensor;\nimport com.qualcomm.robotcore.hardware.NormalizedRGBA;\nimport com.qualcomm.robotcore.hardware.SwitchableLight;\n\nimport org.firstinspires.ftc.robotcore.external.navigation.DistanceUnit;\n\n/**\n * This is an example LinearOpMode that shows how to use a color sensor in a generic\n * way, regardless of which particular make or model of color sensor is used. The Op Mode\n * assumes that the color sensor is configured with a name of \"sensor_color\".\n *\n * There will be some variation in the values measured depending on the specific sensor you are using.\n *\n * You can increase the gain (a multiplier to make the sensor report higher values) by holding down\n * the A button on the gamepad, and decrease the gain by holding down the B button on the gamepad.\n *\n * If the color sensor has a light which is controllable from software, you can use the X button on\n * the gamepad to toggle the light on and off. The REV sensors don't support this, but instead have\n * a physical switch on them to turn the light on and off, beginning with REV Color Sensor V2.\n *\n * If the color sensor also supports short-range distance measurements (usually via an infrared\n * proximity sensor), the reported distance will be written to telemetry. As of September 2020,\n * the only color sensors that support this are the ones from REV Robotics. These infrared proximity\n * sensor measurements are only useful at very small distances, and are sensitive to ambient light\n * and surface reflectivity. You should use a different sensor if you need precise distance measurements.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this Op Mode to the Driver Station OpMode list\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n  /** The colorSensor field will contain a reference to our color sensor hardware object */\n  NormalizedColorSensor colorSensor;\n\n  /** The relativeLayout field is used to aid in providing interesting visual feedback\n   * in this sample application; you probably *don't* need this when you use a color sensor on your\n   * robot. Note that you won't see anything change on the Driver Station, only on the Robot Controller. */\n  View relativeLayout;\n\n  /**\n   * The runOpMode() method is the root of this Op Mode, as it is in all LinearOpModes.\n   * Our implementation here, though is a bit unusual: we've decided to put all the actual work\n   * in the runSample() method rather than directly in runOpMode() itself. The reason we do that is\n   * that in this sample we're changing the background color of the robot controller screen as the\n   * Op Mode runs, and we want to be able to *guarantee* that we restore it to something reasonable\n   * and palatable when the Op Mode ends. The simplest way to do that is to use a try...finally\n   * block around the main, core logic, and an easy way to make that all clear was to separate\n   * the former from the latter in separate methods.\n   */\n  @Override public void runOpMode() {\n\n    // Get a reference to the RelativeLayout so we can later change the background\n    // color of the Robot Controller app to match the hue detected by the RGB sensor.\n    int relativeLayoutId = hardwareMap.appContext.getResources().getIdentifier(\"RelativeLayout\", \"id\", hardwareMap.appContext.getPackageName());\n    relativeLayout = ((Activity) hardwareMap.appContext).findViewById(relativeLayoutId);\n\n    try {\n      runSample(); // actually execute the sample\n    } finally {\n      // On the way out, *guarantee* that the background is reasonable. It doesn't actually start off\n      // as pure white, but it's too much work to dig out what actually was used, and this is good\n      // enough to at least make the screen reasonable again.\n      // Set the panel back to the default color\n      relativeLayout.post(new Runnable() {\n        public void run() {\n          relativeLayout.setBackgroundColor(Color.WHITE);\n        }\n      });\n      }\n  }\n\n  protected void runSample() {\n    // You can give the sensor a gain value, will be multiplied by the sensor's raw value before the\n    // normalized color values are calculated. Color sensors (especially the REV Color Sensor V3)\n    // can give very low values (depending on the lighting conditions), which only use a small part\n    // of the 0-1 range that is available for the red, green, and blue values. In brighter conditions,\n    // you should use a smaller gain than in dark conditions. If your gain is too high, all of the\n    // colors will report at or near 1, and you won't be able to determine what color you are\n    // actually looking at. For this reason, it's better to err on the side of a lower gain\n    // (but always greater than  or equal to 1).\n    float gain = 2;\n\n    // Once per loop, we will update this hsvValues array. The first element (0) will contain the\n    // hue, the second element (1) will contain the saturation, and the third element (2) will\n    // contain the value. See http://web.archive.org/web/20190311170843/https://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html\n    // for an explanation of HSV color.\n    final float[] hsvValues = new float[3];\n\n    // xButtonPreviouslyPressed and xButtonCurrentlyPressed keep track of the previous and current\n    // state of the X button on the gamepad\n    boolean xButtonPreviouslyPressed = false;\n    boolean xButtonCurrentlyPressed = false;\n\n    // Get a reference to our sensor object. It's recommended to use NormalizedColorSensor over\n    // ColorSensor, because NormalizedColorSensor consistently gives values between 0 and 1, while\n    // the values you get from ColorSensor are dependent on the specific sensor you're using.\n    colorSensor = hardwareMap.get(NormalizedColorSensor.class, \"sensor_color\");\n\n    // If possible, turn the light on in the beginning (it might already be on anyway,\n    // we just make sure it is if we can).\n    if (colorSensor instanceof SwitchableLight) {\n      ((SwitchableLight)colorSensor).enableLight(true);\n    }\n\n    // Wait for the start button to be pressed.\n    waitForStart();\n\n    // Loop until we are asked to stop\n    while (opModeIsActive()) {\n      // Explain basic gain information via telemetry\n      telemetry.addLine(\"Hold the A button on gamepad 1 to increase gain, or B to decrease it.\\n\");\n      telemetry.addLine(\"Higher gain values mean that the sensor will report larger numbers for Red, Green, and Blue, and Value\\n\");\n\n      // Update the gain value if either of the A or B gamepad buttons is being held\n      if (gamepad1.a) {\n        // Only increase the gain by a small amount, since this loop will occur multiple times per second.\n        gain += 0.005;\n      } else if (gamepad1.b && gain > 1) { // A gain of less than 1 will make the values smaller, which is not helpful.\n        gain -= 0.005;\n      }\n\n      // Show the gain value via telemetry\n      telemetry.addData(\"Gain\", gain);\n\n      // Tell the sensor our desired gain value (normally you would do this during initialization,\n      // not during the loop)\n      colorSensor.setGain(gain);\n\n      // Check the status of the X button on the gamepad\n      xButtonCurrentlyPressed = gamepad1.x;\n\n      // If the button state is different than what it was, then act\n      if (xButtonCurrentlyPressed != xButtonPreviouslyPressed) {\n        // If the button is (now) down, then toggle the light\n        if (xButtonCurrentlyPressed) {\n          if (colorSensor instanceof SwitchableLight) {\n            SwitchableLight light = (SwitchableLight)colorSensor;\n            light.enableLight(!light.isLightOn());\n          }\n        }\n      }\n      xButtonPreviouslyPressed = xButtonCurrentlyPressed;\n\n      // Get the normalized colors from the sensor\n      NormalizedRGBA colors = colorSensor.getNormalizedColors();\n\n      /* Use telemetry to display feedback on the driver station. We show the red, green, and blue\n       * normalized values from the sensor (in the range of 0 to 1), as well as the equivalent\n       * HSV (hue, saturation and value) values. See http://web.archive.org/web/20190311170843/https://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html\n       * for an explanation of HSV color. */\n\n      // Update the hsvValues array by passing it to Color.colorToHSV()\n      Color.colorToHSV(colors.toColor(), hsvValues);\n\n      telemetry.addLine()\n              .addData(\"Red\", \"%.3f\", colors.red)\n              .addData(\"Green\", \"%.3f\", colors.green)\n              .addData(\"Blue\", \"%.3f\", colors.blue);\n      telemetry.addLine()\n              .addData(\"Hue\", \"%.3f\", hsvValues[0])\n              .addData(\"Saturation\", \"%.3f\", hsvValues[1])\n              .addData(\"Value\", \"%.3f\", hsvValues[2]);\n      telemetry.addData(\"Alpha\", \"%.3f\", colors.alpha);\n\n      /* If this color sensor also has a distance sensor, display the measured distance.\n       * Note that the reported distance is only useful at very close range, and is impacted by\n       * ambient light and surface reflectivity. */\n      if (colorSensor instanceof DistanceSensor) {\n        telemetry.addData(\"Distance (cm)\", \"%.3f\", ((DistanceSensor) colorSensor).getDistance(DistanceUnit.CM));\n      }\n\n      telemetry.update();\n\n      // Change the Robot Controller's background color to match the color detected by the color sensor.\n      relativeLayout.post(new Runnable() {\n        public void run() {\n          relativeLayout.setBackgroundColor(Color.HSVToColor(hsvValues));\n        }\n      });\n    }\n  }\n}\n",
    "SensorBNO055IMU": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.bosch.BNO055IMU;\nimport com.qualcomm.hardware.bosch.JustLoggingAccelerationIntegrator;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\n\nimport org.firstinspires.ftc.robotcore.external.Func;\nimport org.firstinspires.ftc.robotcore.external.navigation.Acceleration;\nimport org.firstinspires.ftc.robotcore.external.navigation.AngleUnit;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesOrder;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesReference;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.external.navigation.Position;\nimport org.firstinspires.ftc.robotcore.external.navigation.Velocity;\n\nimport java.util.Locale;\n\n/**\n * {@link SensorBNO055IMU} gives a short demo on how to use the BNO055 Inertial Motion Unit (IMU) from AdaFruit.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n *\n * @see <a href=\"http://www.adafruit.com/products/2472\">Adafruit IMU</a>\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled                            // Comment this out to add to the opmode list\npublic class REPLACE_CLASS extends LinearOpMode\n    {\n    //----------------------------------------------------------------------------------------------\n    // State\n    //----------------------------------------------------------------------------------------------\n\n    // The IMU sensor object\n    BNO055IMU imu;\n\n    // State used for updating telemetry\n    Orientation angles;\n    Acceleration gravity;\n\n    //----------------------------------------------------------------------------------------------\n    // Main logic\n    //----------------------------------------------------------------------------------------------\n\n    @Override public void runOpMode() {\n\n        // Set up the parameters with which we will use our IMU. Note that integration\n        // algorithm here just reports accelerations to the logcat log; it doesn't actually\n        // provide positional information.\n        BNO055IMU.Parameters parameters = new BNO055IMU.Parameters();\n        parameters.angleUnit           = BNO055IMU.AngleUnit.DEGREES;\n        parameters.accelUnit           = BNO055IMU.AccelUnit.METERS_PERSEC_PERSEC;\n        parameters.calibrationDataFile = \"BNO055IMUCalibration.json\"; // see the calibration sample opmode\n        parameters.loggingEnabled      = true;\n        parameters.loggingTag          = \"IMU\";\n        parameters.accelerationIntegrationAlgorithm = new JustLoggingAccelerationIntegrator();\n\n        // Retrieve and initialize the IMU. We expect the IMU to be attached to an I2C port\n        // on a Core Device Interface Module, configured to be a sensor of type \"AdaFruit IMU\",\n        // and named \"imu\".\n        imu = hardwareMap.get(BNO055IMU.class, \"imu\");\n        imu.initialize(parameters);\n\n        // Set up our telemetry dashboard\n        composeTelemetry();\n\n        // Wait until we're told to go\n        waitForStart();\n\n        // Start the logging of measured acceleration\n        imu.startAccelerationIntegration(new Position(), new Velocity(), 1000);\n\n        // Loop and update the dashboard\n        while (opModeIsActive()) {\n            telemetry.update();\n        }\n    }\n\n    //----------------------------------------------------------------------------------------------\n    // Telemetry Configuration\n    //----------------------------------------------------------------------------------------------\n\n    void composeTelemetry() {\n\n        // At the beginning of each telemetry update, grab a bunch of data\n        // from the IMU that we will then display in separate lines.\n        telemetry.addAction(new Runnable() { @Override public void run()\n                {\n                // Acquiring the angles is relatively expensive; we don't want\n                // to do that in each of the three items that need that info, as that's\n                // three times the necessary expense.\n                angles   = imu.getAngularOrientation(AxesReference.INTRINSIC, AxesOrder.ZYX, AngleUnit.DEGREES);\n                gravity  = imu.getGravity();\n                }\n            });\n\n        telemetry.addLine()\n            .addData(\"status\", new Func<String>() {\n                @Override public String value() {\n                    return imu.getSystemStatus().toShortString();\n                    }\n                })\n            .addData(\"calib\", new Func<String>() {\n                @Override public String value() {\n                    return imu.getCalibrationStatus().toString();\n                    }\n                });\n\n        telemetry.addLine()\n            .addData(\"heading\", new Func<String>() {\n                @Override public String value() {\n                    return formatAngle(angles.angleUnit, angles.firstAngle);\n                    }\n                })\n            .addData(\"roll\", new Func<String>() {\n                @Override public String value() {\n                    return formatAngle(angles.angleUnit, angles.secondAngle);\n                    }\n                })\n            .addData(\"pitch\", new Func<String>() {\n                @Override public String value() {\n                    return formatAngle(angles.angleUnit, angles.thirdAngle);\n                    }\n                });\n\n        telemetry.addLine()\n            .addData(\"grvty\", new Func<String>() {\n                @Override public String value() {\n                    return gravity.toString();\n                    }\n                })\n            .addData(\"mag\", new Func<String>() {\n                @Override public String value() {\n                    return String.format(Locale.getDefault(), \"%.3f\",\n                            Math.sqrt(gravity.xAccel*gravity.xAccel\n                                    + gravity.yAccel*gravity.yAccel\n                                    + gravity.zAccel*gravity.zAccel));\n                    }\n                });\n    }\n\n    //----------------------------------------------------------------------------------------------\n    // Formatting\n    //----------------------------------------------------------------------------------------------\n\n    String formatAngle(AngleUnit angleUnit, double angle) {\n        return formatDegrees(AngleUnit.DEGREES.fromUnit(angleUnit, angle));\n    }\n\n    String formatDegrees(double degrees){\n        return String.format(Locale.getDefault(), \"%.1f\", AngleUnit.DEGREES.normalize(degrees));\n    }\n}\n",
    "ConceptVuMarkIdentification": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.matrices.OpenGLMatrix;\nimport org.firstinspires.ftc.robotcore.external.matrices.VectorF;\nimport org.firstinspires.ftc.robotcore.external.navigation.AngleUnit;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesOrder;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesReference;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.external.navigation.RelicRecoveryVuMark;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuMarkInstanceId;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackable;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackableDefaultListener;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackables;\n\n/**\n * This OpMode illustrates the basics of using the Vuforia engine to determine\n * the identity of Vuforia VuMarks encountered on the field. The code is structured as\n * a LinearOpMode. It shares much structure with {@link ConceptVuforiaNavigation}; we do not here\n * duplicate the core Vuforia documentation found there, but rather instead focus on the\n * differences between the use of Vuforia for navigation vs VuMark identification.\n *\n * @see ConceptVuforiaNavigation\n * @see VuforiaLocalizer\n * @see VuforiaTrackableDefaultListener\n * see  ftc_app/doc/tutorial/FTC_FieldCoordinateSystemDefinition.pdf\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained in {@link ConceptVuforiaNavigation}.\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\";\n\n    OpenGLMatrix lastLocation = null;\n\n    /**\n     * {@link #vuforia} is the variable we will use to store our instance of the Vuforia\n     * localization engine.\n     */\n    VuforiaLocalizer vuforia;\n\n    @Override public void runOpMode() {\n\n        /*\n         * To start up Vuforia, tell it the view that we wish to use for camera monitor (on the RC phone);\n         */\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters(cameraMonitorViewId);\n\n        // OR...  Do Not Activate the Camera Monitor View, to save power\n        // VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        /*\n         * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n         * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n         * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n         * web site at https://developer.vuforia.com/license-manager.\n         *\n         * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n         * random data. As an example, here is a example of a fragment of a valid key:\n         *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n         * Once you've obtained a license key, copy the string from the Vuforia web site\n         * and paste it in to your code on the next line, between the double quotes.\n         */\n        parameters.vuforiaLicenseKey = \" -- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n        /*\n         * We also indicate which camera on the RC that we wish to use.\n         * Here we chose the back (HiRes) camera (for greater range), but\n         * for a competition robot, the front camera might be more convenient.\n         */\n        parameters.cameraDirection = VuforiaLocalizer.CameraDirection.BACK;\n\n        /**\n         * Instantiate the Vuforia engine\n         */\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n\n        /**\n         * Load the data set containing the VuMarks for Relic Recovery. There's only one trackable\n         * in this data set: all three of the VuMarks in the game were created from this one template,\n         * but differ in their instance id information.\n         * @see VuMarkInstanceId\n         */\n        VuforiaTrackables relicTrackables = this.vuforia.loadTrackablesFromAsset(\"RelicVuMark\");\n        VuforiaTrackable relicTemplate = relicTrackables.get(0);\n        relicTemplate.setName(\"relicVuMarkTemplate\"); // can help in debugging; otherwise not necessary\n\n        telemetry.addData(\">\", \"Press Play to start\");\n        telemetry.update();\n        waitForStart();\n\n        relicTrackables.activate();\n\n        while (opModeIsActive()) {\n\n            /**\n             * See if any of the instances of {@link relicTemplate} are currently visible.\n             * {@link RelicRecoveryVuMark} is an enum which can have the following values:\n             * UNKNOWN, LEFT, CENTER, and RIGHT. When a VuMark is visible, something other than\n             * UNKNOWN will be returned by {@link RelicRecoveryVuMark#from(VuforiaTrackable)}.\n             */\n            RelicRecoveryVuMark vuMark = RelicRecoveryVuMark.from(relicTemplate);\n            if (vuMark != RelicRecoveryVuMark.UNKNOWN) {\n\n                /* Found an instance of the template. In the actual game, you will probably\n                 * loop until this condition occurs, then move on to act accordingly depending\n                 * on which VuMark was visible. */\n                telemetry.addData(\"VuMark\", \"%s visible\", vuMark);\n\n                /* For fun, we also exhibit the navigational pose. In the Relic Recovery game,\n                 * it is perhaps unlikely that you will actually need to act on this pose information, but\n                 * we illustrate it nevertheless, for completeness. */\n                OpenGLMatrix pose = ((VuforiaTrackableDefaultListener)relicTemplate.getListener()).getPose();\n                telemetry.addData(\"Pose\", format(pose));\n\n                /* We further illustrate how to decompose the pose into useful rotational and\n                 * translational components */\n                if (pose != null) {\n                    VectorF trans = pose.getTranslation();\n                    Orientation rot = Orientation.getOrientation(pose, AxesReference.EXTRINSIC, AxesOrder.XYZ, AngleUnit.DEGREES);\n\n                    // Extract the X, Y, and Z components of the offset of the target relative to the robot\n                    double tX = trans.get(0);\n                    double tY = trans.get(1);\n                    double tZ = trans.get(2);\n\n                    // Extract the rotational components of the target relative to the robot\n                    double rX = rot.firstAngle;\n                    double rY = rot.secondAngle;\n                    double rZ = rot.thirdAngle;\n                }\n            }\n            else {\n                telemetry.addData(\"VuMark\", \"not visible\");\n            }\n\n            telemetry.update();\n        }\n    }\n\n    String format(OpenGLMatrix transformationMatrix) {\n        return (transformationMatrix != null) ? transformationMatrix.formatAsTransform() : \"null\";\n    }\n}\n",
    "SensorMRRangeSensor": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.modernrobotics.ModernRoboticsI2cRangeSensor;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\n\nimport org.firstinspires.ftc.robotcore.external.navigation.DistanceUnit;\n\n/**\n * {@link SensorMRRangeSensor} illustrates how to use the Modern Robotics\n * Range Sensor.\n *\n * The op mode assumes that the range sensor is configured with a name of \"sensor_range\".\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n *\n * @see <a href=\"http://modernroboticsinc.com/range-sensor\">MR Range Sensor</a>\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled   // comment out or remove this line to enable this opmode\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    ModernRoboticsI2cRangeSensor rangeSensor;\n\n    @Override public void runOpMode() {\n\n        // get a reference to our compass\n        rangeSensor = hardwareMap.get(ModernRoboticsI2cRangeSensor.class, \"sensor_range\");\n\n        // wait for the start button to be pressed\n        waitForStart();\n\n        while (opModeIsActive()) {\n            telemetry.addData(\"raw ultrasonic\", rangeSensor.rawUltrasonic());\n            telemetry.addData(\"raw optical\", rangeSensor.rawOptical());\n            telemetry.addData(\"cm optical\", \"%.2f cm\", rangeSensor.cmOptical());\n            telemetry.addData(\"cm\", \"%.2f cm\", rangeSensor.getDistance(DistanceUnit.CM));\n            telemetry.update();\n        }\n    }\n}\n",
    "SensorDigitalTouch": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DigitalChannel;\n\n/*\n * This is an example LinearOpMode that shows how to use\n * a REV Robotics Touch Sensor.\n *\n * It assumes that the touch sensor is configured with a name of \"sensor_digital\".\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n    /**\n     * The REV Robotics Touch Sensor\n     * is treated as a digital channel.  It is HIGH if the button is unpressed.\n     * It pulls LOW if the button is pressed.\n     *\n     * Also, when you connect a REV Robotics Touch Sensor to the digital I/O port on the\n     * Expansion Hub using a 4-wire JST cable, the second pin gets connected to the Touch Sensor.\n     * The lower (first) pin stays unconnected.*\n     */\n\n    DigitalChannel digitalTouch;  // Hardware Device Object\n\n    @Override\n    public void runOpMode() {\n\n        // get a reference to our digitalTouch object.\n        digitalTouch = hardwareMap.get(DigitalChannel.class, \"sensor_digital\");\n\n        // set the digital channel to input.\n        digitalTouch.setMode(DigitalChannel.Mode.INPUT);\n\n        // wait for the start button to be pressed.\n        waitForStart();\n\n        // while the op mode is active, loop and read the light levels.\n        // Note we use opModeIsActive() as our loop condition because it is an interruptible method.\n        while (opModeIsActive()) {\n\n            // send the info back to driver station using telemetry function.\n            // if the digital channel returns true it's HIGH and the button is unpressed.\n            if (digitalTouch.getState() == true) {\n                telemetry.addData(\"Digital Touch\", \"Is Not Pressed\");\n            } else {\n                telemetry.addData(\"Digital Touch\", \"Is Pressed\");\n            }\n\n            telemetry.update();\n        }\n    }\n}\n",
    "ConceptVuforiaNavigation": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.util.RobotLog;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.matrices.MatrixF;\nimport org.firstinspires.ftc.robotcore.external.matrices.OpenGLMatrix;\nimport org.firstinspires.ftc.robotcore.external.navigation.AngleUnit;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesOrder;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesReference;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackable;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackableDefaultListener;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackables;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\n/**\n * This 2016-2017 OpMode illustrates the basics of using the Vuforia localizer to determine\n * positioning and orientation of robot on the FTC field.\n * The code is structured as a LinearOpMode\n *\n * Vuforia uses the phone's camera to inspect it's surroundings, and attempt to locate target images.\n *\n * When images are located, Vuforia is able to determine the position and orientation of the\n * image relative to the camera.  This sample code than combines that information with a\n * knowledge of where the target images are on the field, to determine the location of the camera.\n *\n * This example assumes a \"diamond\" field configuration where the red and blue alliance stations\n * are adjacent on the corner of the field furthest from the audience.\n * From the Audience perspective, the Red driver station is on the right.\n * The two vision target are located on the two walls closest to the audience, facing in.\n * The Stones are on the RED side of the field, and the Chips are on the Blue side.\n *\n * A final calculation then uses the location of the camera on the robot to determine the\n * robot's location and orientation on the field.\n *\n * @see VuforiaLocalizer\n * @see VuforiaTrackableDefaultListener\n * see  ftc_app/doc/tutorial/FTC_FieldCoordinateSystemDefinition.pdf\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\";\n\n    OpenGLMatrix lastLocation = null;\n\n    /**\n     * {@link #vuforia} is the variable we will use to store our instance of the Vuforia\n     * localization engine.\n     */\n    VuforiaLocalizer vuforia;\n\n    @Override public void runOpMode() {\n        /*\n         * To start up Vuforia, tell it the view that we wish to use for camera monitor (on the RC phone);\n         * If no camera monitor is desired, use the parameterless constructor instead (commented out below).\n         */\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters(cameraMonitorViewId);\n\n        // OR...  Do Not Activate the Camera Monitor View, to save power\n        // VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        /*\n         * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n         * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n         * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n         * web site at https://developer.vuforia.com/license-manager.\n         *\n         * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n         * random data. As an example, here is a example of a fragment of a valid key:\n         *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n         * Once you've obtained a license key, copy the string from the Vuforia web site\n         * and paste it in to your code on the next line, between the double quotes.\n         */\n        parameters.vuforiaLicenseKey = \" -- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n        /*\n         * We also indicate which camera on the RC that we wish to use.\n         * Here we chose the back (HiRes) camera (for greater range), but\n         * for a competition robot, the front camera might be more convenient.\n         */\n        parameters.cameraDirection = VuforiaLocalizer.CameraDirection.BACK;\n\n        /**\n         * Instantiate the Vuforia engine\n         */\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        /**\n         * Load the data sets that for the trackable objects we wish to track. These particular data\n         * sets are stored in the 'assets' part of our application (you'll see them in the Android\n         * Studio 'Project' view over there on the left of the screen). You can make your own datasets\n         * with the Vuforia Target Manager: https://developer.vuforia.com/target-manager. PDFs for the\n         * example \"StonesAndChips\", datasets can be found in in this project in the\n         * documentation directory.\n         */\n        VuforiaTrackables stonesAndChips = this.vuforia.loadTrackablesFromAsset(\"StonesAndChips\");\n        VuforiaTrackable redTarget = stonesAndChips.get(0);\n        redTarget.setName(\"RedTarget\");  // Stones\n\n        VuforiaTrackable blueTarget  = stonesAndChips.get(1);\n        blueTarget.setName(\"BlueTarget\");  // Chips\n\n        /** For convenience, gather together all the trackable objects in one easily-iterable collection */\n        List<VuforiaTrackable> allTrackables = new ArrayList<VuforiaTrackable>();\n        allTrackables.addAll(stonesAndChips);\n\n        /**\n         * We use units of mm here because that's the recommended units of measurement for the\n         * size values specified in the XML for the ImageTarget trackables in data sets. E.g.:\n         *      <ImageTarget name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\"/>\n         * You don't *have to* use mm here, but the units here and the units used in the XML\n         * target configuration files *must* correspond for the math to work out correctly.\n         */\n        float mmPerInch        = 25.4f;\n        float mmBotWidth       = 18 * mmPerInch;            // ... or whatever is right for your robot\n        float mmFTCFieldWidth  = (12*12 - 2) * mmPerInch;   // the FTC field is ~11'10\" center-to-center of the glass panels\n\n        /**\n         * In order for localization to work, we need to tell the system where each target we\n         * wish to use for navigation resides on the field, and we need to specify where on the robot\n         * the phone resides. These specifications are in the form of <em>transformation matrices.</em>\n         * Transformation matrices are a central, important concept in the math here involved in localization.\n         * See <a href=\"https://en.wikipedia.org/wiki/Transformation_matrix\">Transformation Matrix</a>\n         * for detailed information. Commonly, you'll encounter transformation matrices as instances\n         * of the {@link OpenGLMatrix} class.\n         *\n         * For the most part, you don't need to understand the details of the math of how transformation\n         * matrices work inside (as fascinating as that is, truly). Just remember these key points:\n         * <ol>\n         *\n         *     <li>You can put two transformations together to produce a third that combines the effect of\n         *     both of them. If, for example, you have a rotation transform R and a translation transform T,\n         *     then the combined transformation matrix RT which does the rotation first and then the translation\n         *     is given by {@code RT = T.multiplied(R)}. That is, the transforms are multiplied in the\n         *     <em>reverse</em> of the chronological order in which they applied.</li>\n         *\n         *     <li>A common way to create useful transforms is to use methods in the {@link OpenGLMatrix}\n         *     class REPLACE_CLASS the Orientation class. See, for example, {@link OpenGLMatrix#translation(float,\n         *     float, float)}, {@link OpenGLMatrix#rotation(AngleUnit, float, float, float, float)}, and\n         *     {@link Orientation#getRotationMatrix(AxesReference, AxesOrder, AngleUnit, float, float, float)}.\n         *     Related methods in {@link OpenGLMatrix}, such as {@link OpenGLMatrix#rotated(AngleUnit,\n         *     float, float, float, float)}, are syntactic shorthands for creating a new transform and\n         *     then immediately multiplying the receiver by it, which can be convenient at times.</li>\n         *\n         *     <li>If you want to break open the black box of a transformation matrix to understand\n         *     what it's doing inside, use {@link MatrixF#getTranslation()} to fetch how much the\n         *     transform will move you in x, y, and z, and use {@link Orientation#getOrientation(MatrixF,\n         *     AxesReference, AxesOrder, AngleUnit)} to determine the rotational motion that the transform\n         *     will impart. See {@link #format(OpenGLMatrix)} below for an example.</li>\n         *\n         * </ol>\n         *\n         * This example places the \"stones\" image on the perimeter wall to the Left\n         *  of the Red Driver station wall.  Similar to the Red Beacon Location on the Res-Q\n         *\n         * This example places the \"chips\" image on the perimeter wall to the Right\n         *  of the Blue Driver station.  Similar to the Blue Beacon Location on the Res-Q\n         *\n         * See the doc folder of this project for a description of the field Axis conventions.\n         *\n         * Initially the target is conceptually lying at the origin of the field's coordinate system\n         * (the center of the field), facing up.\n         *\n         * In this configuration, the target's coordinate system aligns with that of the field.\n         *\n         * In a real situation we'd also account for the vertical (Z) offset of the target,\n         * but for simplicity, we ignore that here; for a real robot, you'll want to fix that.\n         *\n         * To place the Stones Target on the Red Audience wall:\n         * - First we rotate it 90 around the field's X axis to flip it upright\n         * - Then we rotate it  90 around the field's Z access to face it away from the audience.\n         * - Finally, we translate it back along the X axis towards the red audience wall.\n         */\n        OpenGLMatrix redTargetLocationOnField = OpenGLMatrix\n                /* Then we translate the target off to the RED WALL. Our translation here\n                is a negative translation in X.*/\n                .translation(-mmFTCFieldWidth/2, 0, 0)\n                .multiplied(Orientation.getRotationMatrix(\n                        /* First, in the fixed (field) coordinate system, we rotate 90deg in X, then 90 in Z */\n                        AxesReference.EXTRINSIC, AxesOrder.XZX,\n                        AngleUnit.DEGREES, 90, 90, 0));\n        redTarget.setLocation(redTargetLocationOnField);\n        RobotLog.ii(TAG, \"Red Target=%s\", format(redTargetLocationOnField));\n\n       /*\n        * To place the Stones Target on the Blue Audience wall:\n        * - First we rotate it 90 around the field's X axis to flip it upright\n        * - Finally, we translate it along the Y axis towards the blue audience wall.\n        */\n        OpenGLMatrix blueTargetLocationOnField = OpenGLMatrix\n                /* Then we translate the target off to the Blue Audience wall.\n                Our translation here is a positive translation in Y.*/\n                .translation(0, mmFTCFieldWidth/2, 0)\n                .multiplied(Orientation.getRotationMatrix(\n                        /* First, in the fixed (field) coordinate system, we rotate 90deg in X */\n                        AxesReference.EXTRINSIC, AxesOrder.XZX,\n                        AngleUnit.DEGREES, 90, 0, 0));\n        blueTarget.setLocation(blueTargetLocationOnField);\n        RobotLog.ii(TAG, \"Blue Target=%s\", format(blueTargetLocationOnField));\n\n        /**\n         * Create a transformation matrix describing where the phone is on the robot. Here, we\n         * put the phone on the right hand side of the robot with the screen facing in (see our\n         * choice of BACK camera above) and in landscape mode. Starting from alignment between the\n         * robot's and phone's axes, this is a rotation of -90deg along the Y axis.\n         *\n         * When determining whether a rotation is positive or negative, consider yourself as looking\n         * down the (positive) axis of rotation from the positive towards the origin. Positive rotations\n         * are then CCW, and negative rotations CW. An example: consider looking down the positive Z\n         * axis towards the origin. A positive rotation about Z (ie: a rotation parallel to the the X-Y\n         * plane) is then CCW, as one would normally expect from the usual classic 2D geometry.\n         */\n        OpenGLMatrix phoneLocationOnRobot = OpenGLMatrix\n                .translation(mmBotWidth/2,0,0)\n                .multiplied(Orientation.getRotationMatrix(\n                        AxesReference.EXTRINSIC, AxesOrder.YZY,\n                        AngleUnit.DEGREES, -90, 0, 0));\n        RobotLog.ii(TAG, \"phone=%s\", format(phoneLocationOnRobot));\n\n        /**\n         * Let the trackable listeners we care about know where the phone is. We know that each\n         * listener is a {@link VuforiaTrackableDefaultListener} and can so safely cast because\n         * we have not ourselves installed a listener of a different type.\n         */\n        ((VuforiaTrackableDefaultListener)redTarget.getListener()).setPhoneInformation(phoneLocationOnRobot, parameters.cameraDirection);\n        ((VuforiaTrackableDefaultListener)blueTarget.getListener()).setPhoneInformation(phoneLocationOnRobot, parameters.cameraDirection);\n\n        /**\n         * A brief tutorial: here's how all the math is going to work:\n         *\n         * C = phoneLocationOnRobot  maps   phone coords -> robot coords\n         * P = tracker.getPose()     maps   image target coords -> phone coords\n         * L = redTargetLocationOnField maps   image target coords -> field coords\n         *\n         * So\n         *\n         * C.inverted()              maps   robot coords -> phone coords\n         * P.inverted()              maps   phone coords -> imageTarget coords\n         *\n         * Putting that all together,\n         *\n         * L x P.inverted() x C.inverted() maps robot coords to field coords.\n         *\n         * @see VuforiaTrackableDefaultListener#getRobotLocation()\n         */\n\n        /** Wait for the game to begin */\n        telemetry.addData(\">\", \"Press Play to start tracking\");\n        telemetry.update();\n        waitForStart();\n\n        /** Start tracking the data sets we care about. */\n        stonesAndChips.activate();\n\n        while (opModeIsActive()) {\n\n            for (VuforiaTrackable trackable : allTrackables) {\n                /**\n                 * getUpdatedRobotLocation() will return null if no new information is available since\n                 * the last time that call was made, or if the trackable is not currently visible.\n                 * getRobotLocation() will return null if the trackable is not currently visible.\n                 */\n                telemetry.addData(trackable.getName(), ((VuforiaTrackableDefaultListener)trackable.getListener()).isVisible() ? \"Visible\" : \"Not Visible\");    //\n\n                OpenGLMatrix robotLocationTransform = ((VuforiaTrackableDefaultListener)trackable.getListener()).getUpdatedRobotLocation();\n                if (robotLocationTransform != null) {\n                    lastLocation = robotLocationTransform;\n                }\n            }\n            /**\n             * Provide feedback as to where the robot was last located (if we know).\n             */\n            if (lastLocation != null) {\n                //  RobotLog.vv(TAG, \"robot=%s\", format(lastLocation));\n                telemetry.addData(\"Pos\", format(lastLocation));\n            } else {\n                telemetry.addData(\"Pos\", \"Unknown\");\n            }\n            telemetry.update();\n        }\n    }\n\n    /**\n     * A simple utility that extracts positioning information from a transformation matrix\n     * and formats it in a form palatable to a human being.\n     */\n    String format(OpenGLMatrix transformationMatrix) {\n        return transformationMatrix.formatAsTransform();\n    }\n}\n",
    "PushbotAutoDriveByGyro Linear": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.modernrobotics.ModernRoboticsI2cGyro;\nimport com.qualcomm.robotcore.eventloop.opmode.Autonomous;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.hardware.DcMotor;\nimport com.qualcomm.robotcore.util.ElapsedTime;\nimport com.qualcomm.robotcore.util.Range;\n\n/**\n * This file illustrates the concept of driving a path based on Gyro heading and encoder counts.\n * It uses the common Pushbot hardware class REPLACE_CLASS define the drive on the robot.\n * The code is structured as a LinearOpMode\n *\n * The code REQUIRES that you DO have encoders on the wheels,\n *   otherwise you would use: PushbotAutoDriveByTime;\n *\n *  This code ALSO requires that you have a Modern Robotics I2C gyro with the name \"gyro\"\n *   otherwise you would use: PushbotAutoDriveByEncoder;\n *\n *  This code requires that the drive Motors have been configured such that a positive\n *  power command moves them forward, and causes the encoders to count UP.\n *\n *  This code uses the RUN_TO_POSITION mode to enable the Motor controllers to generate the run profile\n *\n *  In order to calibrate the Gyro correctly, the robot must remain stationary during calibration.\n *  This is performed when the INIT button is pressed on the Driver Station.\n *  This code assumes that the robot is stationary when the INIT button is pressed.\n *  If this is not the case, then the INIT should be performed again.\n *\n *  Note: in this example, all angles are referenced to the initial coordinate frame set during the\n *  the Gyro Calibration process, or whenever the program issues a resetZAxisIntegrator() call on the Gyro.\n *\n *  The angle of movement/rotation is assumed to be a standardized rotation around the robot Z axis,\n *  which means that a Positive rotation is Counter Clock Wise, looking down on the field.\n *  This is consistent with the FTC field coordinate conventions set out in the document:\n *  ftc_app\\doc\\tutorial\\FTC_FieldCoordinateSystemDefinition.pdf\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@Autonomous(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    /* Declare OpMode members. */\n    HardwarePushbot         robot   = new HardwarePushbot();   // Use a Pushbot's hardware\n    ModernRoboticsI2cGyro   gyro    = null;                    // Additional Gyro device\n\n    static final double     COUNTS_PER_MOTOR_REV    = 1440 ;    // eg: TETRIX Motor Encoder\n    static final double     DRIVE_GEAR_REDUCTION    = 2.0 ;     // This is < 1.0 if geared UP\n    static final double     WHEEL_DIAMETER_INCHES   = 4.0 ;     // For figuring circumference\n    static final double     COUNTS_PER_INCH         = (COUNTS_PER_MOTOR_REV * DRIVE_GEAR_REDUCTION) /\n                                                      (WHEEL_DIAMETER_INCHES * 3.1415);\n\n    // These constants define the desired driving/control characteristics\n    // The can/should be tweaked to suite the specific robot drive train.\n    static final double     DRIVE_SPEED             = 0.7;     // Nominal speed for better accuracy.\n    static final double     TURN_SPEED              = 0.5;     // Nominal half speed for better accuracy.\n\n    static final double     HEADING_THRESHOLD       = 1 ;      // As tight as we can make it with an integer gyro\n    static final double     P_TURN_COEFF            = 0.1;     // Larger is more responsive, but also less stable\n    static final double     P_DRIVE_COEFF           = 0.15;     // Larger is more responsive, but also less stable\n\n\n    @Override\n    public void runOpMode() {\n\n        /*\n         * Initialize the standard drive system variables.\n         * The init() method of the hardware class REPLACE_CLASS most of the work here\n         */\n        robot.init(hardwareMap);\n        gyro = (ModernRoboticsI2cGyro)hardwareMap.gyroSensor.get(\"gyro\");\n\n        // Ensure the robot it stationary, then reset the encoders and calibrate the gyro.\n        robot.leftDrive.setMode(DcMotor.RunMode.STOP_AND_RESET_ENCODER);\n        robot.rightDrive.setMode(DcMotor.RunMode.STOP_AND_RESET_ENCODER);\n\n        // Send telemetry message to alert driver that we are calibrating;\n        telemetry.addData(\">\", \"Calibrating Gyro\");    //\n        telemetry.update();\n\n        gyro.calibrate();\n\n        // make sure the gyro is calibrated before continuing\n        while (!isStopRequested() && gyro.isCalibrating())  {\n            sleep(50);\n            idle();\n        }\n\n        telemetry.addData(\">\", \"Robot Ready.\");    //\n        telemetry.update();\n\n        robot.leftDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n        robot.rightDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n\n        // Wait for the game to start (Display Gyro value), and reset gyro before we move..\n        while (!isStarted()) {\n            telemetry.addData(\">\", \"Robot Heading = %d\", gyro.getIntegratedZValue());\n            telemetry.update();\n        }\n\n        gyro.resetZAxisIntegrator();\n\n        // Step through each leg of the path,\n        // Note: Reverse movement is obtained by setting a negative distance (not speed)\n        // Put a hold after each turn\n        gyroDrive(DRIVE_SPEED, 48.0, 0.0);    // Drive FWD 48 inches\n        gyroTurn( TURN_SPEED, -45.0);         // Turn  CCW to -45 Degrees\n        gyroHold( TURN_SPEED, -45.0, 0.5);    // Hold -45 Deg heading for a 1/2 second\n        gyroDrive(DRIVE_SPEED, 12.0, -45.0);  // Drive FWD 12 inches at 45 degrees\n        gyroTurn( TURN_SPEED,  45.0);         // Turn  CW  to  45 Degrees\n        gyroHold( TURN_SPEED,  45.0, 0.5);    // Hold  45 Deg heading for a 1/2 second\n        gyroTurn( TURN_SPEED,   0.0);         // Turn  CW  to   0 Degrees\n        gyroHold( TURN_SPEED,   0.0, 1.0);    // Hold  0 Deg heading for a 1 second\n        gyroDrive(DRIVE_SPEED,-48.0, 0.0);    // Drive REV 48 inches\n\n        telemetry.addData(\"Path\", \"Complete\");\n        telemetry.update();\n    }\n\n\n   /**\n    *  Method to drive on a fixed compass bearing (angle), based on encoder counts.\n    *  Move will stop if either of these conditions occur:\n    *  1) Move gets to the desired position\n    *  2) Driver stops the opmode running.\n    *\n    * @param speed      Target speed for forward motion.  Should allow for _/- variance for adjusting heading\n    * @param distance   Distance (in inches) to move from current position.  Negative distance means move backwards.\n    * @param angle      Absolute Angle (in Degrees) relative to last gyro reset.\n    *                   0 = fwd. +ve is CCW from fwd. -ve is CW from forward.\n    *                   If a relative angle is required, add/subtract from current heading.\n    */\n    public void gyroDrive ( double speed,\n                            double distance,\n                            double angle) {\n\n        int     newLeftTarget;\n        int     newRightTarget;\n        int     moveCounts;\n        double  max;\n        double  error;\n        double  steer;\n        double  leftSpeed;\n        double  rightSpeed;\n\n        // Ensure that the opmode is still active\n        if (opModeIsActive()) {\n\n            // Determine new target position, and pass to motor controller\n            moveCounts = (int)(distance * COUNTS_PER_INCH);\n            newLeftTarget = robot.leftDrive.getCurrentPosition() + moveCounts;\n            newRightTarget = robot.rightDrive.getCurrentPosition() + moveCounts;\n\n            // Set Target and Turn On RUN_TO_POSITION\n            robot.leftDrive.setTargetPosition(newLeftTarget);\n            robot.rightDrive.setTargetPosition(newRightTarget);\n\n            robot.leftDrive.setMode(DcMotor.RunMode.RUN_TO_POSITION);\n            robot.rightDrive.setMode(DcMotor.RunMode.RUN_TO_POSITION);\n\n            // start motion.\n            speed = Range.clip(Math.abs(speed), 0.0, 1.0);\n            robot.leftDrive.setPower(speed);\n            robot.rightDrive.setPower(speed);\n\n            // keep looping while we are still active, and BOTH motors are running.\n            while (opModeIsActive() &&\n                   (robot.leftDrive.isBusy() && robot.rightDrive.isBusy())) {\n\n                // adjust relative speed based on heading error.\n                error = getError(angle);\n                steer = getSteer(error, P_DRIVE_COEFF);\n\n                // if driving in reverse, the motor correction also needs to be reversed\n                if (distance < 0)\n                    steer *= -1.0;\n\n                leftSpeed = speed - steer;\n                rightSpeed = speed + steer;\n\n                // Normalize speeds if either one exceeds +/- 1.0;\n                max = Math.max(Math.abs(leftSpeed), Math.abs(rightSpeed));\n                if (max > 1.0)\n                {\n                    leftSpeed /= max;\n                    rightSpeed /= max;\n                }\n\n                robot.leftDrive.setPower(leftSpeed);\n                robot.rightDrive.setPower(rightSpeed);\n\n                // Display drive status for the driver.\n                telemetry.addData(\"Err/St\",  \"%5.1f/%5.1f\",  error, steer);\n                telemetry.addData(\"Target\",  \"%7d:%7d\",      newLeftTarget,  newRightTarget);\n                telemetry.addData(\"Actual\",  \"%7d:%7d\",      robot.leftDrive.getCurrentPosition(),\n                                                             robot.rightDrive.getCurrentPosition());\n                telemetry.addData(\"Speed\",   \"%5.2f:%5.2f\",  leftSpeed, rightSpeed);\n                telemetry.update();\n            }\n\n            // Stop all motion;\n            robot.leftDrive.setPower(0);\n            robot.rightDrive.setPower(0);\n\n            // Turn off RUN_TO_POSITION\n            robot.leftDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n            robot.rightDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n        }\n    }\n\n    /**\n     *  Method to spin on central axis to point in a new direction.\n     *  Move will stop if either of these conditions occur:\n     *  1) Move gets to the heading (angle)\n     *  2) Driver stops the opmode running.\n     *\n     * @param speed Desired speed of turn.\n     * @param angle      Absolute Angle (in Degrees) relative to last gyro reset.\n     *                   0 = fwd. +ve is CCW from fwd. -ve is CW from forward.\n     *                   If a relative angle is required, add/subtract from current heading.\n     */\n    public void gyroTurn (  double speed, double angle) {\n\n        // keep looping while we are still active, and not on heading.\n        while (opModeIsActive() && !onHeading(speed, angle, P_TURN_COEFF)) {\n            // Update telemetry & Allow time for other processes to run.\n            telemetry.update();\n        }\n    }\n\n    /**\n     *  Method to obtain & hold a heading for a finite amount of time\n     *  Move will stop once the requested time has elapsed\n     *\n     * @param speed      Desired speed of turn.\n     * @param angle      Absolute Angle (in Degrees) relative to last gyro reset.\n     *                   0 = fwd. +ve is CCW from fwd. -ve is CW from forward.\n     *                   If a relative angle is required, add/subtract from current heading.\n     * @param holdTime   Length of time (in seconds) to hold the specified heading.\n     */\n    public void gyroHold( double speed, double angle, double holdTime) {\n\n        ElapsedTime holdTimer = new ElapsedTime();\n\n        // keep looping while we have time remaining.\n        holdTimer.reset();\n        while (opModeIsActive() && (holdTimer.time() < holdTime)) {\n            // Update telemetry & Allow time for other processes to run.\n            onHeading(speed, angle, P_TURN_COEFF);\n            telemetry.update();\n        }\n\n        // Stop all motion;\n        robot.leftDrive.setPower(0);\n        robot.rightDrive.setPower(0);\n    }\n\n    /**\n     * Perform one cycle of closed loop heading control.\n     *\n     * @param speed     Desired speed of turn.\n     * @param angle     Absolute Angle (in Degrees) relative to last gyro reset.\n     *                  0 = fwd. +ve is CCW from fwd. -ve is CW from forward.\n     *                  If a relative angle is required, add/subtract from current heading.\n     * @param PCoeff    Proportional Gain coefficient\n     * @return\n     */\n    boolean onHeading(double speed, double angle, double PCoeff) {\n        double   error ;\n        double   steer ;\n        boolean  onTarget = false ;\n        double leftSpeed;\n        double rightSpeed;\n\n        // determine turn power based on +/- error\n        error = getError(angle);\n\n        if (Math.abs(error) <= HEADING_THRESHOLD) {\n            steer = 0.0;\n            leftSpeed  = 0.0;\n            rightSpeed = 0.0;\n            onTarget = true;\n        }\n        else {\n            steer = getSteer(error, PCoeff);\n            rightSpeed  = speed * steer;\n            leftSpeed   = -rightSpeed;\n        }\n\n        // Send desired speeds to motors.\n        robot.leftDrive.setPower(leftSpeed);\n        robot.rightDrive.setPower(rightSpeed);\n\n        // Display it for the driver.\n        telemetry.addData(\"Target\", \"%5.2f\", angle);\n        telemetry.addData(\"Err/St\", \"%5.2f/%5.2f\", error, steer);\n        telemetry.addData(\"Speed.\", \"%5.2f:%5.2f\", leftSpeed, rightSpeed);\n\n        return onTarget;\n    }\n\n    /**\n     * getError determines the error between the target angle and the robot's current heading\n     * @param   targetAngle  Desired angle (relative to global reference established at last Gyro Reset).\n     * @return  error angle: Degrees in the range +/- 180. Centered on the robot's frame of reference\n     *          +ve error means the robot should turn LEFT (CCW) to reduce error.\n     */\n    public double getError(double targetAngle) {\n\n        double robotError;\n\n        // calculate error in -179 to +180 range  (\n        robotError = targetAngle - gyro.getIntegratedZValue();\n        while (robotError > 180)  robotError -= 360;\n        while (robotError <= -180) robotError += 360;\n        return robotError;\n    }\n\n    /**\n     * returns desired steering force.  +/- 1 range.  +ve = steer left\n     * @param error   Error angle in robot relative degrees\n     * @param PCoeff  Proportional Gain Coefficient\n     * @return\n     */\n    public double getSteer(double error, double PCoeff) {\n        return Range.clip(error * PCoeff, -1, 1);\n    }\n\n}\n",
    "SensorDIO": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DeviceInterfaceModule;\nimport com.qualcomm.robotcore.hardware.DigitalChannel;\n\n/*\n * This is an example LinearOpMode that shows how to use the digital inputs and outputs on the\n * the Modern Robotics Device Interface Module.  In addition, it shows how to use the Red and Blue LED\n *\n * This op mode assumes that there is a Device Interface Module attached, named 'dim'.\n * On this DIM there is a digital input named 'digin' and an output named 'digout'\n *\n * To fully exercise this sample, connect pin 3 of the digin connector to pin 3 of the digout.\n * Note: Pin 1 is indicated by the black stripe, so pin 3 is at the opposite end.\n *\n * The X button on the gamepad will be used to activate the digital output pin.\n * The Red/Blue LED will be used to indicate the state of the digital input pin.\n * Blue = false (0V), Red = true (5V)\n * If the two pins are linked, the gamepad will change the LED color.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n*/\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\nfinal int BLUE_LED_CHANNEL = 0;\nfinal int RED_LED_CHANNEL = 1;\n\n  @Override\n  public void runOpMode() {\n\n    boolean               inputPin;             // Input State\n    boolean               outputPin;            // Output State\n    DeviceInterfaceModule dim;                  // Device Object\n    DigitalChannel        digIn;                // Device Object\n    DigitalChannel        digOut;               // Device Object\n\n    // get a reference to a Modern Robotics DIM, and IO channels.\n    dim = hardwareMap.get(DeviceInterfaceModule.class, \"dim\");   //  Use generic form of device mapping\n    digIn  = hardwareMap.get(DigitalChannel.class, \"digin\");     //  Use generic form of device mapping\n    digOut = hardwareMap.get(DigitalChannel.class, \"digout\");    //  Use generic form of device mapping\n\n    digIn.setMode(DigitalChannel.Mode.INPUT);          // Set the direction of each channel\n    digOut.setMode(DigitalChannel.Mode.OUTPUT);\n\n    // wait for the start button to be pressed.\n    telemetry.addData(\">\", \"Press play, and then user X button to set DigOut\");\n    telemetry.update();\n    waitForStart();\n\n    while (opModeIsActive())  {\n\n        outputPin = gamepad1.x ;        //  Set the output pin based on x button\n        digOut.setState(outputPin);\n        inputPin = digIn.getState();    //  Read the input pin\n\n        // Display input pin state on LEDs\n        if (inputPin) {\n            dim.setLED(RED_LED_CHANNEL, true);\n            dim.setLED(BLUE_LED_CHANNEL, false);\n        }\n        else {\n            dim.setLED(RED_LED_CHANNEL, false);\n            dim.setLED(BLUE_LED_CHANNEL, true);\n        }\n\n        telemetry.addData(\"Output\", outputPin );\n        telemetry.addData(\"Input\", inputPin );\n        telemetry.addData(\"LED\",   inputPin ? \"Red\" : \"Blue\" );\n        telemetry.update();\n    }\n  }\n}\n",
    "ConceptVuMarkIdentificationWebcam": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.robotcore.external.matrices.OpenGLMatrix;\nimport org.firstinspires.ftc.robotcore.external.matrices.VectorF;\nimport org.firstinspires.ftc.robotcore.external.navigation.AngleUnit;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesOrder;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesReference;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.external.navigation.RelicRecoveryVuMark;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuMarkInstanceId;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackable;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackableDefaultListener;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackables;\n\n/**\n * This OpMode illustrates the basics of using the Vuforia engine to determine\n * the identity of Vuforia VuMarks encountered on the field. The code is structured as\n * a LinearOpMode. It shares much structure with {@link ConceptVuforiaNavigationWebcam}; we do not here\n * duplicate the core Vuforia documentation found there, but rather instead focus on the\n * differences between the use of Vuforia for navigation vs VuMark identification.\n *\n * @see ConceptVuforiaNavigationWebcam\n * @see VuforiaLocalizer\n * @see VuforiaTrackableDefaultListener\n * see  ftc_app/doc/tutorial/FTC_FieldCoordinateSystemDefinition.pdf\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained in {@link ConceptVuforiaNavigationWebcam}.\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\";\n\n    OpenGLMatrix lastLocation = null;\n\n    /**\n     * {@link #vuforia} is the variable we will use to store our instance of the Vuforia\n     * localization engine.\n     */\n    VuforiaLocalizer vuforia;\n\n    /**\n     * This is the webcam we are to use. As with other hardware devices such as motors and\n     * servos, this device is identified using the robot configuration tool in the FTC application.\n     */\n    WebcamName webcamName;\n\n    @Override public void runOpMode() {\n\n        /*\n         * Retrieve the camera we are to use.\n         */\n        webcamName = hardwareMap.get(WebcamName.class, \"Webcam 1\");\n\n        /*\n         * To start up Vuforia, tell it the view that we wish to use for camera monitor (on the RC phone);\n         * If no camera monitor is desired, use the parameterless constructor instead (commented out below).\n         */\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters(cameraMonitorViewId);\n\n        // OR...  Do Not Activate the Camera Monitor View, to save power\n        // VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        /*\n         * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n         * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n         * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n         * web site at https://developer.vuforia.com/license-manager.\n         *\n         * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n         * random data. As an example, here is a example of a fragment of a valid key:\n         *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n         * Once you've obtained a license key, copy the string from the Vuforia web site\n         * and paste it in to your code on the next line, between the double quotes.\n         */\n        parameters.vuforiaLicenseKey = \" -- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n\n        /**\n         * We also indicate which camera on the RC we wish to use. For pedagogical purposes,\n         * we use the same logic as in {@link ConceptVuforiaNavigationWebcam}.\n         */\n        parameters.cameraName = webcamName;\n        this.vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        /**\n         * Load the data set containing the VuMarks for Relic Recovery. There's only one trackable\n         * in this data set: all three of the VuMarks in the game were created from this one template,\n         * but differ in their instance id information.\n         * @see VuMarkInstanceId\n         */\n        VuforiaTrackables relicTrackables = this.vuforia.loadTrackablesFromAsset(\"RelicVuMark\");\n        VuforiaTrackable relicTemplate = relicTrackables.get(0);\n        relicTemplate.setName(\"relicVuMarkTemplate\"); // can help in debugging; otherwise not necessary\n\n        telemetry.addData(\">\", \"Press Play to start\");\n        telemetry.update();\n        waitForStart();\n\n        relicTrackables.activate();\n\n        while (opModeIsActive()) {\n\n            /**\n             * See if any of the instances of {@link relicTemplate} are currently visible.\n             * {@link RelicRecoveryVuMark} is an enum which can have the following values:\n             * UNKNOWN, LEFT, CENTER, and RIGHT. When a VuMark is visible, something other than\n             * UNKNOWN will be returned by {@link RelicRecoveryVuMark#from(VuforiaTrackable)}.\n             */\n            RelicRecoveryVuMark vuMark = RelicRecoveryVuMark.from(relicTemplate);\n            if (vuMark != RelicRecoveryVuMark.UNKNOWN) {\n\n                /* Found an instance of the template. In the actual game, you will probably\n                 * loop until this condition occurs, then move on to act accordingly depending\n                 * on which VuMark was visible. */\n                telemetry.addData(\"VuMark\", \"%s visible\", vuMark);\n\n                /* For fun, we also exhibit the navigational pose. In the Relic Recovery game,\n                 * it is perhaps unlikely that you will actually need to act on this pose information, but\n                 * we illustrate it nevertheless, for completeness. */\n                OpenGLMatrix pose = ((VuforiaTrackableDefaultListener)relicTemplate.getListener()).getFtcCameraFromTarget();\n                telemetry.addData(\"Pose\", format(pose));\n\n                /* We further illustrate how to decompose the pose into useful rotational and\n                 * translational components */\n                if (pose != null) {\n                    VectorF trans = pose.getTranslation();\n                    Orientation rot = Orientation.getOrientation(pose, AxesReference.EXTRINSIC, AxesOrder.XYZ, AngleUnit.DEGREES);\n\n                    // Extract the X, Y, and Z components of the offset of the target relative to the robot\n                    double tX = trans.get(0);\n                    double tY = trans.get(1);\n                    double tZ = trans.get(2);\n\n                    // Extract the rotational components of the target relative to the robot\n                    double rX = rot.firstAngle;\n                    double rY = rot.secondAngle;\n                    double rZ = rot.thirdAngle;\n                }\n            }\n            else {\n                telemetry.addData(\"VuMark\", \"not visible\");\n            }\n\n            telemetry.update();\n        }\n    }\n\n    String format(OpenGLMatrix transformationMatrix) {\n        return (transformationMatrix != null) ? transformationMatrix.formatAsTransform() : \"null\";\n    }\n}\n",
    "SensorREV2mDistance": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.rev.Rev2mDistanceSensor;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.hardware.DistanceSensor;\n\nimport org.firstinspires.ftc.robotcore.external.navigation.DistanceUnit;\n\n/**\n * {@link SensorREV2mDistance} illustrates how to use the REV Robotics\n * Time-of-Flight Range Sensor.\n *\n * The op mode assumes that the range sensor is configured with a name of \"sensor_range\".\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n *\n * @see <a href=\"http://revrobotics.com\">REV Robotics Web Page</a>\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    private DistanceSensor sensorRange;\n\n    @Override\n    public void runOpMode() {\n        // you can use this as a regular DistanceSensor.\n        sensorRange = hardwareMap.get(DistanceSensor.class, \"sensor_range\");\n\n        // you can also cast this to a Rev2mDistanceSensor if you want to use added\n        // methods associated with the Rev2mDistanceSensor class.\n        Rev2mDistanceSensor sensorTimeOfFlight = (Rev2mDistanceSensor)sensorRange;\n\n        telemetry.addData(\">>\", \"Press start to continue\");\n        telemetry.update();\n\n        waitForStart();\n        while(opModeIsActive()) {\n            // generic DistanceSensor methods.\n            telemetry.addData(\"deviceName\",sensorRange.getDeviceName() );\n            telemetry.addData(\"range\", String.format(\"%.01f mm\", sensorRange.getDistance(DistanceUnit.MM)));\n            telemetry.addData(\"range\", String.format(\"%.01f cm\", sensorRange.getDistance(DistanceUnit.CM)));\n            telemetry.addData(\"range\", String.format(\"%.01f m\", sensorRange.getDistance(DistanceUnit.METER)));\n            telemetry.addData(\"range\", String.format(\"%.01f in\", sensorRange.getDistance(DistanceUnit.INCH)));\n\n            // Rev2mDistanceSensor specific methods.\n            telemetry.addData(\"ID\", String.format(\"%x\", sensorTimeOfFlight.getModelID()));\n            telemetry.addData(\"did time out\", Boolean.toString(sensorTimeOfFlight.didTimeoutOccur()));\n\n            telemetry.update();\n        }\n    }\n\n}",
    "ConceptSoundsASJava": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.ftccommon.SoundPlayer;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\n\nimport java.io.File;\n\n/**\n * This file demonstrates how to play simple sounds on both the RC and DS phones.\n * It illustrates how to build sounds into your application as a resource.\n * This technique is best suited for use with Android Studio since it assumes you will be creating a new application\n *\n * If you are using OnBotJava, please see the ConceptSoundsOnBotJava sample\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n *\n * Operation:\n *\n * Gamepad X & B buttons are used to trigger sounds in this example, but any event can be used.\n * Note: Time should be allowed for sounds to complete before playing other sounds.\n *\n * For sound files to be used as a compiled-in resource, they need to be located in a folder called \"raw\" under your \"res\" (resources) folder.\n * You can create your own \"raw\" folder from scratch, or you can copy the one from the FtcRobotController module.\n *\n *     Android Studio coders will ultimately need a folder in your path as follows:\n *       <project root>/TeamCode/src/main/res/raw\n *\n *     Copy any .wav files you want to play into this folder.\n *     Make sure that your files ONLY use lower-case characters, and have no spaces or special characters other than underscore.\n *\n *     The name you give your .wav files will become the resource ID for these sounds.\n *     eg:  gold.wav becomes R.raw.gold\n *\n *     If you wish to use the sounds provided for this sample, they are located in:\n *     <project root>/FtcRobotController/src/main/res/raw\n *     You can copy and paste the entire 'raw' folder using Android Studio.\n *\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    // Declare OpMode members.\n    private boolean goldFound;      // Sound file present flags\n    private boolean silverFound;\n\n    private boolean isX = false;    // Gamepad button state variables\n    private boolean isB = false;\n\n    private boolean wasX = false;   // Gamepad button history variables\n    private boolean WasB = false;\n\n    @Override\n    public void runOpMode() {\n\n        // Determine Resource IDs for sounds built into the RC application.\n        int silverSoundID = hardwareMap.appContext.getResources().getIdentifier(\"silver\", \"raw\", hardwareMap.appContext.getPackageName());\n        int goldSoundID   = hardwareMap.appContext.getResources().getIdentifier(\"gold\",   \"raw\", hardwareMap.appContext.getPackageName());\n\n        // Determine if sound resources are found.\n        // Note: Preloading is NOT required, but it's a good way to verify all your sounds are available before you run.\n        if (goldSoundID != 0)\n            goldFound   = SoundPlayer.getInstance().preload(hardwareMap.appContext, goldSoundID);\n\n        if (silverSoundID != 0)\n            silverFound = SoundPlayer.getInstance().preload(hardwareMap.appContext, silverSoundID);\n\n        // Display sound status\n        telemetry.addData(\"gold resource\",   goldFound ?   \"Found\" : \"NOT found\\n Add gold.wav to /src/main/res/raw\" );\n        telemetry.addData(\"silver resource\", silverFound ? \"Found\" : \"Not found\\n Add silver.wav to /src/main/res/raw\" );\n\n        // Wait for the game to start (driver presses PLAY)\n        telemetry.addData(\">\", \"Press Start to continue\");\n        telemetry.update();\n        waitForStart();\n\n        telemetry.addData(\">\", \"Press X, B to play sounds.\");\n        telemetry.update();\n\n        // run until the end of the match (driver presses STOP)\n        while (opModeIsActive()) {\n\n            // say Silver each time gamepad X is pressed (This sound is a resource)\n            if (silverFound && (isX = gamepad1.x) && !wasX) {\n                SoundPlayer.getInstance().startPlaying(hardwareMap.appContext, silverSoundID);\n                telemetry.addData(\"Playing\", \"Resource Silver\");\n                telemetry.update();\n            }\n\n            // say Gold each time gamepad B is pressed  (This sound is a resource)\n            if (goldFound && (isB = gamepad1.b) && !WasB) {\n                SoundPlayer.getInstance().startPlaying(hardwareMap.appContext, goldSoundID);\n                telemetry.addData(\"Playing\", \"Resource Gold\");\n                telemetry.update();\n            }\n\n            // Save last button states\n            wasX = isX;\n            WasB = isB;\n        }\n    }\n}\n",
    "ConceptTensorFlowObjectDetectionWebcam": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport java.util.List;\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.tfod.TFObjectDetector;\nimport org.firstinspires.ftc.robotcore.external.tfod.Recognition;\n\n/**\n * This 2020-2021 OpMode illustrates the basics of using the TensorFlow Object Detection API to\n * determine the position of the Freight Frenzy game elements.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n  /* Note: This sample uses the all-objects Tensor Flow model (FreightFrenzy_BCDM.tflite), which contains\n   * the following 4 detectable objects\n   *  0: Ball,\n   *  1: Cube,\n   *  2: Duck,\n   *  3: Marker (duck location tape marker)\n   *\n   *  Two additional model assets are available which only contain a subset of the objects:\n   *  FreightFrenzy_BC.tflite  0: Ball,  1: Cube\n   *  FreightFrenzy_DM.tflite  0: Duck,  1: Marker\n   */\n    private static final String TFOD_MODEL_ASSET = \"FreightFrenzy_BCDM.tflite\";\n    private static final String[] LABELS = {\n      \"Ball\",\n      \"Cube\",\n      \"Duck\",\n      \"Marker\"\n    };\n\n    /*\n     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n     * web site at https://developer.vuforia.com/license-manager.\n     *\n     * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n     * random data. As an example, here is a example of a fragment of a valid key:\n     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n     * Once you've obtained a license key, copy the string from the Vuforia web site\n     * and paste it in to your code on the next line, between the double quotes.\n     */\n    private static final String VUFORIA_KEY =\n            \" -- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n    /**\n     * {@link #vuforia} is the variable we will use to store our instance of the Vuforia\n     * localization engine.\n     */\n    private VuforiaLocalizer vuforia;\n\n    /**\n     * {@link #tfod} is the variable we will use to store our instance of the TensorFlow Object\n     * Detection engine.\n     */\n    private TFObjectDetector tfod;\n\n    @Override\n    public void runOpMode() {\n        // The TFObjectDetector uses the camera frames from the VuforiaLocalizer, so we create that\n        // first.\n        initVuforia();\n        initTfod();\n\n        /**\n         * Activate TensorFlow Object Detection before we wait for the start command.\n         * Do it here so that the Camera Stream window will have the TensorFlow annotations visible.\n         **/\n        if (tfod != null) {\n            tfod.activate();\n\n            // The TensorFlow software will scale the input images from the camera to a lower resolution.\n            // This can result in lower detection accuracy at longer distances (> 55cm or 22\").\n            // If your target is at distance greater than 50 cm (20\") you can adjust the magnification value\n            // to artificially zoom in to the center of image.  For best results, the \"aspectRatio\" argument\n            // should be set to the value of the images used to create the TensorFlow Object Detection model\n            // (typically 16/9).\n            tfod.setZoom(2.5, 16.0/9.0);\n        }\n\n        /** Wait for the game to begin */\n        telemetry.addData(\">\", \"Press Play to start op mode\");\n        telemetry.update();\n        waitForStart();\n\n        if (opModeIsActive()) {\n            while (opModeIsActive()) {\n                if (tfod != null) {\n                    // getUpdatedRecognitions() will return null if no new information is available since\n                    // the last time that call was made.\n                    List<Recognition> updatedRecognitions = tfod.getUpdatedRecognitions();\n                    if (updatedRecognitions != null) {\n                      telemetry.addData(\"# Object Detected\", updatedRecognitions.size());\n                      // step through the list of recognitions and display boundary info.\n                      int i = 0;\n                      for (Recognition recognition : updatedRecognitions) {\n                        telemetry.addData(String.format(\"label (%d)\", i), recognition.getLabel());\n                        telemetry.addData(String.format(\"  left,top (%d)\", i), \"%.03f , %.03f\",\n                                recognition.getLeft(), recognition.getTop());\n                        telemetry.addData(String.format(\"  right,bottom (%d)\", i), \"%.03f , %.03f\",\n                                recognition.getRight(), recognition.getBottom());\n                        i++;\n                      }\n                      telemetry.update();\n                    }\n                }\n            }\n        }\n    }\n\n    /**\n     * Initialize the Vuforia localization engine.\n     */\n    private void initVuforia() {\n        /*\n         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.\n         */\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        parameters.vuforiaLicenseKey = VUFORIA_KEY;\n        parameters.cameraName = hardwareMap.get(WebcamName.class, \"Webcam 1\");\n\n        //  Instantiate the Vuforia engine\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        // Loading trackables is not necessary for the TensorFlow Object Detection engine.\n    }\n\n    /**\n     * Initialize the TensorFlow Object Detection engine.\n     */\n    private void initTfod() {\n        int tfodMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\n            \"tfodMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        TFObjectDetector.Parameters tfodParameters = new TFObjectDetector.Parameters(tfodMonitorViewId);\n       tfodParameters.minResultConfidence = 0.8f;\n       tfodParameters.isModelTensorFlow2 = true;\n       tfodParameters.inputSize = 320;\n       tfod = ClassFactory.getInstance().createTFObjectDetector(tfodParameters, vuforia);\n       tfod.loadModelFromAsset(TFOD_MODEL_ASSET, LABELS);\n    }\n}\n",
    "SensorMRIrSeeker": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.IrSeekerSensor;\n\n/*\n * This is an example LinearOpMode that shows how to use\n * the Modern Robotics ITR Seeker\n *\n * The op mode assumes that the IR Seeker\n * is configured with a name of \"sensor_ir\".\n *\n * Set the switch on the Modern Robotics IR beacon to 1200 at 180.  <br>\n * Turn on the IR beacon.\n * Make sure the side of the beacon with the LED on is facing the robot. <br>\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n  @Override\n  public void runOpMode() {\n\n    IrSeekerSensor irSeeker;    // Hardware Device Object\n\n    // get a reference to our GyroSensor object.\n    irSeeker = hardwareMap.get(IrSeekerSensor.class, \"sensor_ir\");\n\n    // wait for the start button to be pressed.\n    waitForStart();\n\n    while (opModeIsActive())  {\n\n      // Ensure we have a IR signal\n      if (irSeeker.signalDetected())\n      {\n        // Display angle and strength\n        telemetry.addData(\"Angle\",    irSeeker.getAngle());\n        telemetry.addData(\"Strength\", irSeeker.getStrength());\n      }\n      else\n      {\n        // Display loss of signal\n        telemetry.addData(\"Seeker\", \"Signal Lost\");\n      }\n\n      telemetry.update();\n    }\n  }\n}\n",
    "ConceptVuforiaUltimateGoalNavigation": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.matrices.OpenGLMatrix;\nimport org.firstinspires.ftc.robotcore.external.matrices.VectorF;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackable;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackableDefaultListener;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackables;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport static org.firstinspires.ftc.robotcore.external.navigation.AngleUnit.DEGREES;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesOrder.XYZ;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesOrder.YZX;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesReference.EXTRINSIC;\nimport static org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer.CameraDirection.BACK;\n\n/**\n * This 2020-2021 OpMode illustrates the basics of using the Vuforia localizer to determine\n * positioning and orientation of robot on the ULTIMATE GOAL FTC field.\n * The code is structured as a LinearOpMode\n *\n * When images are located, Vuforia is able to determine the position and orientation of the\n * image relative to the camera.  This sample code then combines that information with a\n * knowledge of where the target images are on the field, to determine the location of the camera.\n *\n * From the Audience perspective, the Red Alliance station is on the right and the\n * Blue Alliance Station is on the left.\n\n * There are a total of five image targets for the ULTIMATE GOAL game.\n * Three of the targets are placed in the center of the Red Alliance, Audience (Front),\n * and Blue Alliance perimeter walls.\n * Two additional targets are placed on the perimeter wall, one in front of each Tower Goal.\n * Refer to the Field Setup manual for more specific location details\n *\n * A final calculation then uses the location of the camera on the robot to determine the\n * robot's location and orientation on the field.\n *\n * @see VuforiaLocalizer\n * @see VuforiaTrackableDefaultListener\n * see  ultimategoal/doc/tutorial/FTC_FieldCoordinateSystemDefinition.pdf\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    // IMPORTANT:  For Phone Camera, set 1) the camera source and 2) the orientation, based on how your phone is mounted:\n    // 1) Camera Source.  Valid choices are:  BACK (behind screen) or FRONT (selfie side)\n    // 2) Phone Orientation. Choices are: PHONE_IS_PORTRAIT = true (portrait) or PHONE_IS_PORTRAIT = false (landscape)\n    //\n    // NOTE: If you are running on a CONTROL HUB, with only one USB WebCam, you must select CAMERA_CHOICE = BACK; and PHONE_IS_PORTRAIT = false;\n    //\n    private static final VuforiaLocalizer.CameraDirection CAMERA_CHOICE = BACK;\n    private static final boolean PHONE_IS_PORTRAIT = false  ;\n\n    /*\n     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n     * web site at https://developer.vuforia.com/license-manager.\n     *\n     * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n     * random data. As an example, here is a example of a fragment of a valid key:\n     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n     * Once you've obtained a license key, copy the string from the Vuforia web site\n     * and paste it in to your code on the next line, between the double quotes.\n     */\n    private static final String VUFORIA_KEY =\n            \" -- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n    // Since ImageTarget trackables use mm to specifiy their dimensions, we must use mm for all the physical dimension.\n    // We will define some constants and conversions here\n    private static final float mmPerInch        = 25.4f;\n    private static final float mmTargetHeight   = (6) * mmPerInch;          // the height of the center of the target image above the floor\n\n    // Constants for perimeter targets\n    private static final float halfField = 72 * mmPerInch;\n    private static final float quadField  = 36 * mmPerInch;\n\n    // Class Members\n    private OpenGLMatrix lastLocation = null;\n    private VuforiaLocalizer vuforia = null;\n    private boolean targetVisible = false;\n    private float phoneXRotate    = 0;\n    private float phoneYRotate    = 0;\n    private float phoneZRotate    = 0;\n\n    @Override public void runOpMode() {\n        /*\n         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.\n         * We can pass Vuforia the handle to a camera preview resource (on the RC phone);\n         * If no camera monitor is desired, use the parameter-less constructor instead (commented out below).\n         */\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters(cameraMonitorViewId);\n\n        // VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        parameters.vuforiaLicenseKey = VUFORIA_KEY;\n        parameters.cameraDirection   = CAMERA_CHOICE;\n\n        // Make sure extended tracking is disabled for this example.\n        parameters.useExtendedTracking = false;\n\n        //  Instantiate the Vuforia engine\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        // Load the data sets for the trackable objects. These particular data\n        // sets are stored in the 'assets' part of our application.\n        VuforiaTrackables targetsUltimateGoal = this.vuforia.loadTrackablesFromAsset(\"UltimateGoal\");\n        VuforiaTrackable blueTowerGoalTarget = targetsUltimateGoal.get(0);\n        blueTowerGoalTarget.setName(\"Blue Tower Goal Target\");\n        VuforiaTrackable redTowerGoalTarget = targetsUltimateGoal.get(1);\n        redTowerGoalTarget.setName(\"Red Tower Goal Target\");\n        VuforiaTrackable redAllianceTarget = targetsUltimateGoal.get(2);\n        redAllianceTarget.setName(\"Red Alliance Target\");\n        VuforiaTrackable blueAllianceTarget = targetsUltimateGoal.get(3);\n        blueAllianceTarget.setName(\"Blue Alliance Target\");\n        VuforiaTrackable frontWallTarget = targetsUltimateGoal.get(4);\n        frontWallTarget.setName(\"Front Wall Target\");\n\n        // For convenience, gather together all the trackable objects in one easily-iterable collection */\n        List<VuforiaTrackable> allTrackables = new ArrayList<VuforiaTrackable>();\n        allTrackables.addAll(targetsUltimateGoal);\n\n        /**\n         * In order for localization to work, we need to tell the system where each target is on the field, and\n         * where the phone resides on the robot.  These specifications are in the form of <em>transformation matrices.</em>\n         * Transformation matrices are a central, important concept in the math here involved in localization.\n         * See <a href=\"https://en.wikipedia.org/wiki/Transformation_matrix\">Transformation Matrix</a>\n         * for detailed information. Commonly, you'll encounter transformation matrices as instances\n         * of the {@link OpenGLMatrix} class.\n         *\n         * If you are standing in the Red Alliance Station looking towards the center of the field,\n         *     - The X axis runs from your left to the right. (positive from the center to the right)\n         *     - The Y axis runs from the Red Alliance Station towards the other side of the field\n         *       where the Blue Alliance Station is. (Positive is from the center, towards the BlueAlliance station)\n         *     - The Z axis runs from the floor, upwards towards the ceiling.  (Positive is above the floor)\n         *\n         * Before being transformed, each target image is conceptually located at the origin of the field's\n         *  coordinate system (the center of the field), facing up.\n         */\n\n        //Set the position of the perimeter targets with relation to origin (center of field)\n        redAllianceTarget.setLocation(OpenGLMatrix\n                .translation(0, -halfField, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0, 180)));\n\n        blueAllianceTarget.setLocation(OpenGLMatrix\n                .translation(0, halfField, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0, 0)));\n        frontWallTarget.setLocation(OpenGLMatrix\n                .translation(-halfField, 0, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0 , 90)));\n\n        // The tower goal targets are located a quarter field length from the ends of the back perimeter wall.\n        blueTowerGoalTarget.setLocation(OpenGLMatrix\n                .translation(halfField, quadField, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0 , -90)));\n        redTowerGoalTarget.setLocation(OpenGLMatrix\n                .translation(halfField, -quadField, mmTargetHeight)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, 90, 0, -90)));\n\n        //\n        // Create a transformation matrix describing where the phone is on the robot.\n        //\n        // NOTE !!!!  It's very important that you turn OFF your phone's Auto-Screen-Rotation option.\n        // Lock it into Portrait for these numbers to work.\n        //\n        // Info:  The coordinate frame for the robot looks the same as the field.\n        // The robot's \"forward\" direction is facing out along X axis, with the LEFT side facing out along the Y axis.\n        // Z is UP on the robot.  This equates to a bearing angle of Zero degrees.\n        //\n        // The phone starts out lying flat, with the screen facing Up and with the physical top of the phone\n        // pointing to the LEFT side of the Robot.\n        // The two examples below assume that the camera is facing forward out the front of the robot.\n\n        // We need to rotate the camera around it's long axis to bring the correct camera forward.\n        if (CAMERA_CHOICE == BACK) {\n            phoneYRotate = -90;\n        } else {\n            phoneYRotate = 90;\n        }\n\n        // Rotate the phone vertical about the X axis if it's in portrait mode\n        if (PHONE_IS_PORTRAIT) {\n            phoneXRotate = 90 ;\n        }\n\n        // Next, translate the camera lens to where it is on the robot.\n        // In this example, it is centered (left to right), but forward of the middle of the robot, and above ground level.\n        final float CAMERA_FORWARD_DISPLACEMENT  = 4.0f * mmPerInch;   // eg: Camera is 4 Inches in front of robot center\n        final float CAMERA_VERTICAL_DISPLACEMENT = 8.0f * mmPerInch;   // eg: Camera is 8 Inches above ground\n        final float CAMERA_LEFT_DISPLACEMENT     = 0;     // eg: Camera is ON the robot's center line\n\n        OpenGLMatrix robotFromCamera = OpenGLMatrix\n                    .translation(CAMERA_FORWARD_DISPLACEMENT, CAMERA_LEFT_DISPLACEMENT, CAMERA_VERTICAL_DISPLACEMENT)\n                    .multiplied(Orientation.getRotationMatrix(EXTRINSIC, YZX, DEGREES, phoneYRotate, phoneZRotate, phoneXRotate));\n\n        /**  Let all the trackable listeners know where the phone is.  */\n        for (VuforiaTrackable trackable : allTrackables) {\n            ((VuforiaTrackableDefaultListener) trackable.getListener()).setPhoneInformation(robotFromCamera, parameters.cameraDirection);\n        }\n\n        // WARNING:\n        // In this sample, we do not wait for PLAY to be pressed.  Target Tracking is started immediately when INIT is pressed.\n        // This sequence is used to enable the new remote DS Camera Preview feature to be used with this sample.\n        // CONSEQUENTLY do not put any driving commands in this loop.\n        // To restore the normal opmode structure, just un-comment the following line:\n\n        // waitForStart();\n\n        // Note: To use the remote camera preview:\n        // AFTER you hit Init on the Driver Station, use the \"options menu\" to select \"Camera Stream\"\n        // Tap the preview window to receive a fresh image.\n\n        targetsUltimateGoal.activate();\n        while (!isStopRequested()) {\n\n            // check all the trackable targets to see which one (if any) is visible.\n            targetVisible = false;\n            for (VuforiaTrackable trackable : allTrackables) {\n                if (((VuforiaTrackableDefaultListener)trackable.getListener()).isVisible()) {\n                    telemetry.addData(\"Visible Target\", trackable.getName());\n                    targetVisible = true;\n\n                    // getUpdatedRobotLocation() will return null if no new information is available since\n                    // the last time that call was made, or if the trackable is not currently visible.\n                    OpenGLMatrix robotLocationTransform = ((VuforiaTrackableDefaultListener)trackable.getListener()).getUpdatedRobotLocation();\n                    if (robotLocationTransform != null) {\n                        lastLocation = robotLocationTransform;\n                    }\n                    break;\n                }\n            }\n\n            // Provide feedback as to where the robot is located (if we know).\n            if (targetVisible) {\n                // express position (translation) of robot in inches.\n                VectorF translation = lastLocation.getTranslation();\n                telemetry.addData(\"Pos (in)\", \"{X, Y, Z} = %.1f, %.1f, %.1f\",\n                        translation.get(0) / mmPerInch, translation.get(1) / mmPerInch, translation.get(2) / mmPerInch);\n\n                // express the rotation of the robot in degrees.\n                Orientation rotation = Orientation.getOrientation(lastLocation, EXTRINSIC, XYZ, DEGREES);\n                telemetry.addData(\"Rot (deg)\", \"{Roll, Pitch, Heading} = %.0f, %.0f, %.0f\", rotation.firstAngle, rotation.secondAngle, rotation.thirdAngle);\n            }\n            else {\n                telemetry.addData(\"Visible Target\", \"none\");\n            }\n            telemetry.update();\n        }\n\n        // Disable Tracking when we are done;\n        targetsUltimateGoal.deactivate();\n    }\n}\n",
    "PushbotTeleopTank Iterative": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.OpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.util.Range;\n\n/**\n * This file provides basic Telop driving for a Pushbot robot.\n * The code is structured as an Iterative OpMode\n *\n * This OpMode uses the common Pushbot hardware class REPLACE_CLASS define the devices on the robot.\n * All device access is managed through the HardwarePushbot class.\n *\n * This particular OpMode executes a basic Tank Drive Teleop for a PushBot\n * It raises and lowers the claw using the Gampad Y and A buttons respectively.\n * It also opens and closes the claws slowly using the left and right Bumper buttons.\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends OpMode{\n\n    /* Declare OpMode members. */\n    HardwarePushbot robot       = new HardwarePushbot(); // use the class REPLACE_CLASS to define a Pushbot's hardware\n    double          clawOffset  = 0.0 ;                  // Servo mid position\n    final double    CLAW_SPEED  = 0.02 ;                 // sets rate to move servo\n\n    /*\n     * Code to run ONCE when the driver hits INIT\n     */\n    @Override\n    public void init() {\n        /* Initialize the hardware variables.\n         * The init() method of the hardware class REPLACE_CLASS all the work here\n         */\n        robot.init(hardwareMap);\n\n        // Send telemetry message to signify robot waiting;\n        telemetry.addData(\"Say\", \"Hello Driver\");    //\n    }\n\n    /*\n     * Code to run REPEATEDLY after the driver hits INIT, but before they hit PLAY\n     */\n    @Override\n    public void init_loop() {\n    }\n\n    /*\n     * Code to run ONCE when the driver hits PLAY\n     */\n    @Override\n    public void start() {\n    }\n\n    /*\n     * Code to run REPEATEDLY after the driver hits PLAY but before they hit STOP\n     */\n    @Override\n    public void loop() {\n        double left;\n        double right;\n\n        // Run wheels in tank mode (note: The joystick goes negative when pushed forwards, so negate it)\n        left = -gamepad1.left_stick_y;\n        right = -gamepad1.right_stick_y;\n\n        robot.leftDrive.setPower(left);\n        robot.rightDrive.setPower(right);\n\n        // Use gamepad left & right Bumpers to open and close the claw\n        if (gamepad1.right_bumper)\n            clawOffset += CLAW_SPEED;\n        else if (gamepad1.left_bumper)\n            clawOffset -= CLAW_SPEED;\n\n        // Move both servos to new position.  Assume servos are mirror image of each other.\n        clawOffset = Range.clip(clawOffset, -0.5, 0.5);\n        robot.leftClaw.setPosition(robot.MID_SERVO + clawOffset);\n        robot.rightClaw.setPosition(robot.MID_SERVO - clawOffset);\n\n        // Use gamepad buttons to move the arm up (Y) and down (A)\n        if (gamepad1.y)\n            robot.leftArm.setPower(robot.ARM_UP_POWER);\n        else if (gamepad1.a)\n            robot.leftArm.setPower(robot.ARM_DOWN_POWER);\n        else\n            robot.leftArm.setPower(0.0);\n\n        // Send telemetry message to signify robot running;\n        telemetry.addData(\"claw\",  \"Offset = %.2f\", clawOffset);\n        telemetry.addData(\"left\",  \"%.2f\", left);\n        telemetry.addData(\"right\", \"%.2f\", right);\n    }\n\n    /*\n     * Code to run ONCE after the driver hits STOP\n     */\n    @Override\n    public void stop() {\n    }\n}\n",
    "SampleRevBlinkinLedDriver": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.rev.RevBlinkinLedDriver;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.OpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\n\nimport org.firstinspires.ftc.robotcore.external.Telemetry;\nimport org.firstinspires.ftc.robotcore.internal.system.Deadline;\n\nimport java.util.concurrent.TimeUnit;\n\n/*\n * Display patterns of a REV Robotics Blinkin LED Driver.\n * AUTO mode cycles through all of the patterns.\n * MANUAL mode allows the user to manually change patterns using the\n * left and right bumpers of a gamepad.\n *\n * Configure the driver on a servo port, and name it \"blinkin\".\n *\n * Displays the first pattern upon init.\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends OpMode {\n\n    /*\n     * Change the pattern every 10 seconds in AUTO mode.\n     */\n    private final static int LED_PERIOD = 10;\n\n    /*\n     * Rate limit gamepad button presses to every 500ms.\n     */\n    private final static int GAMEPAD_LOCKOUT = 500;\n\n    RevBlinkinLedDriver blinkinLedDriver;\n    RevBlinkinLedDriver.BlinkinPattern pattern;\n\n    Telemetry.Item patternName;\n    Telemetry.Item display;\n    DisplayKind displayKind;\n    Deadline ledCycleDeadline;\n    Deadline gamepadRateLimit;\n\n    protected enum DisplayKind {\n        MANUAL,\n        AUTO\n    }\n\n    @Override\n    public void init()\n    {\n        displayKind = DisplayKind.AUTO;\n\n        blinkinLedDriver = hardwareMap.get(RevBlinkinLedDriver.class, \"blinkin\");\n        pattern = RevBlinkinLedDriver.BlinkinPattern.RAINBOW_RAINBOW_PALETTE;\n        blinkinLedDriver.setPattern(pattern);\n\n        display = telemetry.addData(\"Display Kind: \", displayKind.toString());\n        patternName = telemetry.addData(\"Pattern: \", pattern.toString());\n\n        ledCycleDeadline = new Deadline(LED_PERIOD, TimeUnit.SECONDS);\n        gamepadRateLimit = new Deadline(GAMEPAD_LOCKOUT, TimeUnit.MILLISECONDS);\n    }\n\n    @Override\n    public void loop()\n    {\n        handleGamepad();\n\n        if (displayKind == DisplayKind.AUTO) {\n            doAutoDisplay();\n        } else {\n            /*\n             * MANUAL mode: Nothing to do, setting the pattern as a result of a gamepad event.\n             */\n        }\n    }\n\n    /*\n     * handleGamepad\n     *\n     * Responds to a gamepad button press.  Demonstrates rate limiting for\n     * button presses.  If loop() is called every 10ms and and you don't rate\n     * limit, then any given button press may register as multiple button presses,\n     * which in this application is problematic.\n     *\n     * A: Manual mode, Right bumper displays the next pattern, left bumper displays the previous pattern.\n     * B: Auto mode, pattern cycles, changing every LED_PERIOD seconds.\n     */\n    protected void handleGamepad()\n    {\n        if (!gamepadRateLimit.hasExpired()) {\n            return;\n        }\n\n        if (gamepad1.a) {\n            setDisplayKind(DisplayKind.MANUAL);\n            gamepadRateLimit.reset();\n        } else if (gamepad1.b) {\n            setDisplayKind(DisplayKind.AUTO);\n            gamepadRateLimit.reset();\n        } else if ((displayKind == DisplayKind.MANUAL) && (gamepad1.left_bumper)) {\n            pattern = pattern.previous();\n            displayPattern();\n            gamepadRateLimit.reset();\n        } else if ((displayKind == DisplayKind.MANUAL) && (gamepad1.right_bumper)) {\n            pattern = pattern.next();\n            displayPattern();\n            gamepadRateLimit.reset();\n        }\n    }\n\n    protected void setDisplayKind(DisplayKind displayKind)\n    {\n        this.displayKind = displayKind;\n        display.setValue(displayKind.toString());\n    }\n\n    protected void doAutoDisplay()\n    {\n        if (ledCycleDeadline.hasExpired()) {\n            pattern = pattern.next();\n            displayPattern();\n            ledCycleDeadline.reset();\n        }\n    }\n\n    protected void displayPattern()\n    {\n        blinkinLedDriver.setPattern(pattern);\n        patternName.setValue(pattern.toString());\n    }\n}\n",
    "SensorMRGyro": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.modernrobotics.ModernRoboticsI2cGyro;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.Gyroscope;\nimport com.qualcomm.robotcore.hardware.IntegratingGyroscope;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\nimport org.firstinspires.ftc.robotcore.external.navigation.AngleUnit;\nimport org.firstinspires.ftc.robotcore.external.navigation.AngularVelocity;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesOrder;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesReference;\n\n/*\n * This is an example LinearOpMode that shows how to use the Modern Robotics Gyro.\n *\n * The op mode assumes that the gyro sensor is attached to a Device Interface Module\n * I2C channel and is configured with a name of \"gyro\".\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n*/\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n  /** In this sample, for illustration purposes we use two interfaces on the one gyro object.\n   * That's likely atypical: you'll probably use one or the other in any given situation,\n   * depending on what you're trying to do. {@link IntegratingGyroscope} (and it's base interface,\n   * {@link Gyroscope}) are common interfaces supported by possibly several different gyro\n   * implementations. {@link ModernRoboticsI2cGyro}, by contrast, provides functionality that\n   * is unique to the Modern Robotics gyro sensor.\n   */\n  IntegratingGyroscope gyro;\n  ModernRoboticsI2cGyro modernRoboticsI2cGyro;\n\n  // A timer helps provide feedback while calibration is taking place\n  ElapsedTime timer = new ElapsedTime();\n\n  @Override\n  public void runOpMode() {\n\n    boolean lastResetState = false;\n    boolean curResetState  = false;\n\n    // Get a reference to a Modern Robotics gyro object. We use several interfaces\n    // on this object to illustrate which interfaces support which functionality.\n    modernRoboticsI2cGyro = hardwareMap.get(ModernRoboticsI2cGyro.class, \"gyro\");\n    gyro = (IntegratingGyroscope)modernRoboticsI2cGyro;\n    // If you're only interested int the IntegratingGyroscope interface, the following will suffice.\n    // gyro = hardwareMap.get(IntegratingGyroscope.class, \"gyro\");\n    // A similar approach will work for the Gyroscope interface, if that's all you need.\n\n    // Start calibrating the gyro. This takes a few seconds and is worth performing\n    // during the initialization phase at the start of each opMode.\n    telemetry.log().add(\"Gyro Calibrating. Do Not Move!\");\n    modernRoboticsI2cGyro.calibrate();\n\n    // Wait until the gyro calibration is complete\n    timer.reset();\n    while (!isStopRequested() && modernRoboticsI2cGyro.isCalibrating())  {\n      telemetry.addData(\"calibrating\", \"%s\", Math.round(timer.seconds())%2==0 ? \"|..\" : \"..|\");\n      telemetry.update();\n      sleep(50);\n    }\n\n    telemetry.log().clear(); telemetry.log().add(\"Gyro Calibrated. Press Start.\");\n    telemetry.clear(); telemetry.update();\n\n    // Wait for the start button to be pressed\n    waitForStart();\n    telemetry.log().clear();\n    telemetry.log().add(\"Press A & B to reset heading\");\n\n    // Loop until we're asked to stop\n    while (opModeIsActive())  {\n\n      // If the A and B buttons are pressed just now, reset Z heading.\n      curResetState = (gamepad1.a && gamepad1.b);\n      if (curResetState && !lastResetState) {\n        modernRoboticsI2cGyro.resetZAxisIntegrator();\n      }\n      lastResetState = curResetState;\n\n      // The raw() methods report the angular rate of change about each of the\n      // three axes directly as reported by the underlying sensor IC.\n      int rawX = modernRoboticsI2cGyro.rawX();\n      int rawY = modernRoboticsI2cGyro.rawY();\n      int rawZ = modernRoboticsI2cGyro.rawZ();\n      int heading = modernRoboticsI2cGyro.getHeading();\n      int integratedZ = modernRoboticsI2cGyro.getIntegratedZValue();\n\n      // Read dimensionalized data from the gyro. This gyro can report angular velocities\n      // about all three axes. Additionally, it internally integrates the Z axis to\n      // be able to report an absolute angular Z orientation.\n      AngularVelocity rates = gyro.getAngularVelocity(AngleUnit.DEGREES);\n      float zAngle = gyro.getAngularOrientation(AxesReference.INTRINSIC, AxesOrder.ZYX, AngleUnit.DEGREES).firstAngle;\n\n      // Read administrative information from the gyro\n      int zAxisOffset = modernRoboticsI2cGyro.getZAxisOffset();\n      int zAxisScalingCoefficient = modernRoboticsI2cGyro.getZAxisScalingCoefficient();\n\n      telemetry.addLine()\n        .addData(\"dx\", formatRate(rates.xRotationRate))\n        .addData(\"dy\", formatRate(rates.yRotationRate))\n        .addData(\"dz\", \"%s deg/s\", formatRate(rates.zRotationRate));\n      telemetry.addData(\"angle\", \"%s deg\", formatFloat(zAngle));\n      telemetry.addData(\"heading\", \"%3d deg\", heading);\n      telemetry.addData(\"integrated Z\", \"%3d\", integratedZ);\n      telemetry.addLine()\n        .addData(\"rawX\", formatRaw(rawX))\n        .addData(\"rawY\", formatRaw(rawY))\n        .addData(\"rawZ\", formatRaw(rawZ));\n      telemetry.addLine().addData(\"z offset\", zAxisOffset).addData(\"z coeff\", zAxisScalingCoefficient);\n      telemetry.update();\n    }\n  }\n\n  String formatRaw(int rawValue) {\n    return String.format(\"%d\", rawValue);\n  }\n\n  String formatRate(float rate) {\n    return String.format(\"%.3f\", rate);\n  }\n\n  String formatFloat(float rate) {\n    return String.format(\"%.3f\", rate);\n  }\n\n}\n",
    "SensorBNO055IMUCalibration": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.adafruit.AdafruitBNO055IMU;\nimport com.qualcomm.hardware.bosch.BNO055IMU;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.util.ReadWriteFile;\n\nimport org.firstinspires.ftc.robotcore.external.Func;\nimport org.firstinspires.ftc.robotcore.external.navigation.AngleUnit;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesOrder;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesReference;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.internal.system.AppUtil;\n\nimport java.io.File;\nimport java.util.Locale;\n\n/**\n * {@link SensorBNO055IMUCalibration} calibrates the IMU accelerometer per\n * \"Section 3.11 Calibration\" of the BNO055 specification.\n *\n * <p>Manual calibration of the IMU is definitely NOT necessary: except for the magnetometer\n * (which is not used by the default {@link BNO055IMU.SensorMode#IMU\n * SensorMode#IMU}), the BNO055 is internally self-calibrating and thus can be very successfully\n * used without manual intervention. That said, performing a one-time calibration, saving the\n * results persistently, then loading them again at each run can help reduce the time that automatic\n * calibration requires.</p>\n *\n * <p>This summary of the calibration process, from <a href=\"http://iotdk.intel.com/docs/master/upm/classupm_1_1_b_n_o055.html\">\n * Intel</a>, is informative:</p>\n *\n * <p>\"This device requires calibration in order to operate accurately. [...] Calibration data is\n * lost on a power cycle. See one of the examples for a description of how to calibrate the device,\n * but in essence:</p>\n *\n * <p>There is a calibration status register available [...] that returns the calibration status\n * of the accelerometer (ACC), magnetometer (MAG), gyroscope (GYR), and overall system (SYS).\n * Each of these values range from 0 (uncalibrated) to 3 (fully calibrated). Calibration [ideally]\n * involves certain motions to get all 4 values at 3. The motions are as follows (though see the\n * datasheet for more information):</p>\n *\n * <li>\n *     <ol>GYR: Simply let the sensor sit flat for a few seconds.</ol>\n *     <ol>ACC: Move the sensor in various positions. Start flat, then rotate slowly by 45\n *              degrees, hold for a few seconds, then continue rotating another 45 degrees and\n *              hold, etc. 6 or more movements of this type may be required. You can move through\n *              any axis you desire, but make sure that the device is lying at least once\n *              perpendicular to the x, y, and z axis.</ol>\n *     <ol>MAG: Move slowly in a figure 8 pattern in the air, until the calibration values reaches 3.</ol>\n *     <ol>SYS: This will usually reach 3 when the other items have also reached 3. If not, continue\n *              slowly moving the device though various axes until it does.\"</ol>\n * </li>\n *\n * <p>To calibrate the IMU, run this sample opmode with a gamepad attached to the driver station.\n * Once the IMU has reached sufficient calibration as reported on telemetry, press the 'A'\n * button on the gamepad to write the calibration to a file. That file can then be indicated\n * later when running an opmode which uses the IMU.</p>\n *\n * <p>Note: if your intended uses of the IMU do not include use of all its sensors (for exmaple,\n * you might not use the magnetometer), then it makes little sense for you to wait for full\n * calibration of the sensors you are not using before saving the calibration data. Indeed,\n * it appears that in a SensorMode that doesn't use the magnetometer (for example), the\n * magnetometer cannot actually be calibrated.</p>\n *\n * @see AdafruitBNO055IMU\n * @see BNO055IMU.Parameters#calibrationDataFile\n * @see <a href=\"https://www.bosch-sensortec.com/bst/products/all_products/bno055\">BNO055 product page</a>\n * @see <a href=\"https://ae-bst.resource.bosch.com/media/_tech/media/datasheets/BST_BNO055_DS000_14.pdf\">BNO055 specification</a>\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled                            // Uncomment this to add to the opmode list\npublic class REPLACE_CLASS extends LinearOpMode\n    {\n    //----------------------------------------------------------------------------------------------\n    // State\n    //----------------------------------------------------------------------------------------------\n\n    // Our sensors, motors, and other devices go here, along with other long term state\n    BNO055IMU imu;\n\n    // State used for updating telemetry\n    Orientation angles;\n\n    //----------------------------------------------------------------------------------------------\n    // Main logic\n    //----------------------------------------------------------------------------------------------\n\n    @Override public void runOpMode() {\n\n        telemetry.log().setCapacity(12);\n        telemetry.log().add(\"\");\n        telemetry.log().add(\"Please refer to the calibration instructions\");\n        telemetry.log().add(\"contained in the Adafruit IMU calibration\");\n        telemetry.log().add(\"sample opmode.\");\n        telemetry.log().add(\"\");\n        telemetry.log().add(\"When sufficient calibration has been reached,\");\n        telemetry.log().add(\"press the 'A' button to write the current\");\n        telemetry.log().add(\"calibration data to a file.\");\n        telemetry.log().add(\"\");\n\n        // We are expecting the IMU to be attached to an I2C port on a Core Device Interface Module and named \"imu\".\n        BNO055IMU.Parameters parameters = new BNO055IMU.Parameters();\n        parameters.loggingEnabled = true;\n        parameters.loggingTag     = \"IMU\";\n        imu = hardwareMap.get(BNO055IMU.class, \"imu\");\n        imu.initialize(parameters);\n\n        composeTelemetry();\n        telemetry.log().add(\"Waiting for start...\");\n\n        // Wait until we're told to go\n        while (!isStarted()) {\n            telemetry.update();\n            idle();\n        }\n\n        telemetry.log().add(\"...started...\");\n\n        while (opModeIsActive()) {\n\n            if (gamepad1.a) {\n\n                // Get the calibration data\n                BNO055IMU.CalibrationData calibrationData = imu.readCalibrationData();\n\n                // Save the calibration data to a file. You can choose whatever file\n                // name you wish here, but you'll want to indicate the same file name\n                // when you initialize the IMU in an opmode in which it is used. If you\n                // have more than one IMU on your robot, you'll of course want to use\n                // different configuration file names for each.\n                String filename = \"AdafruitIMUCalibration.json\";\n                File file = AppUtil.getInstance().getSettingsFile(filename);\n                ReadWriteFile.writeFile(file, calibrationData.serialize());\n                telemetry.log().add(\"saved to '%s'\", filename);\n\n                // Wait for the button to be released\n                while (gamepad1.a) {\n                    telemetry.update();\n                    idle();\n                }\n            }\n\n            telemetry.update();\n        }\n    }\n\n    void composeTelemetry() {\n\n        // At the beginning of each telemetry update, grab a bunch of data\n        // from the IMU that we will then display in separate lines.\n        telemetry.addAction(new Runnable() { @Override public void run()\n                {\n                // Acquiring the angles is relatively expensive; we don't want\n                // to do that in each of the three items that need that info, as that's\n                // three times the necessary expense.\n                angles   = imu.getAngularOrientation(AxesReference.INTRINSIC, AxesOrder.ZYX, AngleUnit.DEGREES);\n                }\n            });\n\n        telemetry.addLine()\n            .addData(\"status\", new Func<String>() {\n                @Override public String value() {\n                    return imu.getSystemStatus().toShortString();\n                    }\n                })\n            .addData(\"calib\", new Func<String>() {\n                @Override public String value() {\n                    return imu.getCalibrationStatus().toString();\n                    }\n                });\n\n        telemetry.addLine()\n            .addData(\"heading\", new Func<String>() {\n                @Override public String value() {\n                    return formatAngle(angles.angleUnit, angles.firstAngle);\n                    }\n                })\n            .addData(\"roll\", new Func<String>() {\n                @Override public String value() {\n                    return formatAngle(angles.angleUnit, angles.secondAngle);\n                    }\n                })\n            .addData(\"pitch\", new Func<String>() {\n                @Override public String value() {\n                    return formatAngle(angles.angleUnit, angles.thirdAngle);\n                    }\n                });\n    }\n\n    //----------------------------------------------------------------------------------------------\n    // Formatting\n    //----------------------------------------------------------------------------------------------\n\n    String formatAngle(AngleUnit angleUnit, double angle) {\n        return formatDegrees(AngleUnit.DEGREES.fromUnit(angleUnit, angle));\n    }\n\n    String formatDegrees(double degrees){\n        return String.format(Locale.getDefault(), \"%.1f\", AngleUnit.DEGREES.normalize(degrees));\n    }\n}",
    "ConceptI2cAddressChange": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.modernrobotics.ModernRoboticsUsbDeviceInterfaceModule;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DeviceInterfaceModule;\nimport com.qualcomm.robotcore.hardware.I2cAddr;\nimport com.qualcomm.robotcore.util.RobotLog;\nimport com.qualcomm.robotcore.util.TypeConversion;\n\nimport java.util.concurrent.locks.Lock;\n\n/**\n * An example of a linear op mode that shows how to change the I2C address.\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n  public static final int ADDRESS_SET_NEW_I2C_ADDRESS = 0x70;\n  // trigger bytes used to change I2C address on ModernRobotics sensors.\n  public static final byte TRIGGER_BYTE_1 = 0x55;\n  public static final byte TRIGGER_BYTE_2 = (byte) 0xaa;\n\n  // Expected bytes from the Modern Robotics IR Seeker V3 memory map\n  public static final byte IR_SEEKER_V3_FIRMWARE_REV = 0x12;\n  public static final byte IR_SEEKER_V3_SENSOR_ID = 0x49;\n  public static final I2cAddr IR_SEEKER_V3_ORIGINAL_ADDRESS = I2cAddr.create8bit(0x38);\n\n  // Expected bytes from the Modern Robotics Color Sensor memory map\n  public static final byte COLOR_SENSOR_FIRMWARE_REV = 0x10;\n  public static final byte COLOR_SENSOR_SENSOR_ID = 0x43;\n  public static final byte COLOR_SENSOR_ORIGINAL_ADDRESS = 0x3C;\n\n  public static final byte MANUFACTURER_CODE = 0x4d;\n  // Currently, this is set to expect the bytes from the IR Seeker.\n  // If you change these values so you're setting \"FIRMWARE_REV\" to\n  // COLOR_SENSOR_FIRMWARE_REV, and \"SENSOR_ID\" to \"COLOR_SENSOR_SENSOR_ID\",\n  // you'll be able to change the I2C address of the ModernRoboticsColorSensor.\n  // If the bytes you're expecting are different than what this op mode finds,\n  // a comparison will be printed out into the logfile.\n  public static final byte FIRMWARE_REV = IR_SEEKER_V3_FIRMWARE_REV;\n  public static final byte SENSOR_ID = IR_SEEKER_V3_SENSOR_ID;\n\n  // These byte values are common with most Modern Robotics sensors.\n  public static final int READ_MODE = 0x80;\n  public static final int ADDRESS_MEMORY_START = 0x0;\n  public static final int TOTAL_MEMORY_LENGTH = 0x0c;\n  public static final int BUFFER_CHANGE_ADDRESS_LENGTH = 0x03;\n\n  // The port where your sensor is connected.\n  int port = 5;\n\n  byte[] readCache;\n  Lock readLock;\n  byte[] writeCache;\n  Lock writeLock;\n\n  I2cAddr currentAddress = IR_SEEKER_V3_ORIGINAL_ADDRESS;\n  // I2c addresses on Modern Robotics devices must be divisible by 2, and between 0x7e and 0x10\n  // Different hardware may have different rules.\n  // Be sure to read the requirements for the hardware you're using!\n  // If you use an invalid address, you may make your device completely unusable.\n  I2cAddr newAddress = I2cAddr.create8bit(0x42);\n\n  DeviceInterfaceModule dim;\n\n  @Override\n  public void runOpMode() {\n\n    // set up the hardware devices we are going to use\n    dim = hardwareMap.get(DeviceInterfaceModule.class, \"dim\");\n\n    readCache = dim.getI2cReadCache(port);\n    readLock = dim.getI2cReadCacheLock(port);\n    writeCache = dim.getI2cWriteCache(port);\n    writeLock = dim.getI2cWriteCacheLock(port);\n\n    // I2c addresses on Modern Robotics devices must be divisible by 2, and between 0x7e and 0x10\n    // Different hardware may have different rules.\n    // Be sure to read the requirements for the hardware you're using!\n    ModernRoboticsUsbDeviceInterfaceModule.throwIfModernRoboticsI2cAddressIsInvalid(newAddress);\n\n    // wait for the start button to be pressed\n    waitForStart();\n\n    performAction(\"read\", port, currentAddress, ADDRESS_MEMORY_START, TOTAL_MEMORY_LENGTH);\n\n    while(!dim.isI2cPortReady(port)) {\n      telemetry.addData(\"I2cAddressChange\", \"waiting for the port to be ready...\");\n      telemetry.update();\n      sleep(1000);\n    }\n\n    // update the local cache\n    dim.readI2cCacheFromController(port);\n\n    // make sure the first bytes are what we think they should be.\n    int count = 0;\n    int[] initialArray = {READ_MODE, currentAddress.get8Bit(), ADDRESS_MEMORY_START, TOTAL_MEMORY_LENGTH, FIRMWARE_REV, MANUFACTURER_CODE, SENSOR_ID};\n    while (!foundExpectedBytes(initialArray, readLock, readCache)) {\n      telemetry.addData(\"I2cAddressChange\", \"Confirming that we're reading the correct bytes...\");\n      telemetry.update();\n      dim.readI2cCacheFromController(port);\n      sleep(1000);\n      count++;\n      // if we go too long with failure, we probably are expecting the wrong bytes.\n      if (count >= 10)  {\n        telemetry.addData(\"I2cAddressChange\", String.format(\"Looping too long with no change, probably have the wrong address. Current address: 8bit=0x%02x\", currentAddress.get8Bit()));\n        hardwareMap.irSeekerSensor.get(String.format(\"Looping too long with no change, probably have the wrong address. Current address: 8bit=0x%02x\", currentAddress.get8Bit()));\n        telemetry.update();\n      }\n    }\n\n    // Enable writes to the correct segment of the memory map.\n    performAction(\"write\", port, currentAddress, ADDRESS_SET_NEW_I2C_ADDRESS, BUFFER_CHANGE_ADDRESS_LENGTH);\n\n    // Write out the trigger bytes, and the new desired address.\n    writeNewAddress();\n    dim.setI2cPortActionFlag(port);\n    dim.writeI2cCacheToController(port);\n\n    telemetry.addData(\"I2cAddressChange\", \"Giving the hardware 60 seconds to make the change...\");\n    telemetry.update();\n\n    // Changing the I2C address takes some time.\n    sleep(60000);\n\n    // Query the new address and see if we can get the bytes we expect.\n    dim.enableI2cReadMode(port, newAddress, ADDRESS_MEMORY_START, TOTAL_MEMORY_LENGTH);\n    dim.setI2cPortActionFlag(port);\n    dim.writeI2cCacheToController(port);\n\n    int[] confirmArray = {READ_MODE, newAddress.get8Bit(), ADDRESS_MEMORY_START, TOTAL_MEMORY_LENGTH, FIRMWARE_REV, MANUFACTURER_CODE, SENSOR_ID};\n    while (!foundExpectedBytes(confirmArray, readLock, readCache)) {\n      telemetry.addData(\"I2cAddressChange\", \"Have not confirmed the changes yet...\");\n      telemetry.update();\n      dim.readI2cCacheFromController(port);\n      sleep(1000);\n    }\n\n    telemetry.addData(\"I2cAddressChange\", \"Successfully changed the I2C address. New address: 8bit=0x%02x\", newAddress.get8Bit());\n    telemetry.update();\n    RobotLog.i(\"Successfully changed the I2C address.\" + String.format(\"New address: 8bit=0x%02x\", newAddress.get8Bit()));\n\n    /**** IMPORTANT NOTE ******/\n    // You need to add a line like this at the top of your op mode\n    // to update the I2cAddress in the driver.\n    //irSeeker.setI2cAddress(newAddress);\n    /***************************/\n\n  }\n\n  private boolean foundExpectedBytes(int[] byteArray, Lock lock, byte[] cache) {\n    try {\n      lock.lock();\n      boolean allMatch = true;\n      StringBuilder s = new StringBuilder(300 * 4);\n      String mismatch = \"\";\n      for (int i = 0; i < byteArray.length; i++) {\n        s.append(String.format(\"expected: %02x, got: %02x \\n\", TypeConversion.unsignedByteToInt( (byte) byteArray[i]), cache[i]));\n        if (TypeConversion.unsignedByteToInt(cache[i]) != TypeConversion.unsignedByteToInt( (byte) byteArray[i])) {\n          mismatch = String.format(\"i: %d, byteArray[i]: %02x, cache[i]: %02x\", i, byteArray[i], cache[i]);\n          allMatch = false;\n        }\n      }\n      RobotLog.e(s.toString() + \"\\n allMatch: \" + allMatch + \", mismatch: \" + mismatch);\n      return allMatch;\n    } finally {\n      lock.unlock();\n    }\n  }\n\n  private void performAction(String actionName, int port, I2cAddr i2cAddress, int memAddress, int memLength) {\n    if (actionName.equalsIgnoreCase(\"read\")) dim.enableI2cReadMode(port, i2cAddress, memAddress, memLength);\n    if (actionName.equalsIgnoreCase(\"write\")) dim.enableI2cWriteMode(port, i2cAddress, memAddress, memLength);\n\n    dim.setI2cPortActionFlag(port);\n    dim.writeI2cCacheToController(port);\n    dim.readI2cCacheFromController(port);\n  }\n\n  private void writeNewAddress() {\n    try {\n      writeLock.lock();\n      writeCache[4] = (byte) newAddress.get8Bit();\n      writeCache[5] = TRIGGER_BYTE_1;\n      writeCache[6] = TRIGGER_BYTE_2;\n    } finally {\n      writeLock.unlock();\n    }\n  }\n}\n",
    "SensorMROpticalDistance": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.OpticalDistanceSensor;\n\n/*\n * This is an example LinearOpMode that shows how to use\n * a Modern Robotics Optical Distance Sensor\n * It assumes that the ODS sensor is configured with a name of \"sensor_ods\".\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n  OpticalDistanceSensor odsSensor;  // Hardware Device Object\n\n  @Override\n  public void runOpMode() {\n\n    // get a reference to our Light Sensor object.\n    odsSensor = hardwareMap.get(OpticalDistanceSensor.class, \"sensor_ods\");\n\n    // wait for the start button to be pressed.\n    waitForStart();\n\n    // while the op mode is active, loop and read the light levels.\n    // Note we use opModeIsActive() as our loop condition because it is an interruptible method.\n    while (opModeIsActive()) {\n\n      // send the info back to driver station using telemetry function.\n      telemetry.addData(\"Raw\",    odsSensor.getRawLightDetected());\n      telemetry.addData(\"Normal\", odsSensor.getLightDetected());\n\n      telemetry.update();\n    }\n  }\n}\n",
    "PushbotAutoDriveByEncoder Linear": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Autonomous;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.hardware.DcMotor;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\n/**\n * This file illustrates the concept of driving a path based on encoder counts.\n * It uses the common Pushbot hardware class REPLACE_CLASS define the drive on the robot.\n * The code is structured as a LinearOpMode\n *\n * The code REQUIRES that you DO have encoders on the wheels,\n *   otherwise you would use: PushbotAutoDriveByTime;\n *\n *  This code ALSO requires that the drive Motors have been configured such that a positive\n *  power command moves them forwards, and causes the encoders to count UP.\n *\n *   The desired path in this example is:\n *   - Drive forward for 48 inches\n *   - Spin right for 12 Inches\n *   - Drive Backwards for 24 inches\n *   - Stop and close the claw.\n *\n *  The code is written using a method called: encoderDrive(speed, leftInches, rightInches, timeoutS)\n *  that performs the actual movement.\n *  This methods assumes that each movement is relative to the last stopping place.\n *  There are other ways to perform encoder based moves, but this method is probably the simplest.\n *  This code uses the RUN_TO_POSITION mode to enable the Motor controllers to generate the run profile\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@Autonomous(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    /* Declare OpMode members. */\n    HardwarePushbot         robot   = new HardwarePushbot();   // Use a Pushbot's hardware\n    private ElapsedTime     runtime = new ElapsedTime();\n\n    static final double     COUNTS_PER_MOTOR_REV    = 1440 ;    // eg: TETRIX Motor Encoder\n    static final double     DRIVE_GEAR_REDUCTION    = 2.0 ;     // This is < 1.0 if geared UP\n    static final double     WHEEL_DIAMETER_INCHES   = 4.0 ;     // For figuring circumference\n    static final double     COUNTS_PER_INCH         = (COUNTS_PER_MOTOR_REV * DRIVE_GEAR_REDUCTION) /\n                                                      (WHEEL_DIAMETER_INCHES * 3.1415);\n    static final double     DRIVE_SPEED             = 0.6;\n    static final double     TURN_SPEED              = 0.5;\n\n    @Override\n    public void runOpMode() {\n\n        /*\n         * Initialize the drive system variables.\n         * The init() method of the hardware class REPLACE_CLASS all the work here\n         */\n        robot.init(hardwareMap);\n\n        // Send telemetry message to signify robot waiting;\n        telemetry.addData(\"Status\", \"Resetting Encoders\");    //\n        telemetry.update();\n\n        robot.leftDrive.setMode(DcMotor.RunMode.STOP_AND_RESET_ENCODER);\n        robot.rightDrive.setMode(DcMotor.RunMode.STOP_AND_RESET_ENCODER);\n\n        robot.leftDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n        robot.rightDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n\n        // Send telemetry message to indicate successful Encoder reset\n        telemetry.addData(\"Path0\",  \"Starting at %7d :%7d\",\n                          robot.leftDrive.getCurrentPosition(),\n                          robot.rightDrive.getCurrentPosition());\n        telemetry.update();\n\n        // Wait for the game to start (driver presses PLAY)\n        waitForStart();\n\n        // Step through each leg of the path,\n        // Note: Reverse movement is obtained by setting a negative distance (not speed)\n        encoderDrive(DRIVE_SPEED,  48,  48, 5.0);  // S1: Forward 47 Inches with 5 Sec timeout\n        encoderDrive(TURN_SPEED,   12, -12, 4.0);  // S2: Turn Right 12 Inches with 4 Sec timeout\n        encoderDrive(DRIVE_SPEED, -24, -24, 4.0);  // S3: Reverse 24 Inches with 4 Sec timeout\n\n        robot.leftClaw.setPosition(1.0);            // S4: Stop and close the claw.\n        robot.rightClaw.setPosition(0.0);\n        sleep(1000);     // pause for servos to move\n\n        telemetry.addData(\"Path\", \"Complete\");\n        telemetry.update();\n    }\n\n    /*\n     *  Method to perform a relative move, based on encoder counts.\n     *  Encoders are not reset as the move is based on the current position.\n     *  Move will stop if any of three conditions occur:\n     *  1) Move gets to the desired position\n     *  2) Move runs out of time\n     *  3) Driver stops the opmode running.\n     */\n    public void encoderDrive(double speed,\n                             double leftInches, double rightInches,\n                             double timeoutS) {\n        int newLeftTarget;\n        int newRightTarget;\n\n        // Ensure that the opmode is still active\n        if (opModeIsActive()) {\n\n            // Determine new target position, and pass to motor controller\n            newLeftTarget = robot.leftDrive.getCurrentPosition() + (int)(leftInches * COUNTS_PER_INCH);\n            newRightTarget = robot.rightDrive.getCurrentPosition() + (int)(rightInches * COUNTS_PER_INCH);\n            robot.leftDrive.setTargetPosition(newLeftTarget);\n            robot.rightDrive.setTargetPosition(newRightTarget);\n\n            // Turn On RUN_TO_POSITION\n            robot.leftDrive.setMode(DcMotor.RunMode.RUN_TO_POSITION);\n            robot.rightDrive.setMode(DcMotor.RunMode.RUN_TO_POSITION);\n\n            // reset the timeout time and start motion.\n            runtime.reset();\n            robot.leftDrive.setPower(Math.abs(speed));\n            robot.rightDrive.setPower(Math.abs(speed));\n\n            // keep looping while we are still active, and there is time left, and both motors are running.\n            // Note: We use (isBusy() && isBusy()) in the loop test, which means that when EITHER motor hits\n            // its target position, the motion will stop.  This is \"safer\" in the event that the robot will\n            // always end the motion as soon as possible.\n            // However, if you require that BOTH motors have finished their moves before the robot continues\n            // onto the next step, use (isBusy() || isBusy()) in the loop test.\n            while (opModeIsActive() &&\n                   (runtime.seconds() < timeoutS) &&\n                   (robot.leftDrive.isBusy() && robot.rightDrive.isBusy())) {\n\n                // Display it for the driver.\n                telemetry.addData(\"Path1\",  \"Running to %7d :%7d\", newLeftTarget,  newRightTarget);\n                telemetry.addData(\"Path2\",  \"Running at %7d :%7d\",\n                                            robot.leftDrive.getCurrentPosition(),\n                                            robot.rightDrive.getCurrentPosition());\n                telemetry.update();\n            }\n\n            // Stop all motion;\n            robot.leftDrive.setPower(0);\n            robot.rightDrive.setPower(0);\n\n            // Turn off RUN_TO_POSITION\n            robot.leftDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n            robot.rightDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n\n            //  sleep(250);   // optional pause after each move\n        }\n    }\n}\n",
    "PushbotAutoDriveToLine Linear": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Autonomous;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.hardware.LightSensor;\n\n/**\n * This file illustrates the concept of driving up to a line and then stopping.\n * It uses the common Pushbot hardware class REPLACE_CLASS define the drive on the robot.\n * The code is structured as a LinearOpMode\n *\n * The code shows using two different light sensors:\n *   The Primary sensor shown in this code is a legacy NXT Light sensor (called \"sensor_light\")\n *   Alternative \"commented out\" code uses a MR Optical Distance Sensor (called \"sensor_ods\")\n *   instead of the LEGO sensor.  Chose to use one sensor or the other.\n *\n *   Setting the correct WHITE_THRESHOLD value is key to stopping correctly.\n *   This should be set half way between the light and dark values.\n *   These values can be read on the screen once the OpMode has been INIT, but before it is STARTED.\n *   Move the senso on asnd off the white line and not the min and max readings.\n *   Edit this code to make WHITE_THRESHOLD half way between the min and max.\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n\n@Autonomous(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    /* Declare OpMode members. */\n    HardwarePushbot         robot   = new HardwarePushbot();   // Use a Pushbot's hardware\n    LightSensor             lightSensor;      // Primary LEGO Light sensor,\n    // OpticalDistanceSensor   lightSensor;   // Alternative MR ODS sensor\n\n    static final double     WHITE_THRESHOLD = 0.2;  // spans between 0.1 - 0.5 from dark to light\n    static final double     APPROACH_SPEED  = 0.5;\n\n    @Override\n    public void runOpMode() {\n\n        /* Initialize the drive system variables.\n         * The init() method of the hardware class REPLACE_CLASS all the work here\n         */\n        robot.init(hardwareMap);\n\n        // If there are encoders connected, switch to RUN_USING_ENCODER mode for greater accuracy\n        // robot.leftDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n        // robot.rightDrive.setMode(DcMotor.RunMode.RUN_USING_ENCODER);\n\n        // get a reference to our Light Sensor object.\n        lightSensor = hardwareMap.lightSensor.get(\"sensor_light\");                // Primary LEGO Light Sensor\n        //  lightSensor = hardwareMap.opticalDistanceSensor.get(\"sensor_ods\");  // Alternative MR ODS sensor.\n\n        // turn on LED of light sensor.\n        lightSensor.enableLed(true);\n\n        // Send telemetry message to signify robot waiting;\n        telemetry.addData(\"Status\", \"Ready to run\");    //\n        telemetry.update();\n\n        // Wait for the game to start (driver presses PLAY)\n        // Abort this loop is started or stopped.\n        while (!(isStarted() || isStopRequested())) {\n\n            // Display the light level while we are waiting to start\n            telemetry.addData(\"Light Level\", lightSensor.getLightDetected());\n            telemetry.update();\n            idle();\n        }\n\n        // Start the robot moving forward, and then begin looking for a white line.\n        robot.leftDrive.setPower(APPROACH_SPEED);\n        robot.rightDrive.setPower(APPROACH_SPEED);\n\n        // run until the white line is seen OR the driver presses STOP;\n        while (opModeIsActive() && (lightSensor.getLightDetected() < WHITE_THRESHOLD)) {\n\n            // Display the light level while we are looking for the line\n            telemetry.addData(\"Light Level\",  lightSensor.getLightDetected());\n            telemetry.update();\n        }\n\n        // Stop all motors\n        robot.leftDrive.setPower(0);\n        robot.rightDrive.setPower(0);\n    }\n}\n",
    "SensorMRColor": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport android.app.Activity;\nimport android.graphics.Color;\nimport android.view.View;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.ColorSensor;\n\n/*\n *\n * This is an example LinearOpMode that shows how to use\n * a Modern Robotics Color Sensor.\n *\n * The op mode assumes that the color sensor\n * is configured with a name of \"sensor_color\".\n *\n * You can use the X button on gamepad1 to toggle the LED on and off.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n  ColorSensor colorSensor;    // Hardware Device Object\n\n\n  @Override\n  public void runOpMode() {\n\n    // hsvValues is an array that will hold the hue, saturation, and value information.\n    float hsvValues[] = {0F,0F,0F};\n\n    // values is a reference to the hsvValues array.\n    final float values[] = hsvValues;\n\n    // get a reference to the RelativeLayout so we can change the background\n    // color of the Robot Controller app to match the hue detected by the RGB sensor.\n    int relativeLayoutId = hardwareMap.appContext.getResources().getIdentifier(\"RelativeLayout\", \"id\", hardwareMap.appContext.getPackageName());\n    final View relativeLayout = ((Activity) hardwareMap.appContext).findViewById(relativeLayoutId);\n\n    // bPrevState and bCurrState represent the previous and current state of the button.\n    boolean bPrevState = false;\n    boolean bCurrState = false;\n\n    // bLedOn represents the state of the LED.\n    boolean bLedOn = true;\n\n    // get a reference to our ColorSensor object.\n    colorSensor = hardwareMap.get(ColorSensor.class, \"sensor_color\");\n\n    // Set the LED in the beginning\n    colorSensor.enableLed(bLedOn);\n\n    // wait for the start button to be pressed.\n    waitForStart();\n\n    // while the op mode is active, loop and read the RGB data.\n    // Note we use opModeIsActive() as our loop condition because it is an interruptible method.\n    while (opModeIsActive()) {\n\n      // check the status of the x button on either gamepad.\n      bCurrState = gamepad1.x;\n\n      // check for button state transitions.\n      if (bCurrState && (bCurrState != bPrevState))  {\n\n        // button is transitioning to a pressed state. So Toggle LED\n        bLedOn = !bLedOn;\n        colorSensor.enableLed(bLedOn);\n      }\n\n      // update previous state variable.\n      bPrevState = bCurrState;\n\n      // convert the RGB values to HSV values.\n      Color.RGBToHSV(colorSensor.red() * 8, colorSensor.green() * 8, colorSensor.blue() * 8, hsvValues);\n\n      // send the info back to driver station using telemetry function.\n      telemetry.addData(\"LED\", bLedOn ? \"On\" : \"Off\");\n      telemetry.addData(\"Clear\", colorSensor.alpha());\n      telemetry.addData(\"Red  \", colorSensor.red());\n      telemetry.addData(\"Green\", colorSensor.green());\n      telemetry.addData(\"Blue \", colorSensor.blue());\n      telemetry.addData(\"Hue\", hsvValues[0]);\n\n      // change the background color to match the color detected by the RGB sensor.\n      // pass a reference to the hue, saturation, and value array as an argument\n      // to the HSVToColor method.\n      relativeLayout.post(new Runnable() {\n        public void run() {\n          relativeLayout.setBackgroundColor(Color.HSVToColor(0xff, values));\n        }\n      });\n\n      telemetry.update();\n    }\n\n    // Set the panel back to the default color\n    relativeLayout.post(new Runnable() {\n      public void run() {\n        relativeLayout.setBackgroundColor(Color.WHITE);\n      }\n    });\n  }\n}\n",
    "SensorMRCompass": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.modernrobotics.ModernRoboticsI2cCompassSensor;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.CompassSensor;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\nimport org.firstinspires.ftc.robotcore.external.navigation.Acceleration;\n\n/**\n * The {@link SensorMRCompass} op mode provides a demonstration of the\n * functionality provided by the Modern Robotics compass sensor.\n *\n * The op mode assumes that the MR compass is configured with a name of \"compass\".\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n *\n * @see <a href=\"http://www.modernroboticsinc.com/compass\">MR Compass Sensor</a>\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled   // comment out or remove this line to enable this opmode\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    ModernRoboticsI2cCompassSensor compass;\n    ElapsedTime                    timer = new ElapsedTime();\n\n    @Override public void runOpMode() {\n\n        // get a reference to our compass\n        compass = hardwareMap.get(ModernRoboticsI2cCompassSensor.class, \"compass\");\n\n        telemetry.log().setCapacity(20);\n        telemetry.log().add(\"The compass sensor operates quite well out-of-the\");\n        telemetry.log().add(\"box, as shipped by the manufacturer. Precision can\");\n        telemetry.log().add(\"however be somewhat improved with calibration.\");\n        telemetry.log().add(\"\");\n        telemetry.log().add(\"To calibrate the compass once the opmode is\");\n        telemetry.log().add(\"started, make sure the compass is level, then\");\n        telemetry.log().add(\"press 'A' on the gamepad. Next, slowly rotate the \");\n        telemetry.log().add(\"compass in a full 360 degree circle while keeping\");\n        telemetry.log().add(\"it level. When complete, press 'B'.\");\n\n        // wait for the start button to be pressed\n        waitForStart();\n        telemetry.log().clear();\n\n        while (opModeIsActive()) {\n\n            // If the A button is pressed, start calibration and wait for the A button to rise\n            if (gamepad1.a && !compass.isCalibrating()) {\n\n                telemetry.log().clear();\n                telemetry.log().add(\"Calibration started\");\n                telemetry.log().add(\"Slowly rotate compass 360deg\");\n                telemetry.log().add(\"Press 'B' when complete\");\n                compass.setMode(CompassSensor.CompassMode.CALIBRATION_MODE);\n                timer.reset();\n\n                while (gamepad1.a && opModeIsActive()) {\n                    doTelemetry();\n                    idle();\n                }\n            }\n\n            // If the B button is pressed, stop calibration and wait for the B button to rise\n            if (gamepad1.b && compass.isCalibrating()) {\n\n                telemetry.log().clear();\n                telemetry.log().add(\"Calibration complete\");\n                compass.setMode(CompassSensor.CompassMode.MEASUREMENT_MODE);\n\n                if (compass.calibrationFailed()) {\n                    telemetry.log().add(\"Calibration failed\");\n                    compass.writeCommand(ModernRoboticsI2cCompassSensor.Command.NORMAL);\n                }\n\n                while (gamepad1.a && opModeIsActive()) {\n                    doTelemetry();\n                    idle();\n                }\n            }\n\n            doTelemetry();\n        }\n    }\n\n    protected void doTelemetry() {\n\n        if (compass.isCalibrating()) {\n\n            telemetry.addData(\"compass\", \"calibrating %s\", Math.round(timer.seconds())%2==0 ? \"|..\" : \"..|\");\n\n        } else {\n\n            // getDirection() returns a traditional compass heading in the range [0,360),\n            // with values increasing in a CW direction\n            telemetry.addData(\"heading\", \"%.1f\", compass.getDirection());\n\n            // getAcceleration() returns the current 3D acceleration experienced by\n            // the sensor. This is used internally to the sensor to compute its tilt and thence\n            // to correct the magnetometer reading to produce tilt-corrected values in getDirection()\n            Acceleration accel = compass.getAcceleration();\n            double accelMagnitude = Math.sqrt(accel.xAccel*accel.xAccel + accel.yAccel*accel.yAccel + accel.zAccel*accel.zAccel);\n            telemetry.addData(\"accel\", accel);\n            telemetry.addData(\"accel magnitude\", \"%.3f\", accelMagnitude);\n\n            // getMagneticFlux returns the 3D magnetic field flux experienced by the sensor\n            telemetry.addData(\"mag flux\", compass.getMagneticFlux());\n        }\n\n        // the command register provides status data\n        telemetry.addData(\"command\", \"%s\", compass.readCommand());\n\n        telemetry.update();\n    }\n}\n",
    "ConceptScanServo": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.Servo;\n\n/**\n * This OpMode scans a single servo back and forwards until Stop is pressed.\n * The code is structured as a LinearOpMode\n * INCREMENT sets how much to increase/decrease the servo position each cycle\n * CYCLE_MS sets the update period.\n *\n * This code assumes a Servo configured with the name \"left_hand\" as is found on a pushbot.\n *\n * NOTE: When any servo position is set, ALL attached servos are activated, so ensure that any other\n * connected servos are able to move freely before running this test.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    static final double INCREMENT   = 0.01;     // amount to slew servo each CYCLE_MS cycle\n    static final int    CYCLE_MS    =   50;     // period of each cycle\n    static final double MAX_POS     =  1.0;     // Maximum rotational position\n    static final double MIN_POS     =  0.0;     // Minimum rotational position\n\n    // Define class REPLACE_CLASS\n    Servo   servo;\n    double  position = (MAX_POS - MIN_POS) / 2; // Start at halfway position\n    boolean rampUp = true;\n\n\n    @Override\n    public void runOpMode() {\n\n        // Connect to servo (Assume PushBot Left Hand)\n        // Change the text in quotes to match any servo name on your robot.\n        servo = hardwareMap.get(Servo.class, \"left_hand\");\n\n        // Wait for the start button\n        telemetry.addData(\">\", \"Press Start to scan Servo.\" );\n        telemetry.update();\n        waitForStart();\n\n\n        // Scan servo till stop pressed.\n        while(opModeIsActive()){\n\n            // slew the servo, according to the rampUp (direction) variable.\n            if (rampUp) {\n                // Keep stepping up until we hit the max value.\n                position += INCREMENT ;\n                if (position >= MAX_POS ) {\n                    position = MAX_POS;\n                    rampUp = !rampUp;   // Switch ramp direction\n                }\n            }\n            else {\n                // Keep stepping down until we hit the min value.\n                position -= INCREMENT ;\n                if (position <= MIN_POS ) {\n                    position = MIN_POS;\n                    rampUp = !rampUp;  // Switch ramp direction\n                }\n            }\n\n            // Display the current value\n            telemetry.addData(\"Servo Position\", \"%5.2f\", position);\n            telemetry.addData(\">\", \"Press Stop to end test.\" );\n            telemetry.update();\n\n            // Set the servo to the new position and pause;\n            servo.setPosition(position);\n            sleep(CYCLE_MS);\n            idle();\n        }\n\n        // Signal done;\n        telemetry.addData(\">\", \"Done\");\n        telemetry.update();\n    }\n}\n",
    "SensorKLNavxMicro": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.hardware.kauailabs.NavxMicroNavigationSensor;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.Gyroscope;\nimport com.qualcomm.robotcore.hardware.IntegratingGyroscope;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\nimport org.firstinspires.ftc.robotcore.external.navigation.AngleUnit;\nimport org.firstinspires.ftc.robotcore.external.navigation.AngularVelocity;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesOrder;\nimport org.firstinspires.ftc.robotcore.external.navigation.AxesReference;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\n\n/*\n * This is an example LinearOpMode that shows how to use Kauai Labs navX Micro Robotics Navigation\n * Sensor. It assumes that the sensor is configured with a name of \"navx\".\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n */\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    /** In this sample, for illustration purposes we use two interfaces on the one gyro object.\n     * That's likely atypical: you'll probably use one or the other in any given situation,\n     * depending on what you're trying to do. {@link IntegratingGyroscope} (and it's base interface,\n     * {@link Gyroscope}) are common interfaces supported by possibly several different gyro\n     * implementations. {@link NavxMicroNavigationSensor}, by contrast, provides functionality that\n     * is unique to the navX Micro sensor.\n     */\n    IntegratingGyroscope gyro;\n    NavxMicroNavigationSensor navxMicro;\n\n    // A timer helps provide feedback while calibration is taking place\n    ElapsedTime timer = new ElapsedTime();\n\n    @Override public void runOpMode() throws InterruptedException {\n        // Get a reference to a Modern Robotics GyroSensor object. We use several interfaces\n        // on this object to illustrate which interfaces support which functionality.\n        navxMicro = hardwareMap.get(NavxMicroNavigationSensor.class, \"navx\");\n        gyro = (IntegratingGyroscope)navxMicro;\n        // If you're only interested int the IntegratingGyroscope interface, the following will suffice.\n        // gyro = hardwareMap.get(IntegratingGyroscope.class, \"navx\");\n\n        // The gyro automatically starts calibrating. This takes a few seconds.\n        telemetry.log().add(\"Gyro Calibrating. Do Not Move!\");\n\n        // Wait until the gyro calibration is complete\n        timer.reset();\n        while (navxMicro.isCalibrating())  {\n            telemetry.addData(\"calibrating\", \"%s\", Math.round(timer.seconds())%2==0 ? \"|..\" : \"..|\");\n            telemetry.update();\n            Thread.sleep(50);\n        }\n        telemetry.log().clear(); telemetry.log().add(\"Gyro Calibrated. Press Start.\");\n        telemetry.clear(); telemetry.update();\n\n        // Wait for the start button to be pressed\n        waitForStart();\n        telemetry.log().clear();\n\n        while (opModeIsActive()) {\n\n            // Read dimensionalized data from the gyro. This gyro can report angular velocities\n            // about all three axes. Additionally, it internally integrates the Z axis to\n            // be able to report an absolute angular Z orientation.\n            AngularVelocity rates = gyro.getAngularVelocity(AngleUnit.DEGREES);\n            Orientation angles = gyro.getAngularOrientation(AxesReference.INTRINSIC, AxesOrder.ZYX, AngleUnit.DEGREES);\n\n            telemetry.addLine()\n                .addData(\"dx\", formatRate(rates.xRotationRate))\n                .addData(\"dy\", formatRate(rates.yRotationRate))\n                .addData(\"dz\", \"%s deg/s\", formatRate(rates.zRotationRate));\n\n            telemetry.addLine()\n                .addData(\"heading\", formatAngle(angles.angleUnit, angles.firstAngle))\n                .addData(\"roll\", formatAngle(angles.angleUnit, angles.secondAngle))\n                .addData(\"pitch\", \"%s deg\", formatAngle(angles.angleUnit, angles.thirdAngle));\n            telemetry.update();\n\n            idle(); // Always call idle() at the bottom of your while(opModeIsActive()) loop\n        }\n    }\n\n    String formatRate(float rate) {\n        return String.format(\"%.3f\", rate);\n    }\n\n    String formatAngle(AngleUnit angleUnit, double angle) {\n        return formatDegrees(AngleUnit.DEGREES.fromUnit(angleUnit, angle));\n    }\n\n    String formatDegrees(double degrees){\n        return String.format(\"%.1f\", AngleUnit.DEGREES.normalize(degrees));\n    }\n}\n",
    "ConceptSoundsSKYSTONE": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport android.content.Context;\n\nimport com.qualcomm.ftccommon.SoundPlayer;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\n\n/**\n * This file demonstrates how to play one of the several SKYSTONE/Star Wars sounds loaded into the SDK.\n * It does this by creating a simple \"chooser\" controlled by the gamepad Up Down buttons.\n * This code also prevents sounds from stacking up by setting a \"playing\" flag, which is cleared when the sound finishes playing.\n *\n * Use Android Studios to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list\n *\n * Operation:\n *      Use the DPAD to change the selected sound, and the Right Bumper to play it.\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\" };\n    boolean soundPlaying = false;\n\n    @Override\n    public void runOpMode() {\n\n        // Variables for choosing from the available sounds\n        int     soundIndex      = 0;\n        int     soundID         = -1;\n        boolean was_dpad_up     = false;\n        boolean was_dpad_down   = false;\n\n        Context myApp = hardwareMap.appContext;\n\n        // create a sound parameter that holds the desired player parameters.\n        SoundPlayer.PlaySoundParams params = new SoundPlayer.PlaySoundParams();\n        params.loopControl = 0;\n        params.waitForNonLoopingSoundsToFinish = true;\n\n        // In this sample, we will skip waiting for the user to press play, and start displaying sound choices right away\n        while (!isStopRequested()) {\n\n            // Look for DPAD presses to change the selection\n            if (gamepad1.dpad_down && !was_dpad_down) {\n                // Go to next sound (with list wrap) and display it\n                soundIndex = (soundIndex + 1) % sounds.length;\n            }\n\n            if (gamepad1.dpad_up && !was_dpad_up) {\n                // Go to previous sound (with list wrap) and display it\n                soundIndex = (soundIndex + sounds.length - 1) % sounds.length;\n            }\n\n            // Look for trigger to see if we should play sound\n            // Only start a new sound if we are currently not playing one.\n            if (gamepad1.right_bumper && !soundPlaying) {\n\n                // Determine Resource IDs for the sounds you want to play, and make sure it's valid.\n                if ((soundID = myApp.getResources().getIdentifier(sounds[soundIndex], \"raw\", myApp.getPackageName())) != 0){\n\n                    // Signal that the sound is now playing.\n                    soundPlaying = true;\n\n                    // Start playing, and also Create a callback that will clear the playing flag when the sound is complete.\n                    SoundPlayer.getInstance().startPlaying(myApp, soundID, params, null,\n                            new Runnable() {\n                                public void run() {\n                                    soundPlaying = false;\n                                }} );\n                }\n            }\n\n            // Remember the last state of the dpad to detect changes.\n            was_dpad_up     = gamepad1.dpad_up;\n            was_dpad_down   = gamepad1.dpad_down;\n\n            // Display the current sound choice, and the playing status.\n            telemetry.addData(\"\", \"Use DPAD up/down to choose sound.\");\n            telemetry.addData(\"\", \"Press Right Bumper to play sound.\");\n            telemetry.addData(\"\", \"\");\n            telemetry.addData(\"Sound >\", sounds[soundIndex]);\n            telemetry.addData(\"Status >\", soundPlaying ? \"Playing\" : \"Stopped\");\n            telemetry.update();\n        }\n    }\n}\n",
    "ConceptGamepadRumble": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.Gamepad;\nimport com.qualcomm.robotcore.util.ElapsedTime;\n\n/**\n * This sample illustrates using the rumble feature of many gamepads.\n *\n * Note: Some gamepads \"rumble\" better than others.\n *   The Xbox & PS4 have a left (rumble1) and right (rumble2) rumble motor.\n *   These two gamepads have two distinct rumble modes: Large on the left, and small on the right\n *   The ETpark gamepad may only respond to rumble1, and may only run at full power.\n *   The Logitech F310 gamepad does not have *any* rumble ability.\n *\n *   Moral:  You should use this sample to experiment with your specific gamepads to explore their rumble features.\n *\n * The rumble motors are accessed through the standard gamepad1 and gamepad2 objects.\n *   Several new methods were added to the Gamepad class REPLACE_CLASS FTC SDK Rev 7\n *   The key methods are as follows:\n *\n *   .rumble(double rumble1, double rumble2, int durationMs)\n *     This method sets the rumble power of both motors for a specific time duration.\n *     Both rumble arguments are motor-power levels in the 0.0 to 1.0 range.\n *     durationMs is the desired length of the rumble action in milliseconds.\n *     This method returns immediately.\n *     Note:\n *       Use a durationMs of Gamepad.RUMBLE_DURATION_CONTINUOUS to provide a continuous rumble\n *       Use a power of 0, or duration of 0 to stop a rumble.\n *\n *   .rumbleBlips(int count) allows an easy way to signal the driver with a series of rumble blips.\n *     Just specify how many blips you want.\n *     This method returns immediately.\n *\n *   .runRumbleEffect(customRumbleEffect) allows you to run a custom rumble sequence that you have\n *     built using the Gamepad.RumbleEffect.Builder()\n *     A \"custom effect\" is a sequence of steps, where each step can rumble any of the\n *     rumble motors for a specific period at a specific power level.\n *     The Custom Effect will play as the (un-blocked) OpMode continues to run\n *\n *   .isRumbling() returns true if there is a rumble effect in progress.\n *     Use this call to prevent stepping on a current rumble.\n *\n *   .stopRumble()              Stop any ongoing rumble or custom rumble effect.\n *\n *   .rumble(int durationMs)    Full power rumble for fixed duration.\n *\n *   Note: Whenever a new Rumble command is issued, any currently executing rumble action will\n *   be truncated, and the new action started immediately.  Take these precautions:\n *      1) Do Not SPAM the rumble motors by issuing rapid fire commands\n *      2) Multiple sources for rumble commands must coordinate to avoid tromping on each other.\n *\n *   This can be achieved several possible ways:\n *   1) Only having one source for rumble actions\n *   2) Issuing rumble commands on transitions, rather than states.\n *      e.g. The moment a touch sensor is pressed, rather than the entire time it is being pressed.\n *   3) Scheduling rumble commands based on timed events. e.g. 10 seconds prior to endgame\n *   4) Rumble on non-overlapping mechanical actions. e.g. arm fully-extended or fully-retracted.\n *   5) Use isRumbling() to hold off on a new rumble if one is already in progress.\n *\n * The examples shown here are representstive of how to invoke a gamepad rumble.\n * It is assumed that these will be modified to suit the specific robot and team strategy needs.\n *\n * ########   Read the telemetry display on the Driver Station Screen for instructions.   ######\n *\n * Ex 1)    This example shows a) how to create a custom rumble effect, and then b) how to trigger it based\n *          on game time.  One use for this might be to alert the driver that half-time or End-game is approaching.\n *\n * Ex 2)    This example shows tying the rumble power to a changing sensor value.\n *          In this case it is the Gamepad trigger, but it could be any sensor output scaled to the 0-1 range.\n *          Since it takes over the rumble motors, it is only performed when the Left Bumper is pressed.\n *          Note that this approach MUST include a way to turn OFF the rumble when the button is released.\n *\n * Ex 3)    This example shows a simple way to trigger a 3-blip sequence.  In this case it is\n *          triggered by the gamepad A (Cross) button, but it could be any sensor, like a touch or light sensor.\n *          Note that this code ensures that it only rumbles once when the input goes true.\n *\n * Ex 4)    This example shows how to trigger a single rumble when an input value gets over a certain value.\n *          In this case it is reading the Right Trigger, but it could be any variable sensor, like a\n *          range sensor, or position sensor.  The code needs to ensure that it is only triggered once, so\n *          it waits till the sensor drops back below the threshold before it can trigger again.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this OpMode to the Driver Station OpMode list.\n */\n\n@Disabled\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\npublic class REPLACE_CLASS extends LinearOpMode\n{\n    boolean lastA = false;                      // Use to track the prior button state.\n    boolean lastLB = false;                     // Use to track the prior button state.\n    boolean highLevel = false;                  // used to prevent multiple level-based rumbles.\n    boolean secondHalf = false;                 // Use to prevent multiple half-time warning rumbles.\n\n    Gamepad.RumbleEffect customRumbleEffect;    // Use to build a custom rumble sequence.\n    ElapsedTime runtime = new ElapsedTime();    // Use to determine when end game is starting.\n\n    final double HALF_TIME = 60.0;              // Wait this many seconds before rumble-alert for half-time.\n    final double TRIGGER_THRESHOLD  = 0.75;     // Squeeze more than 3/4 to get rumble.\n\n    @Override\n    public void runOpMode()\n    {\n        // Example 1. a)   start by creating a three-pulse rumble sequence: right, LEFT, LEFT\n        customRumbleEffect = new Gamepad.RumbleEffect.Builder()\n                .addStep(0.0, 1.0, 500)  //  Rumble right motor 100% for 500 mSec\n                .addStep(0.0, 0.0, 300)  //  Pause for 300 mSec\n                .addStep(1.0, 0.0, 250)  //  Rumble left motor 100% for 250 mSec\n                .addStep(0.0, 0.0, 250)  //  Pause for 250 mSec\n                .addStep(1.0, 0.0, 250)  //  Rumble left motor 100% for 250 mSec\n                .build();\n\n        telemetry.addData(\">\", \"Press Start\");\n        telemetry.update();\n\n        waitForStart();\n        runtime.reset();    // Start game timer.\n\n        // Loop while monitoring buttons for rumble triggers\n        while (opModeIsActive())\n        {\n            // Read and save the current gamepad button states.\n            boolean currentA = gamepad1.a ;\n            boolean currentLB = gamepad1.left_bumper ;\n\n            // Display the current Rumble status.  Just for interest.\n            telemetry.addData(\">\", \"Are we RUMBLING? %s\\n\", gamepad1.isRumbling() ? \"YES\" : \"no\" );\n\n            // ----------------------------------------------------------------------------------------\n            // Example 1. b) Watch the runtime timer, and run the custom rumble when we hit half-time.\n            //               Make sure we only signal once by setting \"secondHalf\" flag to prevent further rumbles.\n            // ----------------------------------------------------------------------------------------\n            if ((runtime.seconds() > HALF_TIME) && !secondHalf)  {\n                gamepad1.runRumbleEffect(customRumbleEffect);\n                secondHalf =true;\n            }\n\n            // Display the time remaining while we are still counting down.\n            if (!secondHalf) {\n                telemetry.addData(\">\", \"Halftime Alert Countdown: %3.0f Sec \\n\", (HALF_TIME - runtime.seconds()) );\n            }\n\n\n            // ----------------------------------------------------------------------------------------\n            // Example 2. If Left Bumper is being pressed, power the rumble motors based on the two trigger depressions.\n            // This is useful to see how the rumble feels at various power levels.\n            // ----------------------------------------------------------------------------------------\n            if (currentLB) {\n                // Left Bumper is being pressed, so send left and right \"trigger\" values to left and right rumble motors.\n                gamepad1.rumble(gamepad1.left_trigger, gamepad1.right_trigger, Gamepad.RUMBLE_DURATION_CONTINUOUS);\n\n                // Show what is being sent to rumbles\n                telemetry.addData(\">\", \"Squeeze triggers to control rumbles\");\n                telemetry.addData(\"> : Rumble\", \"Left: %.0f%%   Right: %.0f%%\", gamepad1.left_trigger * 100, gamepad1.right_trigger * 100);\n            } else {\n                // Make sure rumble is turned off when Left Bumper is released (only one time each press)\n                if (lastLB) {\n                    gamepad1.stopRumble();\n                }\n\n                //  Prompt for manual rumble action\n                telemetry.addData(\">\", \"Hold Left-Bumper to test Manual Rumble\");\n                telemetry.addData(\">\", \"Press A (Cross) for three blips\");\n                telemetry.addData(\">\", \"Squeeze right trigger slowly for 1 blip\");\n            }\n            lastLB = currentLB; // remember the current button state for next time around the loop\n\n\n            // ----------------------------------------------------------------------------------------\n            // Example 3. Blip 3 times at the moment that A (Cross) is pressed. (look for pressed transition)\n            // BUT !!!  Skip it altogether if the Gamepad is already rumbling.\n            // ----------------------------------------------------------------------------------------\n            if (currentA && !lastA) {\n                if (!gamepad1.isRumbling())  // Check for possible overlap of rumbles.\n                    gamepad1.rumbleBlips(3);\n            }\n            lastA = currentA; // remember the current button state for next time around the loop\n\n\n            // ----------------------------------------------------------------------------------------\n            // Example 4. Rumble once when gamepad right trigger goes above the THRESHOLD.\n            // ----------------------------------------------------------------------------------------\n            if (gamepad1.right_trigger > TRIGGER_THRESHOLD) {\n                if (!highLevel) {\n                    gamepad1.rumble(0.9, 0, 200);  // 200 mSec burst on left motor.\n                    highLevel = true;  // Hold off any more triggers\n                }\n            } else {\n                highLevel = false;  // We can trigger again now.\n            }\n\n            // Send the telemetry data to the Driver Station, and then pause to pace the program.\n            telemetry.update();\n            sleep(10);\n        }\n    }\n}\n",
    "ConceptGamepadTouchpad": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\n\nimport org.firstinspires.ftc.robotcore.external.Telemetry;\n\n/**\n * This sample illustrates using the touchpad feature found on some gamepads.\n *\n * The Sony PS4 gamepad can detect two distinct touches on the central touchpad.\n * Other gamepads with different touchpads may provide mixed results.\n *\n * The touchpads are accessed through the standard gamepad1 and gamepad2 objects.\n *   Several new members were added to the Gamepad class REPLACE_CLASS FTC SDK Rev 7\n *\n *   .touchpad_finger_1     returns true if at least one finger is detected.\n *   .touchpad_finger_1_x   finger 1 X coordinate.  Valid if touchpad_finger_1 is true\n *   .touchpad_finger_1_y   finger 1 Y coordinate.  Valid if touchpad_finger_1 is true\n *\n *   .touchpad_finger_2     returns true if a second finger is detected\n *   .touchpad_finger_2_x   finger 2 X coordinate.  Valid if touchpad_finger_2 is true\n *   .touchpad_finger_2_y   finger 2 Y coordinate.  Valid if touchpad_finger_2 is true\n *\n * Finger touches are reported with an X and Y coordinate in following coordinate system.\n *\n *   1) X is the Horizontal axis, and Y is the vertical axis\n *   2) The 0,0 origin is at the center of the touchpad.\n *   3)  1.0, 1.0 is at the top right corner of the touchpad.\n *   4) -1.0,-1.0 is at the bottom left corner of the touchpad.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this OpMode to the Driver Station OpMode list.\n */\n\n@Disabled\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\npublic class REPLACE_CLASS extends LinearOpMode\n{\n    @Override\n    public void runOpMode()\n    {\n        telemetry.setDisplayFormat(Telemetry.DisplayFormat.MONOSPACE);\n\n        telemetry.addData(\">\", \"Press Start\");\n        telemetry.update();\n\n        waitForStart();\n\n        while (opModeIsActive())\n        {\n            boolean finger = false;\n\n            // Display finger 1 x & y position if finger detected\n            if(gamepad1.touchpad_finger_1)\n            {\n                finger = true;\n                telemetry.addLine(String.format(\"Finger 1: x=%5.2f y=%5.2f\\n\", gamepad1.touchpad_finger_1_x, gamepad1.touchpad_finger_1_y));\n            }\n\n            // Display finger 2 x & y position if finger detected\n            if(gamepad1.touchpad_finger_2)\n            {\n                finger = true;\n                telemetry.addLine(String.format(\"Finger 2: x=%5.2f y=%5.2f\\n\", gamepad1.touchpad_finger_2_x, gamepad1.touchpad_finger_2_y));\n            }\n\n            if(!finger)\n            {\n                telemetry.addLine(\"No fingers\");\n            }\n\n            telemetry.update();\n            sleep(10);\n        }\n    }\n}\n",
    "ConceptVuforiaFieldNavigation": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.matrices.OpenGLMatrix;\nimport org.firstinspires.ftc.robotcore.external.matrices.VectorF;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackable;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackableDefaultListener;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackables;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport static org.firstinspires.ftc.robotcore.external.navigation.AngleUnit.DEGREES;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesOrder.XYZ;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesOrder.YZX;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesReference.EXTRINSIC;\nimport static org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer.CameraDirection.BACK;\n\n/**\n * This OpMode illustrates using the Vuforia localizer to determine positioning and orientation of\n * robot on the FTC field using the RC phone's camera.  The code is structured as a LinearOpMode\n *\n * Note: If you are using a WEBCAM see ConceptVuforiaFieldNavigationWebcam.java\n *\n * When images are located, Vuforia is able to determine the position and orientation of the\n * image relative to the camera.  This sample code then combines that information with a\n * knowledge of where the target images are on the field, to determine the location of the camera.\n *\n * Finally, the location of the camera on the robot is used to determine the\n * robot's location and orientation on the field.\n *\n * To learn more about the FTC field coordinate model, see FTC_FieldCoordinateSystemDefinition.pdf in this folder\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    // IMPORTANT:  For Phone Camera, set 1) the camera source and 2) the orientation, based on how your phone is mounted:\n    // 1) Camera Source.  Valid choices are:  BACK (behind screen) or FRONT (selfie side)\n    // 2) Phone Orientation. Choices are: PHONE_IS_PORTRAIT = true (portrait) or PHONE_IS_PORTRAIT = false (landscape)\n\n    private static final VuforiaLocalizer.CameraDirection CAMERA_CHOICE = BACK;\n    private static final boolean PHONE_IS_PORTRAIT = false  ;\n\n    /*\n     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n     * web site at https://developer.vuforia.com/license-manager.\n     *\n     * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n     * random data. As an example, here is a example of a fragment of a valid key:\n     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n     * Once you've obtained a license key, copy the string from the Vuforia web site\n     * and paste it in to your code on the next line, between the double quotes.\n     */\n    private static final String VUFORIA_KEY =\n            \" -- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n    // Since ImageTarget trackables use mm to specifiy their dimensions, we must use mm for all the physical dimension.\n    // We will define some constants and conversions here.  These are useful for the Freight Frenzy field.\n    private static final float mmPerInch        = 25.4f;\n    private static final float mmTargetHeight   = 6 * mmPerInch;          // the height of the center of the target image above the floor\n    private static final float halfField        = 72 * mmPerInch;\n    private static final float halfTile         = 12 * mmPerInch;\n    private static final float oneAndHalfTile   = 36 * mmPerInch;\n\n    // Class Members\n    private OpenGLMatrix lastLocation = null;\n    private VuforiaLocalizer vuforia  = null;\n    private VuforiaTrackables targets = null ;\n\n    private boolean targetVisible = false;\n    private float phoneXRotate    = 0;\n    private float phoneYRotate    = 0;\n    private float phoneZRotate    = 0;\n\n    @Override public void runOpMode() {\n        /*\n         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.\n         * To get an on-phone camera preview, use the code below.\n         * If no camera preview is desired, use the parameter-less constructor instead (commented out below).\n         */\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters(cameraMonitorViewId);\n        // VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        parameters.vuforiaLicenseKey = VUFORIA_KEY;\n        parameters.cameraDirection   = CAMERA_CHOICE;\n\n        // Turn off Extended tracking.  Set this true if you want Vuforia to track beyond the target.\n        parameters.useExtendedTracking = false;\n\n        //  Instantiate the Vuforia engine\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        // Load the data sets for the trackable objects. These particular data\n        // sets are stored in the 'assets' part of our application.\n        targets = this.vuforia.loadTrackablesFromAsset(\"FreightFrenzy\");\n\n        // For convenience, gather together all the trackable objects in one easily-iterable collection */\n        List<VuforiaTrackable> allTrackables = new ArrayList<VuforiaTrackable>();\n        allTrackables.addAll(targets);\n\n        /**\n         * In order for localization to work, we need to tell the system where each target is on the field, and\n         * where the phone resides on the robot.  These specifications are in the form of <em>transformation matrices.</em>\n         * Transformation matrices are a central, important concept in the math here involved in localization.\n         * See <a href=\"https://en.wikipedia.org/wiki/Transformation_matrix\">Transformation Matrix</a>\n         * for detailed information. Commonly, you'll encounter transformation matrices as instances\n         * of the {@link OpenGLMatrix} class.\n         *\n         * If you are standing in the Red Alliance Station looking towards the center of the field,\n         *     - The X axis runs from your left to the right. (positive from the center to the right)\n         *     - The Y axis runs from the Red Alliance Station towards the other side of the field\n         *       where the Blue Alliance Station is. (Positive is from the center, towards the BlueAlliance station)\n         *     - The Z axis runs from the floor, upwards towards the ceiling.  (Positive is above the floor)\n         *\n         * Before being transformed, each target image is conceptually located at the origin of the field's\n         *  coordinate system (the center of the field), facing up.\n         */\n\n        // Name and locate each trackable object\n        identifyTarget(0, \"Blue Storage\",       -halfField,  oneAndHalfTile, mmTargetHeight, 90, 0, 90);\n        identifyTarget(1, \"Blue Alliance Wall\",  halfTile,   halfField,      mmTargetHeight, 90, 0, 0);\n        identifyTarget(2, \"Red Storage\",        -halfField, -oneAndHalfTile, mmTargetHeight, 90, 0, 90);\n        identifyTarget(3, \"Red Alliance Wall\",   halfTile,  -halfField,      mmTargetHeight, 90, 0, 180);\n\n        /*\n         * Create a transformation matrix describing where the phone is on the robot.\n         *\n         * NOTE !!!!  It's very important that you turn OFF your phone's Auto-Screen-Rotation option.\n         * Lock it into Portrait for these numbers to work.\n         *\n         * Info:  The coordinate frame for the robot looks the same as the field.\n         * The robot's \"forward\" direction is facing out along X axis, with the LEFT side facing out along the Y axis.\n         * Z is UP on the robot.  This equates to a heading angle of Zero degrees.\n         *\n         * The phone starts out lying flat, with the screen facing Up and with the physical top of the phone\n         * pointing to the LEFT side of the Robot.\n         * The two examples below assume that the camera is facing forward out the front of the robot.\n         */\n\n        // We need to rotate the camera around its long axis to bring the correct camera forward.\n        if (CAMERA_CHOICE == BACK) {\n            phoneYRotate = -90;\n        } else {\n            phoneYRotate = 90;\n        }\n\n        // Rotate the phone vertical about the X axis if it's in portrait mode\n        if (PHONE_IS_PORTRAIT) {\n            phoneXRotate = 90 ;\n        }\n\n        // Next, translate the camera lens to where it is on the robot.\n        // In this example, it is centered on the robot (left-to-right and front-to-back), and 6 inches above ground level.\n        final float CAMERA_FORWARD_DISPLACEMENT  = 0.0f * mmPerInch;   // eg: Enter the forward distance from the center of the robot to the camera lens\n        final float CAMERA_VERTICAL_DISPLACEMENT = 6.0f * mmPerInch;   // eg: Camera is 6 Inches above ground\n        final float CAMERA_LEFT_DISPLACEMENT     = 0.0f * mmPerInch;   // eg: Enter the left distance from the center of the robot to the camera lens\n\n        OpenGLMatrix robotFromCamera = OpenGLMatrix\n                    .translation(CAMERA_FORWARD_DISPLACEMENT, CAMERA_LEFT_DISPLACEMENT, CAMERA_VERTICAL_DISPLACEMENT)\n                    .multiplied(Orientation.getRotationMatrix(EXTRINSIC, YZX, DEGREES, phoneYRotate, phoneZRotate, phoneXRotate));\n\n        /**  Let all the trackable listeners know where the phone is.  */\n        for (VuforiaTrackable trackable : allTrackables) {\n            ((VuforiaTrackableDefaultListener) trackable.getListener()).setPhoneInformation(robotFromCamera, parameters.cameraDirection);\n        }\n\n        /*\n         * WARNING:\n         * In this sample, we do not wait for PLAY to be pressed.  Target Tracking is started immediately when INIT is pressed.\n         * This sequence is used to enable the new remote DS Camera Stream feature to be used with this sample.\n         * CONSEQUENTLY do not put any driving commands in this loop.\n         * To restore the normal opmode structure, just un-comment the following line:\n         */\n\n        // waitForStart();\n\n        /* Note: To use the remote camera preview:\n         * AFTER you hit Init on the Driver Station, use the \"options menu\" to select \"Camera Stream\"\n         * Tap the preview window to receive a fresh image.\n         * It is not permitted to transition to RUN while the camera preview window is active.\n         * Either press STOP to exit the OpMode, or use the \"options menu\" again, and select \"Camera Stream\" to close the preview window.\n         */\n\n        targets.activate();\n        while (!isStopRequested()) {\n\n            // check all the trackable targets to see which one (if any) is visible.\n            targetVisible = false;\n            for (VuforiaTrackable trackable : allTrackables) {\n                if (((VuforiaTrackableDefaultListener)trackable.getListener()).isVisible()) {\n                    telemetry.addData(\"Visible Target\", trackable.getName());\n                    targetVisible = true;\n\n                    // getUpdatedRobotLocation() will return null if no new information is available since\n                    // the last time that call was made, or if the trackable is not currently visible.\n                    OpenGLMatrix robotLocationTransform = ((VuforiaTrackableDefaultListener)trackable.getListener()).getUpdatedRobotLocation();\n                    if (robotLocationTransform != null) {\n                        lastLocation = robotLocationTransform;\n                    }\n                    break;\n                }\n            }\n\n            // Provide feedback as to where the robot is located (if we know).\n            if (targetVisible) {\n                // express position (translation) of robot in inches.\n                VectorF translation = lastLocation.getTranslation();\n                telemetry.addData(\"Pos (inches)\", \"{X, Y, Z} = %.1f, %.1f, %.1f\",\n                        translation.get(0) / mmPerInch, translation.get(1) / mmPerInch, translation.get(2) / mmPerInch);\n\n                // express the rotation of the robot in degrees.\n                Orientation rotation = Orientation.getOrientation(lastLocation, EXTRINSIC, XYZ, DEGREES);\n                telemetry.addData(\"Rot (deg)\", \"{Roll, Pitch, Heading} = %.0f, %.0f, %.0f\", rotation.firstAngle, rotation.secondAngle, rotation.thirdAngle);\n            }\n            else {\n                telemetry.addData(\"Visible Target\", \"none\");\n            }\n            telemetry.update();\n        }\n\n        // Disable Tracking when we are done;\n        targets.deactivate();\n    }\n\n    /***\n     * Identify a target by naming it, and setting its position and orientation on the field\n     * @param targetIndex\n     * @param targetName\n     * @param dx, dy, dz  Target offsets in x,y,z axes\n     * @param rx, ry, rz  Target rotations in x,y,z axes\n     */\n    void    identifyTarget(int targetIndex, String targetName, float dx, float dy, float dz, float rx, float ry, float rz) {\n        VuforiaTrackable aTarget = targets.get(targetIndex);\n        aTarget.setName(targetName);\n        aTarget.setLocation(OpenGLMatrix.translation(dx, dy, dz)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, rx, ry, rz)));\n    }\n}\n",
    "ConceptVuforiaFieldNavigationWebcam": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.robotcore.external.matrices.OpenGLMatrix;\nimport org.firstinspires.ftc.robotcore.external.matrices.VectorF;\nimport org.firstinspires.ftc.robotcore.external.navigation.Orientation;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackable;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackableDefaultListener;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackables;\n\nimport java.util.ArrayList;\nimport java.util.List;\n\nimport static org.firstinspires.ftc.robotcore.external.navigation.AngleUnit.DEGREES;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesOrder.XYZ;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesOrder.XZY;\nimport static org.firstinspires.ftc.robotcore.external.navigation.AxesReference.EXTRINSIC;\n\n/**\n * This OpMode illustrates using the Vuforia localizer to determine positioning and orientation of\n * robot on the FTC field using a WEBCAM.  The code is structured as a LinearOpMode\n *\n * NOTE: If you are running on a Phone with a built-in camera, use the ConceptVuforiaFieldNavigation example instead of this one.\n * NOTE: It is possible to switch between multiple WebCams (eg: one for the left side and one for the right).\n *       For a related example of how to do this, see ConceptTensorFlowObjectDetectionSwitchableCameras\n *\n * When images are located, Vuforia is able to determine the position and orientation of the\n * image relative to the camera.  This sample code then combines that information with a\n * knowledge of where the target images are on the field, to determine the location of the camera.\n *\n * Finally, the location of the camera on the robot is used to determine the\n * robot's location and orientation on the field.\n *\n * To learn more about the FTC field coordinate model, see FTC_FieldCoordinateSystemDefinition.pdf in this folder\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode {\n\n    /*\n     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n     * web site at https://developer.vuforia.com/license-manager.\n     *\n     * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n     * random data. As an example, here is a example of a fragment of a valid key:\n     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n     * Once you've obtained a license key, copy the string from the Vuforia web site\n     * and paste it in to your code on the next line, between the double quotes.\n     */\n    private static final String VUFORIA_KEY =\n            \" --- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n    // Since ImageTarget trackables use mm to specifiy their dimensions, we must use mm for all the physical dimension.\n    // We will define some constants and conversions here\n    private static final float mmPerInch        = 25.4f;\n    private static final float mmTargetHeight   = 6 * mmPerInch;          // the height of the center of the target image above the floor\n    private static final float halfField        = 72 * mmPerInch;\n    private static final float halfTile         = 12 * mmPerInch;\n    private static final float oneAndHalfTile   = 36 * mmPerInch;\n\n    // Class Members\n    private OpenGLMatrix lastLocation   = null;\n    private VuforiaLocalizer vuforia    = null;\n    private VuforiaTrackables targets   = null ;\n    private WebcamName webcamName       = null;\n\n    private boolean targetVisible       = false;\n\n    @Override public void runOpMode() {\n        // Connect to the camera we are to use.  This name must match what is set up in Robot Configuration\n        webcamName = hardwareMap.get(WebcamName.class, \"Webcam 1\");\n\n        /*\n         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.\n         * We can pass Vuforia the handle to a camera preview resource (on the RC screen);\n         * If no camera-preview is desired, use the parameter-less constructor instead (commented out below).\n         * Note: A preview window is required if you want to view the camera stream on the Driver Station Phone.\n         */\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters(cameraMonitorViewId);\n        // VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        parameters.vuforiaLicenseKey = VUFORIA_KEY;\n\n        // We also indicate which camera we wish to use.\n        parameters.cameraName = webcamName;\n\n        // Turn off Extended tracking.  Set this true if you want Vuforia to track beyond the target.\n        parameters.useExtendedTracking = false;\n\n        //  Instantiate the Vuforia engine\n        vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        // Load the data sets for the trackable objects. These particular data\n        // sets are stored in the 'assets' part of our application.\n        targets = this.vuforia.loadTrackablesFromAsset(\"FreightFrenzy\");\n\n        // For convenience, gather together all the trackable objects in one easily-iterable collection */\n        List<VuforiaTrackable> allTrackables = new ArrayList<VuforiaTrackable>();\n        allTrackables.addAll(targets);\n\n        /**\n         * In order for localization to work, we need to tell the system where each target is on the field, and\n         * where the phone resides on the robot.  These specifications are in the form of <em>transformation matrices.</em>\n         * Transformation matrices are a central, important concept in the math here involved in localization.\n         * See <a href=\"https://en.wikipedia.org/wiki/Transformation_matrix\">Transformation Matrix</a>\n         * for detailed information. Commonly, you'll encounter transformation matrices as instances\n         * of the {@link OpenGLMatrix} class.\n         *\n         * If you are standing in the Red Alliance Station looking towards the center of the field,\n         *     - The X axis runs from your left to the right. (positive from the center to the right)\n         *     - The Y axis runs from the Red Alliance Station towards the other side of the field\n         *       where the Blue Alliance Station is. (Positive is from the center, towards the BlueAlliance station)\n         *     - The Z axis runs from the floor, upwards towards the ceiling.  (Positive is above the floor)\n         *\n         * Before being transformed, each target image is conceptually located at the origin of the field's\n         *  coordinate system (the center of the field), facing up.\n         */\n\n        // Name and locate each trackable object\n        identifyTarget(0, \"Blue Storage\",       -halfField,  oneAndHalfTile, mmTargetHeight, 90, 0, 90);\n        identifyTarget(1, \"Blue Alliance Wall\",  halfTile,   halfField,      mmTargetHeight, 90, 0, 0);\n        identifyTarget(2, \"Red Storage\",        -halfField, -oneAndHalfTile, mmTargetHeight, 90, 0, 90);\n        identifyTarget(3, \"Red Alliance Wall\",   halfTile,  -halfField,      mmTargetHeight, 90, 0, 180);\n\n        /*\n         * Create a transformation matrix describing where the camera is on the robot.\n         *\n         * Info:  The coordinate frame for the robot looks the same as the field.\n         * The robot's \"forward\" direction is facing out along X axis, with the LEFT side facing out along the Y axis.\n         * Z is UP on the robot.  This equates to a bearing angle of Zero degrees.\n         *\n         * For a WebCam, the default starting orientation of the camera is looking UP (pointing in the Z direction),\n         * with the wide (horizontal) axis of the camera aligned with the X axis, and\n         * the Narrow (vertical) axis of the camera aligned with the Y axis\n         *\n         * But, this example assumes that the camera is actually facing forward out the front of the robot.\n         * So, the \"default\" camera position requires two rotations to get it oriented correctly.\n         * 1) First it must be rotated +90 degrees around the X axis to get it horizontal (its now facing out the right side of the robot)\n         * 2) Next it must be be rotated +90 degrees (counter-clockwise) around the Z axis to face forward.\n         *\n         * Finally the camera can be translated to its actual mounting position on the robot.\n         *      In this example, it is centered on the robot (left-to-right and front-to-back), and 6 inches above ground level.\n         */\n\n        final float CAMERA_FORWARD_DISPLACEMENT  = 0.0f * mmPerInch;   // eg: Enter the forward distance from the center of the robot to the camera lens\n        final float CAMERA_VERTICAL_DISPLACEMENT = 6.0f * mmPerInch;   // eg: Camera is 6 Inches above ground\n        final float CAMERA_LEFT_DISPLACEMENT     = 0.0f * mmPerInch;   // eg: Enter the left distance from the center of the robot to the camera lens\n\n        OpenGLMatrix cameraLocationOnRobot = OpenGLMatrix\n                    .translation(CAMERA_FORWARD_DISPLACEMENT, CAMERA_LEFT_DISPLACEMENT, CAMERA_VERTICAL_DISPLACEMENT)\n                    .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XZY, DEGREES, 90, 90, 0));\n\n        /**  Let all the trackable listeners know where the camera is.  */\n        for (VuforiaTrackable trackable : allTrackables) {\n            ((VuforiaTrackableDefaultListener) trackable.getListener()).setCameraLocationOnRobot(parameters.cameraName, cameraLocationOnRobot);\n        }\n\n        /*\n         * WARNING:\n         * In this sample, we do not wait for PLAY to be pressed.  Target Tracking is started immediately when INIT is pressed.\n         * This sequence is used to enable the new remote DS Camera Preview feature to be used with this sample.\n         * CONSEQUENTLY do not put any driving commands in this loop.\n         * To restore the normal opmode structure, just un-comment the following line:\n         */\n\n        // waitForStart();\n\n        /* Note: To use the remote camera preview:\n         * AFTER you hit Init on the Driver Station, use the \"options menu\" to select \"Camera Stream\"\n         * Tap the preview window to receive a fresh image.\n         * It is not permitted to transition to RUN while the camera preview window is active.\n         * Either press STOP to exit the OpMode, or use the \"options menu\" again, and select \"Camera Stream\" to close the preview window.\n         */\n\n        targets.activate();\n        while (!isStopRequested()) {\n\n            // check all the trackable targets to see which one (if any) is visible.\n            targetVisible = false;\n            for (VuforiaTrackable trackable : allTrackables) {\n                if (((VuforiaTrackableDefaultListener)trackable.getListener()).isVisible()) {\n                    telemetry.addData(\"Visible Target\", trackable.getName());\n                    targetVisible = true;\n\n                    // getUpdatedRobotLocation() will return null if no new information is available since\n                    // the last time that call was made, or if the trackable is not currently visible.\n                    OpenGLMatrix robotLocationTransform = ((VuforiaTrackableDefaultListener)trackable.getListener()).getUpdatedRobotLocation();\n                    if (robotLocationTransform != null) {\n                        lastLocation = robotLocationTransform;\n                    }\n                    break;\n                }\n            }\n\n            // Provide feedback as to where the robot is located (if we know).\n            if (targetVisible) {\n                // express position (translation) of robot in inches.\n                VectorF translation = lastLocation.getTranslation();\n                telemetry.addData(\"Pos (inches)\", \"{X, Y, Z} = %.1f, %.1f, %.1f\",\n                        translation.get(0) / mmPerInch, translation.get(1) / mmPerInch, translation.get(2) / mmPerInch);\n\n                // express the rotation of the robot in degrees.\n                Orientation rotation = Orientation.getOrientation(lastLocation, EXTRINSIC, XYZ, DEGREES);\n                telemetry.addData(\"Rot (deg)\", \"{Roll, Pitch, Heading} = %.0f, %.0f, %.0f\", rotation.firstAngle, rotation.secondAngle, rotation.thirdAngle);\n            }\n            else {\n                telemetry.addData(\"Visible Target\", \"none\");\n            }\n            telemetry.update();\n        }\n\n        // Disable Tracking when we are done;\n        targets.deactivate();\n    }\n\n    /***\n     * Identify a target by naming it, and setting its position and orientation on the field\n     * @param targetIndex\n     * @param targetName\n     * @param dx, dy, dz  Target offsets in x,y,z axes\n     * @param rx, ry, rz  Target rotations in x,y,z axes\n     */\n    void    identifyTarget(int targetIndex, String targetName, float dx, float dy, float dz, float rx, float ry, float rz) {\n        VuforiaTrackable aTarget = targets.get(targetIndex);\n        aTarget.setName(targetName);\n        aTarget.setLocation(OpenGLMatrix.translation(dx, dy, dz)\n                .multiplied(Orientation.getRotationMatrix(EXTRINSIC, XYZ, DEGREES, rx, ry, rz)));\n    }\n}\n",
    "ConceptVuforiaDriveToTargetWebcam": "package org.firstinspires.ftc.robotcontroller.external.samples;\n\nimport com.qualcomm.robotcore.eventloop.opmode.Disabled;\nimport com.qualcomm.robotcore.eventloop.opmode.LinearOpMode;\nimport com.qualcomm.robotcore.eventloop.opmode.TeleOp;\nimport com.qualcomm.robotcore.hardware.DcMotor;\nimport com.qualcomm.robotcore.util.Range;\n\nimport org.firstinspires.ftc.robotcore.external.ClassFactory;\nimport org.firstinspires.ftc.robotcore.external.hardware.camera.WebcamName;\nimport org.firstinspires.ftc.robotcore.external.matrices.OpenGLMatrix;\nimport org.firstinspires.ftc.robotcore.external.matrices.VectorF;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaLocalizer;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackable;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackableDefaultListener;\nimport org.firstinspires.ftc.robotcore.external.navigation.VuforiaTrackables;\n\n/**\n * This OpMode illustrates using a webcam to locate and drive towards ANY Vuforia target.\n * The code assumes a basic two-wheel Robot Configuration with motors named left_drive and right_drive.\n * The motor directions must be set so a positive drive goes forward and a positive turn rotates to the right.\n *\n * Under manual control, the left stick will move forward/back, and the right stick will turn left/right.\n * This is called POV Joystick mode, different than Tank Drive (where each joystick controls a wheel).\n * Manually drive the robot until it displays Target data on the Driver Station.\n * Press and hold the *Left Bumper* to enable the automatic \"Drive to target\" mode.\n * Release the Left Bumper to return to manual driving mode.\n *\n * Use DESIRED_DISTANCE to set how close you want the robot to get to the target.\n * Speed and Turn sensitivity can be adjusted using the SPEED_GAIN and TURN_GAIN constants.\n *\n * For more Vuforia details, or to adapt this OpMode for a phone camera, view the\n *  ConceptVuforiaFieldNavigation and ConceptVuforiaFieldNavigationWebcam samples.\n *\n * Use Android Studio to Copy this Class, and Paste it into your team's code folder with a new name.\n * Remove or comment out the @Disabled line to add this opmode to the Driver Station OpMode list.\n *\n * IMPORTANT: In order to use this OpMode, you need to obtain your own Vuforia license key as\n * is explained below.\n */\n\n@TeleOp(name=\"REPLACE_OPMODE_NAME\", group=\"REPLACE_OPMODE_GROUP\")\n@Disabled\npublic class REPLACE_CLASS extends LinearOpMode\n{\n    // Adjust these numbers to suit your robot.\n    final double DESIRED_DISTANCE = 8.0; //  this is how close the camera should get to the target (inches)\n                                         //  The GAIN constants set the relationship between the measured position error,\n                                         //  and how much power is applied to the drive motors.  Drive = Error * Gain\n                                         //  Make these values smaller for smoother control.\n    final double SPEED_GAIN =   0.02 ;   //  Speed Control \"Gain\". eg: Ramp up to 50% power at a 25 inch error.   (0.50 / 25.0)\n    final double TURN_GAIN  =   0.01 ;   //  Turn Control \"Gain\".  eg: Ramp up to 25% power at a 25 degree error. (0.25 / 25.0)\n\n    final double MM_PER_INCH = 25.40 ;   //  Metric conversion\n\n    /*\n     * IMPORTANT: You need to obtain your own license key to use Vuforia. The string below with which\n     * 'parameters.vuforiaLicenseKey' is initialized is for illustration only, and will not function.\n     * A Vuforia 'Development' license key, can be obtained free of charge from the Vuforia developer\n     * web site at https://developer.vuforia.com/license-manager.\n     *\n     * Vuforia license keys are always 380 characters long, and look as if they contain mostly\n     * random data. As an example, here is a example of a fragment of a valid key:\n     *      ... yIgIzTqZ4mWjk9wd3cZO9T1axEqzuhxoGlfOOI2dRzKS4T0hQ8kT ...\n     * Once you've obtained a license key, copy the string from the Vuforia web site\n     * and paste it in to your code on the next line, between the double quotes.\n     */\n    private static final String VUFORIA_KEY =\n            \" --- YOUR NEW VUFORIA KEY GOES HERE  --- \";\n\n    VuforiaLocalizer vuforia    = null;\n    OpenGLMatrix targetPose     = null;\n    String targetName           = \"\";\n\n    private DcMotor leftDrive   = null;\n    private DcMotor rightDrive  = null;\n\n    @Override public void runOpMode()\n    {\n        /*\n         * Configure Vuforia by creating a Parameter object, and passing it to the Vuforia engine.\n         * To get an on-phone camera preview, use the code below.\n         * If no camera preview is desired, use the parameter-less constructor instead (commented out below).\n         */\n        int cameraMonitorViewId = hardwareMap.appContext.getResources().getIdentifier(\"cameraMonitorViewId\", \"id\", hardwareMap.appContext.getPackageName());\n        VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters(cameraMonitorViewId);\n        // VuforiaLocalizer.Parameters parameters = new VuforiaLocalizer.Parameters();\n\n        parameters.vuforiaLicenseKey = VUFORIA_KEY;\n\n        // Turn off Extended tracking.  Set this true if you want Vuforia to track beyond the target.\n        parameters.useExtendedTracking = false;\n\n        // Connect to the camera we are to use.  This name must match what is set up in Robot Configuration\n        parameters.cameraName = hardwareMap.get(WebcamName.class, \"Webcam 1\");\n        this.vuforia = ClassFactory.getInstance().createVuforia(parameters);\n\n        // Load the trackable objects from the Assets file, and give them meaningful names\n        VuforiaTrackables targetsFreightFrenzy = this.vuforia.loadTrackablesFromAsset(\"FreightFrenzy\");\n        targetsFreightFrenzy.get(0).setName(\"Blue Storage\");\n        targetsFreightFrenzy.get(1).setName(\"Blue Alliance Wall\");\n        targetsFreightFrenzy.get(2).setName(\"Red Storage\");\n        targetsFreightFrenzy.get(3).setName(\"Red Alliance Wall\");\n\n        // Start tracking targets in the background\n        targetsFreightFrenzy.activate();\n\n        // Initialize the hardware variables. Note that the strings used here as parameters\n        // to 'get' must correspond to the names assigned during the robot configuration\n        // step (using the FTC Robot Controller app on the phone).\n        leftDrive  = hardwareMap.get(DcMotor.class, \"left_drive\");\n        rightDrive = hardwareMap.get(DcMotor.class, \"right_drive\");\n\n        // To drive forward, most robots need the motor on one side to be reversed, because the axles point in opposite directions.\n        // Pushing the left stick forward MUST make robot go forward. So adjust these two lines based on your first test drive.\n        leftDrive.setDirection(DcMotor.Direction.FORWARD);\n        rightDrive.setDirection(DcMotor.Direction.REVERSE);\n\n        telemetry.addData(\">\", \"Press Play to start\");\n        telemetry.update();\n\n        waitForStart();\n\n        boolean targetFound     = false;    // Set to true when a target is detected by Vuforia\n        double  targetRange     = 0;        // Distance from camera to target in Inches\n        double  targetBearing   = 0;        // Robot Heading, relative to target.  Positive degrees means target is to the right.\n        double  drive           = 0;        // Desired forward power (-1 to +1)\n        double  turn            = 0;        // Desired turning power (-1 to +1)\n\n        while (opModeIsActive())\n        {\n            // Look for first visible target, and save its pose.\n            targetFound = false;\n            for (VuforiaTrackable trackable : targetsFreightFrenzy)\n            {\n                if (((VuforiaTrackableDefaultListener) trackable.getListener()).isVisible())\n                {\n                    targetPose = ((VuforiaTrackableDefaultListener)trackable.getListener()).getVuforiaCameraFromTarget();\n\n                    // if we have a target, process the \"pose\" to determine the position of the target relative to the robot.\n                    if (targetPose != null)\n                    {\n                        targetFound = true;\n                        targetName  = trackable.getName();\n                        VectorF trans = targetPose.getTranslation();\n\n                        // Extract the X & Y components of the offset of the target relative to the robot\n                        double targetX = trans.get(0) / MM_PER_INCH; // Image X axis\n                        double targetY = trans.get(2) / MM_PER_INCH; // Image Z axis\n\n                        // target range is based on distance from robot position to origin (right triangle).\n                        targetRange = Math.hypot(targetX, targetY);\n\n                        // target bearing is based on angle formed between the X axis and the target range line\n                        targetBearing = Math.toDegrees(Math.asin(targetX / targetRange));\n\n                        break;  // jump out of target tracking loop if we find a target.\n                    }\n                }\n            }\n\n            // Tell the driver what we see, and what to do.\n            if (targetFound) {\n                telemetry.addData(\">\",\"HOLD Left-Bumper to Drive to Target\\n\");\n                telemetry.addData(\"Target\", \" %s\", targetName);\n                telemetry.addData(\"Range\",  \"%5.1f inches\", targetRange);\n                telemetry.addData(\"Bearing\",\"%3.0f degrees\", targetBearing);\n            } else {\n                telemetry.addData(\">\",\"Drive using joystick to find target\\n\");\n            }\n\n            // Drive to target Automatically if Left Bumper is being pressed, AND we have found a target.\n            if (gamepad1.left_bumper && targetFound) {\n\n                // Determine heading and range error so we can use them to control the robot automatically.\n                double  rangeError   = (targetRange - DESIRED_DISTANCE);\n                double  headingError = targetBearing;\n\n                // Use the speed and turn \"gains\" to calculate how we want the robot to move.\n                drive = rangeError * SPEED_GAIN;\n                turn  = headingError * TURN_GAIN ;\n\n                telemetry.addData(\"Auto\",\"Drive %5.2f, Turn %5.2f\", drive, turn);\n            } else {\n\n                // drive using manual POV Joystick mode.\n                drive = -gamepad1.left_stick_y  / 2.0;  // Reduce drive rate to 50%.\n                turn  =  gamepad1.right_stick_x / 4.0;  // Reduce turn rate to 25%.\n                telemetry.addData(\"Manual\",\"Drive %5.2f, Turn %5.2f\", drive, turn);\n            }\n            telemetry.update();\n\n            // Calculate left and right wheel powers and send to them to the motors.\n            double leftPower    = Range.clip(drive + turn, -1.0, 1.0) ;\n            double rightPower   = Range.clip(drive - turn, -1.0, 1.0) ;\n            leftDrive.setPower(leftPower);\n            rightDrive.setPower(rightPower);\n\n            sleep(10);\n        }\n    }\n}\n"
}

module.exports = newClassSnippets